import asyncio
import json
import logging
import os
import re
import shutil
import statistics
import subprocess
import uuid
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import requests
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, JSONResponse, Response, StreamingResponse
from pydantic import BaseModel, Field

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è ANSI escape –∫–æ–¥–æ–≤
ANSI_ESCAPE_PATTERN = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫–µ—à –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≥–ª–∞–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
chapters_metadata_cache: Dict[str, Dict[str, Any]] = {}

def strip_ansi_codes(text: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç ANSI escape –∫–æ–¥—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    return ANSI_ESCAPE_PATTERN.sub('', text)

def get_chapter_display_name(chapter_id: str) -> str:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —É–¥–æ–±–æ—á–∏—Ç–∞–µ–º–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –≥–ª–∞–≤—ã –∏–∑ –∫–µ—à–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ñ–æ—Ä–º–∞—Ç: "Vol.X Ch.Y: Chapter Name" –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ chapter_id –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.
    """
    chapter_info = chapters_metadata_cache.get(chapter_id)
    if not chapter_info:
        return chapter_id
    
    volume = chapter_info.get('volume')
    number = chapter_info.get('number')
    name = chapter_info.get('name', '').strip()
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫—Ä–∞—Å–∏–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
    parts = []
    if volume:
        parts.append(f"Vol.{volume}")
    if number is not None:
        parts.append(f"Ch.{number}")
    
    chapter_part = " ".join(parts) if parts else f"Ch.{chapter_id}"
    
    if name:
        return f"{chapter_part}: {name}"
    else:
        return chapter_part

# =========================================================================================
# –ü–†–û–ö–°–ò –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –° –†–û–¢–ê–¶–ò–ï–ô (–∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º ProxyRotator)
# =========================================================================================
from proxy_rotator import get_proxy_rotator

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Ä–æ—Ç–∞—Ç–æ—Ä –ø—Ä–æ–∫—Å–∏
PROXY_ROTATOR = get_proxy_rotator("mangalib")
logger.info(f"Proxy rotator initialized: {PROXY_ROTATOR}")

def get_proxy_for_request() -> Optional[Dict[str, str]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ–∫—Å–∏ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (—Å —Ä–æ—Ç–∞—Ü–∏–µ–π).
    –ï—Å–ª–∏ –ø—Ä–æ–∫—Å–∏ –æ–¥–∏–Ω - —Ä–æ—Ç–∞—Ü–∏—è –Ω–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç.
    –ï—Å–ª–∏ –ø—Ä–æ–∫—Å–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ - –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ä–æ—Ç–∞—Ü–∏—è —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.
    
    Returns:
        dict —Å –ø—Ä–æ–∫—Å–∏ {'http': '...', 'https': '...'} –∏–ª–∏ None
    """
    if PROXY_ROTATOR.get_proxy_count() == 0:
        return None
    
    # –ï—Å–ª–∏ –ø—Ä–æ–∫—Å–∏ –æ–¥–∏–Ω - –≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ (–±–µ–∑ —Ä–æ—Ç–∞—Ü–∏–∏)
    if PROXY_ROTATOR.get_proxy_count() == 1:
        return PROXY_ROTATOR.get_current_proxy()
    
    # –ï—Å–ª–∏ –ø—Ä–æ–∫—Å–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ - —Ä–æ—Ç–∞—Ü–∏—è –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    return PROXY_ROTATOR.get_next_proxy()

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –ø—Ä–æ–∫—Å–∏
if PROXY_ROTATOR.enabled:
    proxy_count = PROXY_ROTATOR.get_proxy_count()
    logger.info(f"‚úÖ Proxy rotation enabled: {proxy_count} proxy(ies), strategy={PROXY_ROTATOR.rotation_strategy}")
else:
    logger.info("‚ÑπÔ∏è  No proxy configured (requests will go directly)")
# =========================================================================================

app = FastAPI()

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class ParseRequest(BaseModel):
    slug: str
    parser: str = "newtoki"

class BuildRequest(BaseModel):
    slug: str
    parser: str = "newtoki"
    type: str = "simple"

class BatchParseRequest(BaseModel):
    slugs: List[str]
    parser: str = "newtoki"
    auto_import: bool = True

class ParseStatus(BaseModel):
    task_id: str
    status: str  # pending, running, completed, failed
    progress: int
    message: str
    created_at: str
    updated_at: str
    total_slugs: int
    completed_slugs: int
    failed_slugs: int
    current_slug: Optional[str] = None
    results: List[Dict[str, Any]] = Field(default_factory=list)
    metrics: Optional[Dict[str, Any]] = None


class BatchParseStatus(BaseModel):
    task_id: str
    status: str  # pending, running, completed, failed
    progress: int
    message: str
    created_at: str
    updated_at: str
    total_slugs: int
    completed_slugs: int
    failed_slugs: int
    current_slug: Optional[str] = None
    results: List[Dict[str, Any]] = Field(default_factory=list)
    metrics: Optional[Dict[str, Any]] = None

class LogEntry(BaseModel):
    timestamp: str
    level: str
    message: str
    task_id: Optional[str] = None
    sequence: int = 0

# –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∑–∞–¥–∞—á (–≤ production –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Redis)
tasks_storage: Dict[str, ParseStatus] = {}

# –•—Ä–∞–Ω–∏–ª–∏—â–µ –ª–æ–≥–æ–≤ –¥–ª—è –∑–∞–¥–∞—á
task_logs: Dict[str, List[LogEntry]] = {}

# –°—á–µ—Ç—á–∏–∫–∏ –∏ –º–∞—Ä–∫–µ—Ä—ã –æ—Ç–ø—Ä–∞–≤–∫–∏ –ª–æ–≥–æ–≤, —á—Ç–æ–±—ã –∏–∑–±–µ–≥–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–µ—Ä–µ–¥–∞—á–∏
task_log_sequence_counter: Dict[str, int] = {}
task_last_sent_sequence: Dict[str, int] = {}

# –•—Ä–∞–Ω–∏–ª–∏—â–µ —Å–æ—Å—Ç–æ—è–Ω–∏–π –±–∏–ª–¥–∞ –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
build_states: Dict[str, Dict[str, Any]] = {}  # task_id -> {"slug": str, "is_ready": bool, "files_ready": bool}

# –•—Ä–∞–Ω–∏–ª–∏—â–µ –∏—Å–ø–æ–ª–Ω—è—é—â–∏—Ö—Å—è –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –¥–ª—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ—Ç–º–µ–Ω—ã –∑–∞–¥–∞—á
running_processes: Dict[str, asyncio.subprocess.Process] = {}
running_processes_lock = asyncio.Lock()

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
def get_melon_base_path() -> Path:
    return Path("/app")

def ensure_utf8_patch():
    """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ø–∞—Ç—á –¥–ª—è UTF-8 –∫–æ–¥–∏—Ä–æ–≤–∫–∏"""
    dublib_path = get_melon_base_path() / "dublib" / "Methods" / "Filesystem.py"

    if dublib_path.exists():
        content = dublib_path.read_text(encoding='utf-8')
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ –ø–∞—Ç—á
        if 'open(path, "w")' in content and 'encoding="utf-8"' not in content:
            logger.info("Applying UTF-8 patch to dublib...")
            content = content.replace(
                'open(path, "w")',
                'open(path, "w", encoding="utf-8")'
            )
            dublib_path.write_text(content, encoding='utf-8')
            logger.info("UTF-8 patch applied successfully")

def log_task_message(task_id: str, level: str, message: str):
    """–î–æ–±–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –ª–æ–≥–∏ –∑–∞–¥–∞—á–∏ (–æ—á–∏—â–∞–µ—Ç ANSI –∫–æ–¥—ã)"""
    if task_id not in task_logs:
        task_logs[task_id] = []
    
    # –û—á–∏—â–∞–µ–º ANSI escape –∫–æ–¥—ã –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è
    clean_message = strip_ansi_codes(message)

    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å—á–µ—Ç—á–∏–∫ –ª–æ–≥–æ–≤ –¥–ª—è –∑–∞–¥–∞—á–∏
    next_sequence = task_log_sequence_counter.get(task_id, 0) + 1
    task_log_sequence_counter[task_id] = next_sequence
    
    log_entry = LogEntry(
        timestamp=datetime.now().isoformat(),
        level=level,
        message=clean_message,
        task_id=task_id,
        sequence=next_sequence
    )
    
    task_logs[task_id].append(log_entry)
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–≥–æ–≤ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 1000)
    if len(task_logs[task_id]) > 1000:
        task_logs[task_id] = task_logs[task_id][-1000:]

        # –ï—Å–ª–∏ —Ö–≤–æ—Å—Ç –ø–æ—á–∏—Å—Ç–∏–ª–∏, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –º–∞—Ä–∫–µ—Ä—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if task_id in task_last_sent_sequence:
            max_sequence_in_cache = task_logs[task_id][-1].sequence if task_logs[task_id] else 0
            if task_last_sent_sequence[task_id] > max_sequence_in_cache:
                task_last_sent_sequence[task_id] = max_sequence_in_cache

        if task_id in task_log_sequence_counter:
            task_log_sequence_counter[task_id] = task_logs[task_id][-1].sequence if task_logs[task_id] else 0

def ensure_cross_device_patch():
    """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –æ—à–∏–±–∫—É 'Invalid cross-device link' –≤ MangaBuilder"""
    builder_path = get_melon_base_path() / "Parsers" / "MangaBuilder.py"
    
    if builder_path.exists():
        content = builder_path.read_text(encoding='utf-8')
        # –ó–∞–º–µ–Ω—è–µ–º os.rename –Ω–∞ shutil.move –¥–ª—è cross-device –æ–ø–µ—Ä–∞—Ü–∏–π
        if "os.rename(" in content and "shutil.move(" not in content:
            logger.info("Applying cross-device patch to MangaBuilder...")
            content = content.replace("os.rename(", "shutil.move(")
            if "import shutil" not in content:
                content = "import shutil\n" + content
            builder_path.write_text(content, encoding='utf-8')
            logger.info("Cross-device patch applied successfully")

async def register_running_process(task_id: str, process: asyncio.subprocess.Process):
    async with running_processes_lock:
        running_processes[task_id] = process


async def unregister_running_process(task_id: str):
    async with running_processes_lock:
        running_processes.pop(task_id, None)


async def get_running_process(task_id: str) -> Optional[asyncio.subprocess.Process]:
    async with running_processes_lock:
        return running_processes.get(task_id)


async def terminate_process(process: asyncio.subprocess.Process) -> None:
    if process.returncode is not None:
        return

    try:
        process.terminate()
        await asyncio.wait_for(process.wait(), timeout=10)
    except asyncio.TimeoutError:
        logger.warning("Process did not terminate gracefully, forcing kill")
        process.kill()
        await process.wait()
    except ProcessLookupError:
        # –ü—Ä–æ—Ü–µ—Å—Å —É–∂–µ –∑–∞–≤–µ—Ä—à—ë–Ω
        pass

def send_progress_to_manga_service(task_id, status, progress, message=None, error=None, logs=None, metrics=None):
    try:
        payload = {
            "status": status,
            "progress": progress,
            "message": message,
            "error": error
        }
        if logs:
            payload["logs"] = logs if isinstance(logs, list) else [logs]
        if metrics is not None:
            payload["metrics"] = metrics
        url = f"http://manga-service:8081/api/parser/progress/{task_id}"
        resp = requests.post(url, json=payload, timeout=5)
        # –£–±—Ä–∞–Ω–æ –∏–∑–±—ã—Ç–æ—á–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ (—Ç–æ–ª—å–∫–æ –æ—à–∏–±–∫–∏ –≤–∞–∂–Ω—ã)
    except Exception as e:
        logger.error(f"Failed to send progress to MangaService: {e}")

def update_task_status(
    task_id: str,
    status: str,
    progress: int,
    message: str,
    result: Optional[Dict] = None,
    error: Optional[str] = None,
    metrics: Optional[Dict[str, Any]] = None
):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ª–æ–≥–∏ –≤ MangaService"""
    if task_id in tasks_storage:
        task = tasks_storage[task_id]
        if task.status == "CANCELLED" and status != "CANCELLED":
            logger.info(f"[{task_id}] Skip status update '{status}' because task already cancelled")
            return
        task.status = status
        task.progress = progress
        task.message = message
        task.updated_at = datetime.now().isoformat()
        collected_metrics: Optional[Dict[str, Any]] = metrics
        if result:
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ø–∏—Å–æ–∫ results (–Ω–µ result!)
            if isinstance(result, list):
                task.results.extend(result)
                if collected_metrics is None:
                    for entry in reversed(result):
                        if isinstance(entry, dict):
                            metrics_candidate = entry.get("metrics")
                            if isinstance(metrics_candidate, dict):
                                collected_metrics = metrics_candidate
                                break
            else:
                task.results.append(result)
                if collected_metrics is None and isinstance(result, dict):
                    metrics_candidate = result.get("metrics")
                    if isinstance(metrics_candidate, dict):
                        collected_metrics = metrics_candidate

        if collected_metrics is not None:
            task.metrics = collected_metrics
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –ª–æ–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –µ—â—ë –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–ª–∏—Å—å
        logs_to_send = None
        if task_id in task_logs and task_logs[task_id]:
            last_sent_sequence = task_last_sent_sequence.get(task_id, 0)
            new_entries = [log for log in task_logs[task_id] if log.sequence > last_sent_sequence]

            if new_entries:
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º–æ–≥–æ –±–∞—Ç—á–∞, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å API
                if len(new_entries) > 50:
                    new_entries = new_entries[-50:]

                logs_to_send = [f"[{log.timestamp}] [{log.level}] {log.message}" for log in new_entries]
                task_last_sent_sequence[task_id] = new_entries[-1].sequence
        
        send_progress_to_manga_service(task_id, status, progress, message, error, logs_to_send, collected_metrics)

        if status in {"COMPLETED", "FAILED", "CANCELLED"}:
            task_last_sent_sequence.pop(task_id, None)
            task_log_sequence_counter.pop(task_id, None)

async def run_melon_command(command: List[str], task_id: str, timeout: int = 600) -> Dict[str, Any]:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –∫–æ–º–∞–Ω–¥—É MelonService –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π timeout, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –º–µ—Ç—Ä–∏–∫"""
    process: Optional[asyncio.subprocess.Process] = None
    try:
        base_path = get_melon_base_path()
        process_started_at = datetime.now()

        full_command = ["python", "main.py"] + command[2:]

        logger.info(f"[{task_id}] COMMAND: {' '.join(full_command)}")
        update_task_status(task_id, "RUNNING", 5, f"–ó–∞–ø—É—Å–∫ –∫–æ–º–∞–Ω–¥—ã: {' '.join(full_command)}")

        process = await asyncio.create_subprocess_exec(
            *full_command,
            cwd=base_path,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        await register_running_process(task_id, process)

        stdout_lines: List[str] = []
        stderr_lines: List[str] = []
        last_update_time = datetime.now()

        chapter_state: Dict[str, Dict[str, Any]] = {}
        chapter_order: List[str] = []
        chapter_metrics: List[Dict[str, Any]] = []
        current_chapter_id: Optional[str] = None

        def extract_timestamp(message: str) -> Optional[datetime]:
            ts_match = re.search(r"\[(\d{4}-\d{2}-\d{2}T[^\]]+)\]", message)
            if ts_match:
                try:
                    return datetime.fromisoformat(ts_match.group(1))
                except ValueError:
                    return None
            return None

        def start_chapter(chapter_id: str, ts: Optional[datetime]):
            nonlocal current_chapter_id
            current_chapter_id = chapter_id
            chapter_order.append(chapter_id)
            chapter_state[chapter_id] = {
                "start": ts or datetime.now(),
                "expected_images": None
            }

        def register_expected_images(chapter_id: Optional[str], images: int):
            if not chapter_id:
                return
            state = chapter_state.get(chapter_id)
            if not state:
                chapter_state[chapter_id] = {
                    "start": datetime.now(),
                    "expected_images": images
                }
            else:
                state["expected_images"] = images

        def finalize_chapter(chapter_id: Optional[str], images: int, finished_ts: Optional[datetime]):
            if not chapter_id:
                return
            state = chapter_state.get(chapter_id, {})
            start_ts: Optional[datetime] = state.get("start")
            end_ts = finished_ts or datetime.now()
            duration = None
            if start_ts:
                duration = (end_ts - start_ts).total_seconds()
            metric_entry = {
                "chapter_id": chapter_id,
                "images": images,
                "expected_images": state.get("expected_images"),
                "duration_seconds": duration,
                "started_at": start_ts.isoformat() if start_ts else None,
                "completed_at": end_ts.isoformat() if end_ts else None,
                "images_per_second": round(images / duration, 4) if duration and images else None
            }
            chapter_metrics.append(metric_entry)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≥–ª–∞–≤—ã
            chapter_display = get_chapter_display_name(chapter_id)
            if duration:
                speed = metric_entry["images_per_second"] or 0.0
                log_task_message(task_id, "DEBUG", f"[Metrics] Chapter {chapter_display}: {images} images in {duration:.2f}s ({speed:.2f} img/s)")
            else:
                log_task_message(task_id, "DEBUG", f"[Metrics] Chapter {chapter_display}: {images} images")

            chapter_state.pop(chapter_id, None)
            if chapter_id in chapter_order:
                chapter_order.remove(chapter_id)

        async def read_stdout():
            nonlocal last_update_time, current_chapter_id
            if not process.stdout:
                return
            while True:
                line = await process.stdout.readline()
                if not line:
                    break
                line_str = line.decode("utf-8", errors="ignore").strip()
                if not line_str:
                    continue

                clean_line = strip_ansi_codes(line_str)
                stdout_lines.append(clean_line)
                log_task_message(task_id, "INFO", clean_line)
                last_update_time = datetime.now()

                build_match = re.search(r"Building chapter (\d+)", clean_line)
                if build_match:
                    start_chapter(build_match.group(1), extract_timestamp(clean_line))

                download_start_match = re.search(r"Starting parallel download of (\d+) images", clean_line)
                if download_start_match:
                    register_expected_images(current_chapter_id, int(download_start_match.group(1)))

                download_done_match = re.search(r"Chapter download completed: (\d+) images", clean_line)
                if download_done_match:
                    chapter_id = current_chapter_id
                    if not chapter_id:
                        for candidate in reversed(chapter_order):
                            if candidate in chapter_state:
                                chapter_id = candidate
                                break
                    finalize_chapter(chapter_id, int(download_done_match.group(1)), extract_timestamp(clean_line))
                    current_chapter_id = None

                if "Chapter" in clean_line and "completed" in clean_line:
                    update_task_status(task_id, "RUNNING", min(90, 10 + len(stdout_lines)), f"–ü–∞—Ä—Å–∏–Ω–≥: {clean_line}")
                elif "Parsing" in clean_line and "..." in clean_line:
                    update_task_status(task_id, "RUNNING", min(90, 10 + len(stdout_lines)), f"–ü–∞—Ä—Å–∏–Ω–≥: {clean_line}")
                elif "Building" in clean_line:
                    update_task_status(task_id, "RUNNING", min(90, 10 + len(stdout_lines)), f"–ë–∏–ª–¥–∏–Ω–≥: {clean_line}")
                elif "Done in" in clean_line:
                    update_task_status(task_id, "RUNNING", 95, f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ: {clean_line}")

        async def read_stderr():
            nonlocal last_update_time
            if not process.stderr:
                return
            while True:
                line = await process.stderr.readline()
                if not line:
                    break
                line_str = line.decode("utf-8", errors="ignore").strip()
                if not line_str:
                    continue
                clean_line = strip_ansi_codes(line_str)
                stderr_lines.append(clean_line)
                log_task_message(task_id, "ERROR", clean_line)
                # –£–±—Ä–∞–Ω–æ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ: logger.warning() - –æ—à–∏–±–∫–∞ —É–∂–µ –≤ task logs
                last_update_time = datetime.now()

        async def heartbeat():
            nonlocal last_update_time
            while process.returncode is None:
                await asyncio.sleep(5)  # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∫–∞–∂–¥—ã–µ 5—Å
                if process.returncode is None:
                    elapsed = (datetime.now() - last_update_time).total_seconds()
                    # –£–±—Ä–∞–Ω–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ heartbeat –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —à—É–º–∞, —Ç–æ–ª—å–∫–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
                    update_task_status(task_id, "RUNNING", min(90, 10 + len(stdout_lines)), f"–ü–∞—Ä—Å–∏–Ω–≥ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ... ({len(stdout_lines)} —Å—Ç—Ä–æ–∫ –ª–æ–≥–æ–≤)")

        try:
            await asyncio.gather(
                read_stdout(),
                read_stderr(),
                heartbeat()
            )

            return_code = await process.wait()
            process_completed_at = datetime.now()

            stdout = "\n".join(stdout_lines)
            stderr = "\n".join(stderr_lines)

            task_cancelled = False
            task = tasks_storage.get(task_id)
            if task and task.status == "CANCELLED":
                task_cancelled = True

            durations = [entry["duration_seconds"] for entry in chapter_metrics if entry.get("duration_seconds")]
            aggregate_metrics: Optional[Dict[str, Any]] = None
            if durations:
                total_images = sum((entry.get("images") or 0) for entry in chapter_metrics)
                total_duration = sum(durations)
                aggregate_metrics = {
                    "chapters": len(chapter_metrics),
                    "total_images": total_images,
                    "total_duration_seconds": total_duration,
                    "avg_duration_seconds": total_duration / len(durations) if durations else None,
                    "median_duration_seconds": statistics.median(durations),
                    "min_duration_seconds": min(durations),
                    "max_duration_seconds": max(durations),
                    "images_per_second": (total_images / total_duration) if total_duration else None
                }

                summary_msg = (
                    f"[Metrics] Chapters processed: {len(chapter_metrics)}, {total_images} images in {total_duration:.2f}s "
                    f"(avg {aggregate_metrics['avg_duration_seconds']:.2f}s/chapter, "
                    f"{aggregate_metrics['images_per_second']:.2f} img/s)"
                )
                log_task_message(task_id, "INFO", summary_msg)
                # –£–±—Ä–∞–Ω–æ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ: logger.info(summary_msg)

            command_metrics = {
                "started_at": process_started_at.isoformat(),
                "completed_at": process_completed_at.isoformat(),
                "duration_seconds": (process_completed_at - process_started_at).total_seconds()
            }

            metrics_payload = {
                "chapters": chapter_metrics,
                "aggregate": aggregate_metrics,
                "command": command_metrics
            }

            if task_cancelled:
                logger.info(f"[{task_id}] Command cancelled by user request")
                return {
                    "success": False,
                    "stdout": stdout,
                    "stderr": stderr,
                    "return_code": return_code,
                    "metrics": metrics_payload,
                    "cancelled": True
                }

            if return_code == 0:
                logger.info(f"[{task_id}] Command completed successfully")
                update_task_status(task_id, "RUNNING", 95, "–ö–æ–º–∞–Ω–¥–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ", metrics=metrics_payload)
                return {
                    "success": True,
                    "stdout": stdout,
                    "stderr": stderr,
                    "metrics": metrics_payload,
                    "cancelled": False
                }

            logger.error(f"[{task_id}] Command failed with return code {return_code}")
            return {
                "success": False,
                "stdout": stdout,
                "stderr": stderr,
                "return_code": return_code,
                "metrics": metrics_payload,
                "cancelled": task_cancelled
            }
        finally:
            await unregister_running_process(task_id)

    except Exception as e:
        logger.error(f"[{task_id}] Error running command: {str(e)}")
        return {
            "success": False,
            "error": str(e)
        }

async def execute_batch_parse_task(task_id: str, slugs: List[str], parser: str, auto_import: bool):
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–∞–∫–µ—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ –º–∞–Ω–≥–∏ —Å –æ—Ç–¥–µ–ª—å–Ω—ã–º —ç—Ç–∞–ø–æ–º –∏–º–ø–æ—Ä—Ç–∞"""
    try:
        ensure_utf8_patch()
        ensure_cross_device_patch()
        
        total_slugs = len(slugs)
        results: List[Dict[str, Any]] = []
        completed = 0
        failed = 0
        successfully_built = []  # –°–ø–∏—Å–æ–∫ —É—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –º–∞–Ω–≥ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞

        parse_durations: List[float] = []
        build_durations: List[float] = []
        import_durations: List[float] = []
        download_images_total = 0
        download_duration_total = 0.0

        def get_result_entry(slug_value: str) -> Dict[str, Any]:
            for entry in results:
                if entry.get("slug") == slug_value:
                    return entry
            new_entry = {
                "slug": slug_value,
                "status": "pending",
                "metrics": {
                    "parse": None,
                    "build": None,
                    "import": None
                }
            }
            results.append(new_entry)
            return new_entry

        def ensure_metrics(entry: Dict[str, Any]):
            if "metrics" not in entry or not isinstance(entry["metrics"], dict):
                entry["metrics"] = {
                    "parse": None,
                    "build": None,
                    "import": None
                }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –±–∏–ª–¥–∞ –¥–ª—è –∑–∞–¥–∞—á–∏
        build_states[task_id] = {"current_slug": None, "is_ready": False, "files_ready": False}
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞—á–∞–ª–∞
        tasks_storage[task_id].status = "running"
        tasks_storage[task_id].message = f"–ù–∞—á–∏–Ω–∞–µ–º –ø–∞–∫–µ—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∏ –±–∏–ª–¥ {total_slugs} –º–∞–Ω–≥–∏"
        tasks_storage[task_id].updated_at = datetime.now().isoformat()
        
        # –≠–¢–ê–ü 1: –ü–∞—Ä—Å–∏–Ω–≥ –∏ –±–∏–ª–¥ –≤—Å–µ—Ö –º–∞–Ω–≥
        logger.info(f"Starting batch parse and build phase for {total_slugs} manga")
        
        for i, slug in enumerate(slugs):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ—Ç–º–µ–Ω–µ–Ω–∞ –ª–∏ –∑–∞–¥–∞—á–∞
            if task_id in tasks_storage and tasks_storage[task_id].status == "CANCELLED":
                logger.info(f"Batch task {task_id} cancelled, stopping parse loop")
                break
            
            try:
                # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –Ω–æ–≤–æ–π –º–∞–Ω–≥–∏
                build_states[task_id].update({
                    "current_slug": slug,
                    "is_ready": False,
                    "files_ready": False
                })
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π —Å–ª–∞–≥
                tasks_storage[task_id].current_slug = slug
                tasks_storage[task_id].message = f"–ü–∞—Ä—Å–∏–Ω–≥: {slug} ({i+1}/{total_slugs})"
                tasks_storage[task_id].progress = int((i / total_slugs) * 35)  # 35% –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ –±–∏–ª–¥–∞
                tasks_storage[task_id].updated_at = datetime.now().isoformat()
                
                logger.info(f"Batch parsing: processing {slug} ({i+1}/{total_slugs})")
                
                # –®–∞–≥ 1: –ü–∞—Ä—Å–∏–Ω–≥
                command = ["python", "main.py", "parse", slug, "-skip-images", "--use", parser]
                result = await run_melon_command(command, task_id, timeout=1800)  # 30 –º–∏–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç
                entry = get_result_entry(slug)
                ensure_metrics(entry)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª–∞ –ª–∏ –∑–∞–¥–∞—á–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞
                if result.get("cancelled"):
                    logger.info(f"Parse command for {slug} was cancelled")
                    entry.update({
                        "status": "cancelled",
                        "step": "parse"
                    })
                    break

                if not result["success"]:
                    failed += 1
                    entry.update({
                        "status": "failed",
                        "step": "parse",
                        "error": result.get('stderr', 'Unknown parse error')
                    })
                    entry["metrics"]["parse"] = result.get("metrics")
                    continue

                entry["status"] = "parsed"
                entry["metrics"]["parse"] = result.get("metrics")

                command_metrics = (result.get("metrics") or {}).get("command") or {}
                command_duration = command_metrics.get("duration_seconds")
                if isinstance(command_duration, (int, float)):
                    parse_durations.append(command_duration)

                aggregate_metrics = (result.get("metrics") or {}).get("aggregate") or {}
                download_images_total += aggregate_metrics.get("total_images", 0) or 0
                download_duration_total += aggregate_metrics.get("total_duration_seconds", 0.0) or 0.0
                
                # –®–∞–≥ 2: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞
                tasks_storage[task_id].message = f"–ë–∏–ª–¥–∏–Ω–≥: {slug} ({i+1}/{total_slugs})"
                tasks_storage[task_id].updated_at = datetime.now().isoformat()
                
                build_command = ["python", "main.py", "build-manga", slug, "--use", parser, "-simple"]
                build_result = await run_melon_command(build_command, task_id, timeout=1800)  # 30 –º–∏–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç
                ensure_metrics(entry)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª–∞ –ª–∏ –∑–∞–¥–∞—á–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞
                if build_result.get("cancelled"):
                    logger.info(f"Build command for {slug} was cancelled")
                    entry.update({
                        "status": "cancelled",
                        "step": "build"
                    })
                    break

                if not build_result["success"]:
                    failed += 1
                    entry.update({
                        "status": "failed",
                        "step": "build",
                        "error": build_result.get('stderr', 'Unknown build error')
                    })
                    entry["metrics"]["build"] = build_result.get("metrics")
                    continue

                entry["status"] = "built"
                entry["metrics"]["build"] = build_result.get("metrics")

                build_command_metrics = (build_result.get("metrics") or {}).get("command") or {}
                build_duration = build_command_metrics.get("duration_seconds")
                if isinstance(build_duration, (int, float)):
                    build_durations.append(build_duration)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ —É—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö
                successfully_built.append(slug)
                logger.info(f"Successfully built {slug}")
                
            except Exception as e:
                failed += 1
                entry = get_result_entry(slug)
                ensure_metrics(entry)
                entry.update({
                    "status": "failed",
                    "step": "general",
                    "error": str(e)
                })
                logger.error(f"Error processing {slug}: {str(e)}")
        
        # –≠–¢–ê–ü 2: –ò–º–ø–æ—Ä—Ç –≤—Å–µ—Ö —É—Å–ø–µ—à–Ω–æ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –º–∞–Ω–≥
        if auto_import and successfully_built:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ—Ç–º–µ–Ω–µ–Ω–∞ –ª–∏ –∑–∞–¥–∞—á–∞ –ø–µ—Ä–µ–¥ –∏–º–ø–æ—Ä—Ç–æ–º
            if task_id in tasks_storage and tasks_storage[task_id].status == "CANCELLED":
                logger.info(f"Batch task {task_id} cancelled, skipping import phase")
            else:
                logger.info(f"Starting import phase for {len(successfully_built)} successfully built manga")
                
                # –î–∞–µ–º –≤—Ä–µ–º—è –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–æ–≤
                tasks_storage[task_id].message = f"–û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–æ–≤..."
                tasks_storage[task_id].progress = 40
                tasks_storage[task_id].updated_at = datetime.now().isoformat()
                await asyncio.sleep(10)  # 10 —Å–µ–∫—É–Ω–¥ –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è I/O –æ–ø–µ—Ä–∞—Ü–∏–π
            
            for i, slug in enumerate(successfully_built):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ—Ç–º–µ–Ω–µ–Ω–∞ –ª–∏ –∑–∞–¥–∞—á–∞
                if task_id in tasks_storage and tasks_storage[task_id].status == "CANCELLED":
                    logger.info(f"Batch task {task_id} cancelled, stopping import loop")
                    break
                
                import_started_at = datetime.now()
                entry = get_result_entry(slug)
                ensure_metrics(entry)

                try:
                    tasks_storage[task_id].current_slug = slug
                    tasks_storage[task_id].message = f"–ò–º–ø–æ—Ä—Ç: {slug} ({i+1}/{len(successfully_built)})"
                    tasks_storage[task_id].progress = 40 + int((i / len(successfully_built)) * 55)  # 40-95% –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
                    tasks_storage[task_id].updated_at = datetime.now().isoformat()
                    
                    logger.info(f"Starting import for {slug}")
                    
                    import_url = "http://manga-service:8081/parser/import/" + slug
                    response = requests.post(import_url, timeout=180)
                    import_duration = (datetime.now() - import_started_at).total_seconds()

                    entry.setdefault("manga_info", {})
                    entry["metrics"]["import"] = {
                        "duration_seconds": import_duration,
                        "status_code": response.status_code,
                        "success": response.status_code == 200
                    }

                    if response.status_code == 200:
                        try:
                            import_data = response.json()
                        except Exception:
                            import_data = {}
                        logger.info(f"Successfully imported {slug}")
                        entry.update({
                            "status": "completed",
                            "import_status": "success",
                            "imported": True
                        })
                        entry["manga_info"].update({
                            "title": import_data.get("title", ""),
                            "chapters": import_data.get("chapters", 0),
                            "import_result": import_data
                        })
                        completed += 1
                        import_durations.append(import_duration)
                    else:
                        logger.warning(f"Failed to import {slug}: HTTP {response.status_code}")
                        entry.update({
                            "status": "failed",
                            "import_status": "failed",
                            "import_error": f"Import failed with HTTP {response.status_code}",
                            "import_response": response.text[:200] if response.text else "No response body"
                        })
                except Exception as e:
                    import_duration = (datetime.now() - import_started_at).total_seconds()
                    logger.error(f"Failed to import {slug}: {str(e)}")
                    entry.setdefault("manga_info", {})
                    entry["metrics"]["import"] = {
                        "duration_seconds": import_duration,
                        "status_code": None,
                        "success": False,
                        "error": str(e)
                    }
                    entry.update({
                        "status": "failed",
                        "import_status": "failed",
                        "import_error": str(e)
                    })
        
        summary_metrics: Dict[str, Any] = {}

        if parse_durations:
            summary_metrics["parse"] = {
                "count": len(parse_durations),
                "avg_duration_seconds": statistics.mean(parse_durations),
                "median_duration_seconds": statistics.median(parse_durations),
                "min_duration_seconds": min(parse_durations),
                "max_duration_seconds": max(parse_durations)
            }

        if build_durations:
            summary_metrics["build"] = {
                "count": len(build_durations),
                "avg_duration_seconds": statistics.mean(build_durations),
                "median_duration_seconds": statistics.median(build_durations),
                "min_duration_seconds": min(build_durations),
                "max_duration_seconds": max(build_durations)
            }

        if import_durations:
            summary_metrics["import"] = {
                "count": len(import_durations),
                "avg_duration_seconds": statistics.mean(import_durations),
                "median_duration_seconds": statistics.median(import_durations),
                "min_duration_seconds": min(import_durations),
                "max_duration_seconds": max(import_durations)
            }

        if download_images_total and download_duration_total:
            summary_metrics["download"] = {
                "images": download_images_total,
                "total_duration_seconds": download_duration_total,
                "images_per_second": (download_images_total / download_duration_total) if download_duration_total else None
            }

        if summary_metrics:
            log_task_message(task_id, "INFO", f"[Metrics] Batch summary: {json.dumps(summary_metrics, ensure_ascii=False)}")
            results.append({
                "slug": None,
                "type": "summary",
                "metrics": summary_metrics
            })

        # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
        tasks_storage[task_id].message = f"–ü–∞–∫–µ—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –£—Å–ø–µ—à–Ω–æ: {completed}, –û—à–∏–±–æ–∫: {failed}"
        tasks_storage[task_id].progress = 100
        tasks_storage[task_id].status = "completed"
        tasks_storage[task_id].completed_slugs = completed
        tasks_storage[task_id].failed_slugs = failed
        tasks_storage[task_id].results = results
        tasks_storage[task_id].updated_at = datetime.now().isoformat()
        
        logger.info(f"Batch parsing completed. Successful: {completed}, Failed: {failed}")
        
        # –û—á–∏—â–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –±–∏–ª–¥–∞
        if task_id in build_states:
            del build_states[task_id]

    except Exception as e:
        update_task_status(task_id, "FAILED", 100, f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
        logger.error(f"Critical error in batch parsing: {str(e)}")

# –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã

@app.get("/tasks")
async def get_all_tasks():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö –∑–∞–¥–∞—á"""
    tasks = []
    for task_id, task in tasks_storage.items():
        task_dict = {
            "task_id": task.task_id,
            "status": task.status,
            "progress": task.progress,
            "message": task.message,
            "created_at": task.created_at,
            "updated_at": task.updated_at,
            "total_slugs": getattr(task, 'total_slugs', 1),
            "completed_slugs": getattr(task, 'completed_slugs', 0),
            "failed_slugs": getattr(task, 'failed_slugs', 0),
            "current_slug": getattr(task, 'current_slug', None),
            "slug": getattr(task, 'current_slug', None),
        }
        tasks.append(task_dict)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å–æ–∑–¥–∞–Ω–∏—è (–Ω–æ–≤—ã–µ —Å–≤–µ—Ä—Ö—É)
    tasks.sort(key=lambda x: x['created_at'], reverse=True)
    return tasks

@app.post("/tasks/clear-completed")
async def clear_completed_tasks():
    """–û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –∏ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–¥–∞—á"""
    completed_tasks = []
    for task_id, task in list(tasks_storage.items()):
        if task.status in ["completed", "failed"]:
            completed_tasks.append(task_id)
            del tasks_storage[task_id]
            if task_id in task_logs:
                del task_logs[task_id]
            task_log_sequence_counter.pop(task_id, None)
            task_last_sent_sequence.pop(task_id, None)
            if task_id in build_states:
                del build_states[task_id]
    
    return {"cleared": len(completed_tasks), "task_ids": completed_tasks}

@app.get("/")
async def get_main():
    return {"message": "MelonService API Server", "status": "running", "monitor": "/monitor.html"}

@app.post("/parse")
async def parse_manga(request: ParseRequest, background_tasks: BackgroundTasks):
    """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π –º–∞–Ω–≥–∏"""
    task_id = str(uuid.uuid4())
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É
    task = ParseStatus(
        task_id=task_id,
        status="pending",
        progress=0,
        message=f"–ü–∞—Ä—Å–∏–Ω–≥ –º–∞–Ω–≥–∏ {request.slug}",
        created_at=datetime.now().isoformat(),
        updated_at=datetime.now().isoformat(),
        total_slugs=1,
        completed_slugs=0,
        failed_slugs=0,
        current_slug=request.slug
    )
    
    tasks_storage[task_id] = task
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á—É —á–µ—Ä–µ–∑ BackgroundTasks
    background_tasks.add_task(execute_parse_task, task_id, request.slug, request.parser)
    
    return {"task_id": task_id, "status": "pending"}

@app.post("/build")
async def build_manga(request: BuildRequest, background_tasks: BackgroundTasks):
    """–ë–∏–ª–¥ –æ–¥–Ω–æ–π –º–∞–Ω–≥–∏"""
    task_id = str(uuid.uuid4())
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É
    task = ParseStatus(
        task_id=task_id,
        status="pending",
        progress=0,
        message=f"–ë–∏–ª–¥ –º–∞–Ω–≥–∏ {request.slug}",
        created_at=datetime.now().isoformat(),
        updated_at=datetime.now().isoformat(),
        total_slugs=1,
        completed_slugs=0,
        failed_slugs=0,
        current_slug=request.slug
    )
    
    tasks_storage[task_id] = task
    
    # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –±–∏–ª–¥–∞
    logger.info(f"[{task_id}] BUILD START: {request.slug} (type: {request.type})")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á—É —á–µ—Ä–µ–∑ BackgroundTasks
    background_tasks.add_task(execute_build_task, task_id, request.slug, request.parser, None, request.type)
    
    return {"task_id": task_id, "status": "pending"}

@app.post("/batch-start")
async def start_batch_parse(request: BatchParseRequest):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–∞–∫–µ—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ –º–∞–Ω–≥–∏"""
    task_id = str(uuid.uuid4())
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É
    task = BatchParseStatus(
        task_id=task_id,
        status="pending",
        progress=0,
        message=f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –ø–∞–∫–µ—Ç–Ω–æ–º—É –ø–∞—Ä—Å–∏–Ω–≥—É {len(request.slugs)} –º–∞–Ω–≥–∏",
        created_at=datetime.now().isoformat(),
        updated_at=datetime.now().isoformat(),
        total_slugs=len(request.slugs),
        completed_slugs=0,
        failed_slugs=0
    )
    
    tasks_storage[task_id] = task
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á—É –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
    asyncio.create_task(execute_batch_parse_task(task_id, request.slugs, request.parser, request.auto_import))
    
    return {"task_id": task_id, "status": "started"}

@app.post("/batch-parse")
async def batch_parse_alias(request: BatchParseRequest):
    """–ê–ª–∏–∞—Å –¥–ª—è batch-start - –∑–∞–ø—É—Å–∫–∞–µ—Ç –ø–∞–∫–µ—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å–ø–∏—Å–∫–∞ –º–∞–Ω–≥–∏"""
    return await start_batch_parse(request)

@app.get("/logs/{task_id}")
async def get_task_logs(task_id: str, limit: int = 100):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ª–æ–≥–æ–≤ –∑–∞–¥–∞—á–∏"""
    if task_id not in task_logs:
        raise HTTPException(status_code=404, detail="Task logs not found")
    
    logs = task_logs[task_id]
    if limit > 0:
        logs = logs[-limit:]
    
    return {"task_id": task_id, "logs": [log.dict() for log in logs]}

@app.get("/logs/{task_id}/stream")
async def stream_task_logs(task_id: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ª–æ–≥–æ–≤ –∑–∞–¥–∞—á–∏ –≤ —Ä–µ–∂–∏–º–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ (SSE)"""
    if task_id not in task_logs:
        raise HTTPException(status_code=404, detail="Task not found")
    
    async def generate():
        last_log_index = 0
        while True:
            if task_id in task_logs and len(task_logs[task_id]) > last_log_index:
                new_logs = task_logs[task_id][last_log_index:]
                for log_entry in new_logs:
                    yield f"data: {log_entry.json()}\n\n"
                last_log_index = len(task_logs[task_id])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –ª–∏ –∑–∞–¥–∞—á–∞
            if task_id in tasks_storage and tasks_storage[task_id].status in ["completed", "failed"]:
                break
                
            await asyncio.sleep(1)
    
    return StreamingResponse(generate(), media_type="text/event-stream")

@app.get("/status/{task_id}")
async def get_task_status(task_id: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏"""
    if task_id not in tasks_storage:
        raise HTTPException(status_code=404, detail="Task not found")
    
    task = tasks_storage[task_id]
    result = {
        "task_id": task.task_id,
        "status": task.status,
        "progress": task.progress,
        "message": task.message,
        "created_at": task.created_at,
        "updated_at": task.updated_at,
        "total_slugs": task.total_slugs,
        "completed_slugs": task.completed_slugs,
        "failed_slugs": task.failed_slugs,
        "current_slug": task.current_slug,
        "results": task.results,
        "metrics": task.metrics
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –±–∏–ª–¥–∞, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
    if task_id in build_states:
        result["build_state"] = build_states[task_id]

    if task_id in task_logs:
        result["logs"] = [log.dict() for log in task_logs[task_id][-200:]]
    
    return result

async def execute_parse_task(task_id: str, slug: str, parser: str):
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞—á—É –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–¥–Ω–æ–π –º–∞–Ω–≥–∏"""
    try:
        # –°–ò–ù–ò–ô –õ–û–ì: –ù–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
        logger.info(f"\033[94müîç Starting parsing: {slug}\033[0m")
        
        ensure_utf8_patch()
        ensure_cross_device_patch()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞—á–∞–ª–∞
        update_task_status(task_id, "IMPORTING_MANGA", 5, "–ü—Ä–∏–º–µ–Ω–µ–Ω—ã –ø–∞—Ç—á–∏")
        
        command = ["python", "main.py", "parse", slug, "-skip-images", "--use", parser]
        result = await run_melon_command(command, task_id, timeout=1800)
        
        metrics_payload = result.get("metrics") if isinstance(result, dict) else None

        if result["success"]:
            update_task_status(task_id, "IMPORTING_MANGA", 80, "–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç...")
            
            # –í–ê–ñ–ù–û: –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º slug –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ JSON —Ñ–∞–π–ª–∞
            # MelonService —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∞–π–ª—ã –ë–ï–ó ID: "sweet-home-kim-carnby-.json"
            # –ù–æ slug –º–æ–∂–µ—Ç –ø—Ä–∏–π—Ç–∏ —Å ID: "3754--sweet-home-kim-carnby-"
            normalized_slug = slug
            if "--" in slug:
                parts = slug.split("--", 1)
                if len(parts) == 2 and parts[0].isdigit():
                    normalized_slug = parts[1]
                    logger.info(f"üîß –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è slug –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ JSON: '{slug}' ‚Üí '{normalized_slug}'")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–∑–¥–∞–ª—Å—è –ª–∏ JSON —Ñ–∞–π–ª (—Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º slug)
            json_path = get_melon_base_path() / "Output" / parser / "titles" / f"{normalized_slug}.json"
            logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ JSON —Ñ–∞–π–ª–∞: {json_path}")
            
            if json_path.exists():
                logger.info(f"‚úÖ JSON —Ñ–∞–π–ª –Ω–∞–π–¥–µ–Ω: {json_path}")
                # –ß–∏—Ç–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–∞–Ω–≥–µ
                with open(json_path, 'r', encoding='utf-8') as f:
                    manga_data = json.load(f)

                update_task_status(
                    task_id,
                    "COMPLETED",
                    100,
                    "–ü–∞—Ä—Å–∏–Ω–≥ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω",
                    {
                        "filename": normalized_slug,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π slug
                        "title": manga_data.get("localized_name") or manga_data.get("eng_name") or manga_data.get("title", ""),
                        "chapters": sum(len(chapters) for chapters in manga_data.get("content", {}).values()),
                        "branches": len(manga_data.get("content", {})),
                        "metrics": metrics_payload
                    },
                    metrics=metrics_payload
                )
            else:
                logger.error(f"‚ùå JSON —Ñ–∞–π–ª –ù–ï –Ω–∞–π–¥–µ–Ω: {json_path}")
                # –õ–æ–≥–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                titles_dir = get_melon_base_path() / "Output" / parser / "titles"
                if titles_dir.exists():
                    available_files = [f.stem for f in titles_dir.glob("*.json")]
                    logger.error(f"üìÇ –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã: {available_files}")
                update_task_status(task_id, "FAILED", 100, "–ü–∞—Ä—Å–∏–Ω–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω, –Ω–æ JSON —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        else:
            update_task_status(
                task_id,
                "FAILED",
                100,
                f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {result.get('stderr', 'Unknown error')}",
                {
                    "slug": slug,
                    "metrics": metrics_payload
                },
                metrics=metrics_payload
            )

    except Exception as e:
        update_task_status(task_id, "FAILED", 100, f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")

async def execute_build_task(task_id: str, slug: str, parser: str, target_language: str = None, build_type: str = "simple"):
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞—á—É –±–∏–ª–¥–∞ –æ–¥–Ω–æ–π –º–∞–Ω–≥–∏"""
    try:
        # –°–ò–ù–ò–ô –õ–û–ì: –ü–µ—Ä–µ—Ö–æ–¥ –æ—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫ –±–∏–ª–¥–∏–Ω–≥—É
        logger.info(f"\033[94müî® Parsing completed ‚Üí Starting build: {slug}\033[0m")
        
        update_task_status(task_id, "IMPORTING_MANGA", 10, "–ù–∞—á–∏–Ω–∞–µ–º –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∞—Ä—Ö–∏–≤–∞...")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—á –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º build –∫–æ–º–∞–Ω–¥—ã
        ensure_cross_device_patch()
        
        # –í–ê–ñ–ù–û: –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º slug (—É–±–∏—Ä–∞–µ–º ID, –µ—Å–ª–∏ –µ—Å—Ç—å)
        # MelonService —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∞–π–ª—ã –ë–ï–ó ID: "sweet-home-kim-carnby-.json"
        # –ù–æ MangaService –º–æ–∂–µ—Ç –ø–µ—Ä–µ–¥–∞—Ç—å slug —Å ID: "3754--sweet-home-kim-carnby-"
        normalized_slug = slug
        if "--" in slug:
            parts = slug.split("--", 1)
            if len(parts) == 2 and parts[0].isdigit():
                normalized_slug = parts[1]
                logger.info(f"üîß –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è slug –¥–ª—è –±–∏–ª–¥–∞: '{slug}' ‚Üí '{normalized_slug}'")
        
        # –ö–æ–º–∞–Ω–¥–∞ –±–∏–ª–¥–∞ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º slug
        command = ["python", "main.py", "build-manga", normalized_slug, "--use", parser]
        
        if build_type == "simple":
            command.append("-simple")
        elif build_type == "zip":
            command.append("-zip")
        elif build_type == "cbz":
            command.append("-cbz")
        
        result = await run_melon_command(command, task_id, timeout=1800)
        metrics_payload = result.get("metrics") if isinstance(result, dict) else None
        
        if result["success"]:
            logger.info(f"[{task_id}] BUILD SUCCESS: {slug} completed ({build_type})")
            update_task_status(
                task_id,
                "COMPLETED",
                100,
                f"–ê—Ä—Ö–∏–≤ —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω ({build_type})",
                {
                    "filename": slug,
                    "archive_type": build_type,
                    "metrics": metrics_payload
                },
                metrics=metrics_payload
            )
        else:
            error_msg = f"–û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è: {result.get('stderr', 'Unknown error')}"
            logger.error(f"[{task_id}] BUILD FAILED: {slug} - {error_msg}")
            
            update_task_status(
                task_id,
                "FAILED",
                100,
                error_msg,
                {
                    "slug": slug,
                    "metrics": metrics_payload
                },
                metrics=metrics_payload
            )

    except Exception as e:
        logger.error(f"Critical error in build task {task_id}: {str(e)}")
        update_task_status(task_id, "FAILED", 100, f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")


@app.post("/tasks/{task_id}/cancel")
async def cancel_task(task_id: str):
    """–û—Ç–º–µ–Ω–∞ –≤—ã–ø–æ–ª–Ω—è—é—â–µ–π—Å—è –∑–∞–¥–∞—á–∏"""
    if task_id not in tasks_storage:
        raise HTTPException(status_code=404, detail="Task not found")

    task = tasks_storage[task_id]

    if task.status in ["COMPLETED", "FAILED", "CANCELLED"]:
        return {
            "cancelled": False,
            "status": task.status,
            "message": "Task already finished"
        }

    process = await get_running_process(task_id)
    if process:
        await terminate_process(process)

    log_task_message(task_id, "INFO", "‚ö†Ô∏è –ó–∞–¥–∞—á–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    update_task_status(task_id, "CANCELLED", task.progress, "–ó–∞–¥–∞—á–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º", error="Cancelled by user")

    if task_id in build_states:
        del build_states[task_id]

    return {"cancelled": True, "status": "CANCELLED"}

@app.get("/list-parsed")
async def list_parsed_manga():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –º–∞–Ω–≥"""
    try:
        output_path = get_melon_base_path() / "Output"
        parsed_manga = []
        
        for parser_dir in output_path.iterdir():
            if parser_dir.is_dir():
                titles_dir = parser_dir / "titles"
                if titles_dir.exists():
                    for json_file in titles_dir.glob("*.json"):
                        try:
                            with open(json_file, 'r', encoding='utf-8') as f:
                                manga_data = json.load(f)
                                # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –ø–æ–ª–µ–π
                                title = (manga_data.get("title") or 
                                        manga_data.get("localized_name") or 
                                        manga_data.get("eng_name") or 
                                        manga_data.get("slug", "Unknown"))
                                
                                parsed_manga.append({
                                    "slug": json_file.stem,
                                    "title": title,
                                    "parser": parser_dir.name,
                                    "chapters": sum(len(chapters) for chapters in manga_data.get("content", {}).values()),
                                    "branches": len(manga_data.get("content", {}))
                                })
                        except Exception as e:
                            logger.warning(f"Error reading {json_file}: {str(e)}")
        
        return {"manga_list": parsed_manga}
    
    except Exception as e:
        logger.error(f"Error listing parsed manga: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/delete/{filename}")
async def delete_manga(filename: str):
    """–£–¥–∞–ª–µ–Ω–∏–µ –º–∞–Ω–≥–∏"""
    try:
        output_path = get_melon_base_path() / "Output"
        deleted_items = []
        
        for parser_dir in output_path.iterdir():
            if parser_dir.is_dir():
                # –£–¥–∞–ª—è–µ–º JSON —Ñ–∞–π–ª
                json_file = parser_dir / "titles" / f"{filename}.json"
                if json_file.exists():
                    json_file.unlink()
                    deleted_items.append(f"JSON —Ñ–∞–π–ª: {json_file.name}")
                
                # –£–¥–∞–ª—è–µ–º –ø–∞–ø–∫—É —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
                images_dir = parser_dir / "archives" / filename
                if images_dir.exists():
                    shutil.rmtree(images_dir)
                    deleted_items.append(f"–ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏: {filename}")
        
        if deleted_items:
            logger.info(f"Deleted manga '{filename}': {', '.join(deleted_items)}")
            return {
                "success": True,
                "message": f"–ú–∞–Ω–≥–∞ '{filename}' —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–∞",
                "deleted_items": deleted_items
            }
        else:
            return {
                "success": False,
                "message": f"–ú–∞–Ω–≥–∞ '{filename}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
            }
    
    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –º–∞–Ω–≥–∏ '{filename}': {str(e)}"
        logger.error(error_msg)
        return {"success": False, "message": error_msg}

@app.get("/manga-info/{filename}")
async def get_manga_info(filename: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–∞–Ω–≥–µ"""
    try:
        output_path = get_melon_base_path() / "Output"
        logger.info(f"üîç –ü–æ–∏—Å–∫ manga-info –¥–ª—è filename='{filename}'")
        logger.info(f"üìÇ Output path: {output_path}")
        
        for parser_dir in output_path.iterdir():
            if parser_dir.is_dir():
                json_file = parser_dir / "titles" / f"{filename}.json"
                logger.info(f"üîé –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª: {json_file} (exists={json_file.exists()})")
                
                if json_file.exists():
                    logger.info(f"‚úÖ –§–∞–π–ª –Ω–∞–π–¥–µ–Ω: {json_file}")
                    with open(json_file, 'r', encoding='utf-8') as f:
                        return json.load(f)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        all_files = []
        for parser_dir in output_path.iterdir():
            if parser_dir.is_dir():
                titles_dir = parser_dir / "titles"
                if titles_dir.exists():
                    all_files.extend([f.stem for f in titles_dir.glob("*.json")])
        
        logger.error(f"‚ùå –§–∞–π–ª '{filename}.json' –Ω–µ –Ω–∞–π–¥–µ–Ω. –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã: {all_files}")
        raise HTTPException(status_code=404, detail=f"–ú–∞–Ω–≥–∞ '{filename}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ã–µ: {all_files}")
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting manga info for {filename}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/catalog/{page}")
async def get_catalog(page: int, parser: str = "mangalib", limit: int = 60):
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ slug'–æ–≤ –º–∞–Ω–≥ –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞ MangaLib –ø–æ –Ω–æ–º–µ—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    
    Args:
        page: –ù–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞ (–Ω–∞—á–∏–Ω–∞—è —Å 1)
        parser: –ü–∞—Ä—Å–µ—Ä (mangalib, slashlib, hentailib)
        limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–∞–Ω–≥ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 60)
    
    Returns:
        JSON —Å–æ —Å–ø–∏—Å–∫–æ–º slug'–æ–≤ –º–∞–Ω–≥
    """
    try:
        logger.info(f"Fetching catalog page {page} for {parser}, limit: {limit}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º Site-Id
        site_ids = {
            "mangalib": "1",
            "slashlib": "2", 
            "hentailib": "4"
        }
        site_id = site_ids.get(parser, "1")
        
        # –ó–∞–ø—Ä–æ—Å –∫ MangaLib API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–∞—Ç–∞–ª–æ–≥–∞
        # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∫–∞–∫ –≤ –ø–∞—Ä—Å–µ—Ä–µ: fields[]=value&fields[]=value2
        api_url = f"https://api.cdnlibs.org/api/manga?fields[]=rate_avg&fields[]=rate&fields[]=releaseDate&page={page}"
        
        # –ò–º–∏—Ç–∏—Ä—É–µ–º –Ω–∞—Å—Ç–æ—è—â–∏–π –±—Ä–∞—É–∑–µ—Ä —Å –ø–æ–ª–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        headers = {
            "Site-Id": site_id,
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Origin": f"https://{parser}.me",
            "Referer": f"https://{parser}.me/manga-list",
            "Sec-Fetch-Dest": "empty",
            "Sec-Fetch-Mode": "cors",
            "Sec-Fetch-Site": "cross-site",
            "Sec-Ch-Ua": '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
            "Sec-Ch-Ua-Mobile": "?0",
            "Sec-Ch-Ua-Platform": '"Windows"'
        }
        
        # –ó–∞–ø—Ä–æ—Å –±–µ–∑ params, –≤—Å—ë –≤ URL (–∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–æ—Ç–∞—Ü–∏—é –ø—Ä–æ–∫—Å–∏)
        current_proxy = get_proxy_for_request()
        response = requests.get(api_url, headers=headers, proxies=current_proxy, timeout=30)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        logger.debug(f"Request URL: {response.url}")
        
        response.raise_for_status()
        
        data = response.json()
        logger.debug(f"API Response keys: {data.keys() if isinstance(data, dict) else 'not a dict'}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–∞–Ω–≥
        manga_list = data.get("data", [])
        if not manga_list and isinstance(data, list):
            manga_list = data
        
        meta = data.get("meta", {})
        total = meta.get("total", meta.get("total_results", 0))
        per_page = meta.get("per_page", len(manga_list))
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ slug'–æ–≤
        slugs = []
        for manga in manga_list[:limit]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ limit —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            # MangaLib –∏–∑–º–µ–Ω–∏–ª —Å—Ç—Ä—É–∫—Ç—É—Ä—É URL: —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è slug_url (—Ñ–æ—Ä–º–∞—Ç: ID--slug)
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: slug_url > slug > eng_name
            slug = manga.get("slug_url", manga.get("slug", manga.get("eng_name", "")))
            if slug:
                slugs.append(slug)
        
        logger.info(f"Successfully fetched {len(slugs)} manga slugs from page {page} (total in response: {len(manga_list)})")
        
        return {
            "success": True,
            "page": page,
            "parser": parser,
            "limit": limit,
            "per_page": per_page,
            "total": total,
            "count": len(slugs),
            "slugs": slugs
        }
        
    except requests.exceptions.Timeout:
        error_msg = f"Timeout fetching catalog page {page}"
        logger.error(error_msg)
        return {"success": False, "error": error_msg}
    except requests.exceptions.RequestException as e:
        error_msg = f"Request error fetching catalog: {str(e)}"
        logger.error(error_msg)
        return {"success": False, "error": error_msg}
    except Exception as e:
        error_msg = f"Error fetching catalog page {page}: {str(e)}"
        logger.error(error_msg)
        return {"success": False, "error": error_msg}

@app.get("/manga-info/{slug}/chapters-only")
async def get_chapters_metadata_only_endpoint(slug: str, parser: str = "mangalib"):
    try:
        site_ids = {
            "mangalib": "1",
            "slashlib": "2", 
            "hentailib": "4"
        }
        site_id = site_ids.get(parser, "1")
        
        # –ü—Ä—è–º–æ–π –∑–∞–ø—Ä–æ—Å –∫ MangaLib API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –≥–ª–∞–≤
        api_url = f"https://api.cdnlibs.org/api/manga/{slug}/chapters"
        
        # –ò–º–∏—Ç–∏—Ä—É–µ–º –Ω–∞—Å—Ç–æ—è—â–∏–π –±—Ä–∞—É–∑–µ—Ä
        headers = {
            "Site-Id": site_id,
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Origin": f"https://{parser}.me",
            "Referer": f"https://{parser}.me/{slug}",
            "Sec-Fetch-Dest": "empty",
            "Sec-Fetch-Mode": "cors",
            "Sec-Fetch-Site": "cross-site",
            "Sec-Ch-Ua": '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
            "Sec-Ch-Ua-Mobile": "?0",
            "Sec-Ch-Ua-Platform": '"Windows"'
        }
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–æ—Ç–∞—Ü–∏—é –ø—Ä–æ–∫—Å–∏
        current_proxy = get_proxy_for_request()
        response = requests.get(api_url, headers=headers, proxies=current_proxy, timeout=30)
        
        if response.status_code == 200:
            data = response.json().get("data", [])
            
            chapters = []
            for chapter_data in data:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –≤–µ—Ç–∫–∏ –≥–ª–∞–≤—ã
                for branch_data in chapter_data.get("branches", []):
                    chapter_info = {
                        "volume": chapter_data.get("volume"),
                        "number": chapter_data.get("number"),
                        "name": chapter_data.get("name", ""),
                        "id": branch_data.get("id"),
                        "branch_id": branch_data.get("branch_id")
                    }
                    chapters.append(chapter_info)
                    
                    # –ó–∞–ø–æ–ª–Ω—è–µ–º –∫–µ—à –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                    chapter_id = str(branch_data.get("id"))
                    chapters_metadata_cache[chapter_id] = chapter_info
            
            logger.info(f"Successfully retrieved {len(chapters)} chapters metadata for slug: {slug}")
            logger.debug(f"Cached metadata for {len(chapters)} chapters")
            
            return {
                "success": True,
                "slug": slug,
                "parser": parser,
                "total_chapters": len(chapters),
                "chapters": chapters
            }
        else:
            error_msg = f"MangaLib API returned status {response.status_code}"
            logger.error(f"Error getting chapters metadata: {error_msg}")
            return {
                "success": False,
                "error": error_msg,
                "status_code": response.status_code
            }
            
    except requests.Timeout:
        error_msg = "Request to MangaLib API timed out"
        logger.error(error_msg)
        return {
            "success": False,
            "error": error_msg
        }
    except Exception as e:
        error_msg = f"Error getting chapters metadata: {str(e)}"
        logger.error(error_msg)
        return {
            "success": False,
            "error": error_msg
        }

@app.get("/images/{filename}/{chapter}/{page}")
async def get_image(filename: str, chapter: str, page: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    try:
        output_path = get_melon_base_path() / "Output"
        
        for parser_dir in output_path.iterdir():
            if parser_dir.is_dir():
                manga_dir = parser_dir / "archives" / filename
                if manga_dir.exists():
                    # –ò—â–µ–º –ø–∞–ø–∫—É –≥–ª–∞–≤—ã, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –Ω–æ–º–µ—Ä–∞ –≥–ª–∞–≤—ã
                    chapter_dir = None
                    for potential_dir in manga_dir.iterdir():
                        if potential_dir.is_dir():
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å–Ω–∞—á–∞–ª–∞
                            if potential_dir.name == chapter:
                                chapter_dir = potential_dir
                                break
                            # –ü–æ—Ç–æ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ —Å –Ω–æ–º–µ—Ä–∞ –≥–ª–∞–≤—ã
                            elif potential_dir.name.startswith(f"{chapter}.") or potential_dir.name.startswith(f"{chapter} "):
                                chapter_dir = potential_dir
                                break
                    
                    if chapter_dir:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –ø–∞–ø–∫–µ –≥–ª–∞–≤—ã
                        for ext in ['.png', '.jpg', '.jpeg', '.webp']:
                            image_path = chapter_dir / f"{page}{ext}"
                            if image_path.exists():
                                return FileResponse(
                                    path=str(image_path),
                                    media_type=f"image/{ext[1:]}",
                                    headers={
                                        "Cache-Control": "public, max-age=3600",
                                        "Access-Control-Allow-Origin": "*"
                                    }
                                )
                
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ archives, –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤ images
                manga_dir = parser_dir / "images" / filename
                if manga_dir.exists():
                    # –ò—â–µ–º –ø–∞–ø–∫—É –≥–ª–∞–≤—ã, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –Ω–æ–º–µ—Ä–∞ –≥–ª–∞–≤—ã
                    chapter_dir = None
                    for potential_dir in manga_dir.iterdir():
                        if potential_dir.is_dir():
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å–Ω–∞—á–∞–ª–∞
                            if potential_dir.name == chapter:
                                chapter_dir = potential_dir
                                break
                            # –ü–æ—Ç–æ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ª–∏ —Å –Ω–æ–º–µ—Ä–∞ –≥–ª–∞–≤—ã
                            elif potential_dir.name.startswith(f"{chapter}.") or potential_dir.name.startswith(f"{chapter} "):
                                chapter_dir = potential_dir
                                break
                    
                    if chapter_dir:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –ø–∞–ø–∫–µ –≥–ª–∞–≤—ã
                        for ext in ['.png', '.jpg', '.jpeg', '.webp']:
                            image_path = chapter_dir / f"{page}{ext}"
                            if image_path.exists():
                                return FileResponse(
                                    path=str(image_path),
                                    media_type=f"image/{ext[1:]}",
                                    headers={
                                        "Cache-Control": "public, max-age=3600",
                                        "Access-Control-Allow-Origin": "*"
                                    }
                                )
        
        raise HTTPException(status_code=404, detail=f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {filename}/{chapter}/{page}")
    
    except Exception as e:
        logger.error(f"Error serving image {filename}/{chapter}/{page}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/cover/{filename}")
async def get_cover(filename: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±–ª–æ–∂–∫–∏ –º–∞–Ω–≥–∏"""
    try:
        output_path = get_melon_base_path() / "Output"
        
        for parser_dir in output_path.iterdir():
            if parser_dir.is_dir():
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—É—Ç–∏ –∫ –æ–±–ª–æ–∂–∫–µ
                possible_paths = [
                    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø—É—Ç–∏ (–¥–ª—è —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞)
                    parser_dir / "archives" / filename / "cover.jpg",
                    parser_dir / "archives" / filename / "cover.png",
                    parser_dir / "archives" / filename / "cover.webp",
                    parser_dir / "images" / filename / "cover.jpg",
                    parser_dir / "images" / filename / "cover.png",
                    parser_dir / "images" / filename / "cover.webp",
                ]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø—É—Ç–∏
                for cover_path in possible_paths:
                    if cover_path.exists():
                        return FileResponse(
                            path=str(cover_path),
                            media_type=f"image/{cover_path.suffix[1:]}",
                            headers={
                                "Cache-Control": "public, max-age=86400",  # –ö—ç—à –Ω–∞ —Å—É—Ç–∫–∏
                                "Access-Control-Allow-Origin": "*"
                            }
                        )
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–ø–∫—É covers —Å UUID —Ñ–∞–π–ª–∞–º–∏
                covers_dir = parser_dir / "images" / filename / "covers"
                if covers_dir.exists():
                    # –ò—â–µ–º –ø–µ—Ä–≤—ã–π —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø–∞–ø–∫–µ covers
                    for cover_file in covers_dir.glob("*"):
                        if cover_file.is_file() and cover_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp']:
                            return FileResponse(
                                path=str(cover_file),
                                media_type=f"image/{cover_file.suffix[1:]}",
                                headers={
                                    "Cache-Control": "public, max-age=86400",  # –ö—ç—à –Ω–∞ —Å—É—Ç–∫–∏
                                    "Access-Control-Allow-Origin": "*"
                                }
                            )
        
        raise HTTPException(status_code=404, detail=f"–û–±–ª–æ–∂–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–ª—è –º–∞–Ω–≥–∏: {filename}")
    
    except Exception as e:
        logger.error(f"Error serving cover for {filename}: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8084)