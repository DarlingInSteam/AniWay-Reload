from Source.Core.Base.Formats.Manga import Branch, Chapter, Types
from Source.Core.Base.Parsers.MangaParser import MangaParser
from Source.Core.Base.Formats.BaseFormat import Statuses

from dublib.Methods.Data import RemoveRecurringSubstrings, Zerotify
from dublib.WebRequestor import WebRequestor

from datetime import datetime
from time import sleep
from typing import Optional

# –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
from .parallel_downloader import AdaptiveParallelDownloader

class Parser(MangaParser):
    def amend(self, branch: Branch, chapter: Chapter):
        """
        –î–æ–ø–æ–ª–Ω—è–µ—Ç –≥–ª–∞–≤—É –¥–∞–π–Ω—ã–º–∏ –æ —Å–ª–∞–π–¥–∞—Ö.
            branch ‚Äì –¥–∞–Ω–Ω—ã–µ –≤–µ—Ç–≤–∏;
            chapter ‚Äì –¥–∞–Ω–Ω—ã–µ –≥–ª–∞–≤—ã.
        """

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–ª–∞–≤ –∏ —Ç–µ–∫—É—â–∏–π –∏–Ω–¥–µ–∫—Å
        total_chapters = sum(len(b.chapters) for b in self._Title.branches)
        current_chapter_index = 0
        
        # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å —Ç–µ–∫—É—â–µ–π –≥–ª–∞–≤—ã
        for b in self._Title.branches:
            for ch in b.chapters:
                current_chapter_index += 1
                if ch.id == chapter.id:
                    break
            else:
                continue
            break
        
        # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≥–ª–∞–≤—ã —á–µ—Ä–µ–∑ Portals
        self._Portals.chapter_parsing_start(self._Title, chapter, current_chapter_index, total_chapters)

        import time
        start_time = time.time()
        Slides = self.__GetSlides(branch.id, chapter)
        parse_time = time.time() - start_time
        
        # –°–ò–ù–ò–ô –õ–û–ì: –ü—Ä–æ–≥—Ä–µ—Å—Å –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        chapter_name = f"Vol.{chapter.volume} " if chapter.volume else ""
        chapter_name += f"Ch.{chapter.number}"
        if chapter.name:
            chapter_name += f": {chapter.name}"
        
        slides_count = len(Slides)
        self._SystemObjects.logger.info(f"\033[94müîç [{current_chapter_index}/{total_chapters}] {chapter_name} - {slides_count} pages ({parse_time:.2f}s)\033[0m")

        for Slide in Slides:
            chapter.add_slide(Slide["link"], Slide["width"], Slide["height"])

        # –ù–û–í–û–ï: –í—ã–≤–æ–¥–∏–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≥–ª–∞–≤—ã
        slides_count = len(Slides)
        print(f"[{current_chapter_index}/{total_chapters}] Chapter {chapter_name} completed ({slides_count} slides)")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≥–ª–∞–≤—ã —á–µ—Ä–µ–∑ Portals
        self._Portals.chapter_parsing_complete(self._Title, chapter, current_chapter_index, total_chapters, slides_count)

    def _InitializeRequestor(self) -> WebRequestor:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥—É–ª—å WEB-–∑–∞–ø—Ä–æ—Å–æ–≤."""

        WebRequestorObject = super()._InitializeRequestor()

        # –î–æ–±–∞–≤–ª—è–µ–º –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω –µ—Å–ª–∏ –µ—Å—Ç—å
        if self._Settings.custom["token"]:
            WebRequestorObject.config.add_header("Authorization", self._Settings.custom["token"])

        # PROXY ROTATION SUPPORT:
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏:
        # 1. ProxyRotator –∏–∑ settings.json (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω –∏ –µ—Å—Ç—å –ø—Ä–æ–∫—Å–∏)
        # 2. –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è HTTP_PROXY/HTTPS_PROXY
        # 3. –ë–µ–∑ –ø—Ä–æ–∫—Å–∏

        import sys
        import os
        from pathlib import Path

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ MelonService –≤ sys.path –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ proxy_rotator
        melon_service_path = Path(__file__).parent.parent.parent
        if str(melon_service_path) not in sys.path:
            sys.path.insert(0, str(melon_service_path))

        try:
            from proxy_rotator import ProxyRotator

            # –°–æ–∑–¥–∞—ë–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Ä–æ—Ç–∞—Ç–æ—Ä–∞ –¥–ª—è –ø–∞—Ä—Å–µ—Ä–∞
            rotator = ProxyRotator(parser="mangalib")

            if rotator.enabled and rotator.get_proxy_count() > 0:
                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–∫—Å–∏ (—Å —Ä–æ—Ç–∞—Ü–∏–µ–π –µ—Å–ª–∏ –∏—Ö –Ω–µ—Å–∫–æ–ª—å–∫–æ)
                if rotator.get_proxy_count() == 1:
                    proxy_dict = rotator.get_current_proxy()
                    print(f"[INFO] üîí Single proxy mode (no rotation): {rotator.get_proxy_count()} proxy")
                else:
                    proxy_dict = rotator.get_next_proxy()
                    print(f"[INFO] üîÑ Proxy rotation enabled: {rotator.get_proxy_count()} proxies, strategy={rotator.rotation_strategy}")

                if proxy_dict:
                    try:
                        if hasattr(WebRequestorObject, '_WebRequestor__Session'):
                            WebRequestorObject._WebRequestor__Session.proxies.update(proxy_dict)
                            print(f"[INFO] ‚úÖ Proxy configured via ProxyRotator")
                        elif hasattr(WebRequestorObject, 'session'):
                            WebRequestorObject.session.proxies.update(proxy_dict)
                            print(f"[INFO] ‚úÖ Proxy configured via ProxyRotator (public session)")
                    except Exception as e:
                        print(f"[WARNING] ‚ö†Ô∏è  Failed to set proxy from ProxyRotator: {e}")
            else:
                print(f"[INFO] ‚ÑπÔ∏è  ProxyRotator disabled, checking environment variables...")

                # Fallback: –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
                http_proxy = os.getenv("HTTP_PROXY") or os.getenv("http_proxy")
                https_proxy = os.getenv("HTTPS_PROXY") or os.getenv("https_proxy")

                if http_proxy or https_proxy:
                    proxies = {}
                    if http_proxy:
                        proxies['http'] = http_proxy
                    if https_proxy:
                        proxies['https'] = https_proxy

                    try:
                        if hasattr(WebRequestorObject, '_WebRequestor__Session'):
                            WebRequestorObject._WebRequestor__Session.proxies.update(proxies)
                            print(f"[INFO] ‚úÖ Proxy configured from env vars: {http_proxy or https_proxy}")
                        elif hasattr(WebRequestorObject, 'session'):
                            WebRequestorObject.session.proxies.update(proxies)
                            print(f"[INFO] ‚úÖ Proxy configured from env vars (public session)")
                    except Exception as e:
                        print(f"[WARNING] ‚ö†Ô∏è  Failed to configure proxy from env: {e}")
                else:
                    print(f"[INFO] ‚ÑπÔ∏è  No proxy configured (direct connection)")

        except ImportError as e:
            print(f"[WARNING] ‚ö†Ô∏è  ProxyRotator not available: {e}")
            print(f"[INFO] ‚ÑπÔ∏è  Falling back to environment variables...")

            # Fallback –Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
            http_proxy = os.getenv("HTTP_PROXY") or os.getenv("http_proxy")
            https_proxy = os.getenv("HTTPS_PROXY") or os.getenv("https_proxy")

            if http_proxy or https_proxy:
                proxies = {}
                if http_proxy:
                    proxies['http'] = http_proxy
                if https_proxy:
                    proxies['https'] = https_proxy

                try:
                    if hasattr(WebRequestorObject, '_WebRequestor__Session'):
                        WebRequestorObject._WebRequestor__Session.proxies.update(proxies)
                        print(f"[INFO] ‚úÖ Proxy configured from env vars: {http_proxy or https_proxy}")
                    elif hasattr(WebRequestorObject, 'session'):
                        WebRequestorObject.session.proxies.update(proxies)
                        print(f"[INFO] ‚úÖ Proxy configured from env vars (public)")
                except Exception as e:
                    print(f"[WARNING] ‚ö†Ô∏è  Failed to configure proxy: {e}")

        return WebRequestorObject

    def _download_image_wrapper(self, url: str) -> str | None:
        """Thread-safe –æ–±–µ—Ä—Ç–∫–∞ —Å –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–π —Å–µ—Å—Å–∏–µ–π requests –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ç–æ–∫–∞.

        :param url: URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        :return: –ò–º—è —Ñ–∞–π–ª–∞ –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, None –µ—Å–ª–∏ –æ—à–∏–±–∫–∞
        """
        import os
        from pathlib import Path
        import hashlib
        import requests
        from urllib.parse import urlparse, unquote

        directory = self._SystemObjects.temper.parser_temp
        os.makedirs(directory, exist_ok=True)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL —Å —É—á—ë—Ç–æ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
        parsed_url = urlparse(url)
        decoded_path = unquote(parsed_url.path or "")
        trimmed_path = decoded_path.rstrip("/")
        path_obj = Path(trimmed_path) if trimmed_path else Path("")

        resolved_suffix = path_obj.suffix if trimmed_path else ""
        resolved_name = path_obj.stem if trimmed_path else ""

        if not resolved_name or resolved_name in {".", ".."}:
            candidate_name = path_obj.name if trimmed_path else ""
            if candidate_name not in {"", ".", ".."}:
                resolved_name = Path(candidate_name).stem
            else:
                resolved_name = ""

        if not resolved_suffix and trimmed_path:
            resolved_suffix = Path(path_obj.name).suffix

        if not resolved_name:
            resolved_name = hashlib.sha1(url.encode("utf-8")).hexdigest()

        image_filename = f"{resolved_name}{resolved_suffix}"
        image_path = os.path.join(directory, image_filename)

        # –ï—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –Ω–µ FORCE_MODE, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–º—è
        if os.path.exists(image_path) and not self._SystemObjects.FORCE_MODE:
            return image_filename

        try:
            # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π WebRequestor
            requestor = self._ImagesDownloader._ImagesDownloader__Requestor

            # –°–æ–∑–¥–∞–µ–º –ù–ï–ó–ê–í–ò–°–ò–ú–£–Æ —Å–µ—Å—Å–∏—é requests –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ—Ç–æ–∫–∞
            session = requests.Session()

            # –ö–æ–ø–∏—Ä—É–µ–º cookies –∏–∑ WebRequestor Session (thread-safe read)
            source_session = None
            if hasattr(requestor, '_WebRequestor__Session'):
                source_session = requestor._WebRequestor__Session
            elif hasattr(requestor, 'session'):
                source_session = requestor.session
            elif hasattr(requestor, '_session'):
                source_session = requestor._session

            if source_session and hasattr(source_session, 'cookies'):
                try:
                    # –ö–†–ò–¢–ò–ß–ù–û: –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º update() –Ω–∞–ø—Ä—è–º—É—é - —ç—Ç–æ –≤—ã–∑—ã–≤–∞–µ—Ç deadlock!
                    cookies_dict = dict(source_session.cookies)
                    for name, value in cookies_dict.items():
                        session.cookies.set(name, value)
                except Exception:
                    pass

            # –ö–æ–ø–∏—Ä—É–µ–º headers –∏–∑ source session
            if source_session and hasattr(source_session, 'headers'):
                try:
                    headers_dict = dict(source_session.headers)
                    session.headers.update(headers_dict)
                except Exception:
                    pass

            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ headers –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9,ru;q=0.8',
                'Referer': 'https://mangalib.me/',
            })

            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–∫—Å–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            proxies = None
            if hasattr(self, '_ProxyRotator') and self._ProxyRotator:
                proxy = self._ProxyRotator.get_next_proxy()
                if proxy and isinstance(proxy, dict):
                    proxies = proxy

            # –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô HTTP –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –Ω–µ–∑–∞–≤–∏—Å–∏–º—É—é —Å–µ—Å—Å–∏—é!
            # stream=True –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç IncompleteRead –Ω–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–∞—Ö
            response = session.get(url, timeout=30, proxies=proxies, stream=True)

            if response.status_code == 200:
                # –ß–∏—Ç–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —á–∞—Å—Ç—è–º, –∑–∞—â–∏—Ç–∞ –æ—Ç IncompleteRead
                content = b""
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        content += chunk

                if len(content) > 1000:
                    with open(image_path, "wb") as f:
                        f.write(content)
                    return image_filename

        except Exception as e:
            # –¢–∏—Ö–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—à–∏–±–∫–∏, retry –º–µ—Ö–∞–Ω–∏–∑–º –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç
            pass
        finally:
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–µ—Å—Å–∏—é
            if 'session' in locals():
                session.close()

        return None

    def _get_scaled_delay(
        self,
        base_value: float,
        *,
        baseline_proxies: int = 5,
        minimum: float = 0.02,
        maximum: Optional[float] = None
    ) -> float:
        """–ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –∑–∞–¥–µ—Ä–∂–∫—É –ø–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏."""

        proxies = getattr(self, "_proxy_count_cache", None)
        if not isinstance(proxies, int) or proxies <= 0:
            proxies = self._get_proxy_count()

        proxies = max(1, proxies)
        baseline = max(1, baseline_proxies)
        scale = baseline / proxies
        scaled = base_value * scale

        if maximum is not None:
            scaled = min(scaled, maximum)

        if scaled < minimum:
            return minimum

        return scaled

    def _get_common_delay(self) -> float:
        cached = getattr(self, "_common_delay", None)
        if cached is None:
            self._common_delay = self._get_scaled_delay(
                getattr(self._Settings.common, "delay", 0.25),
                minimum=0.05
            )
            return self._common_delay
        return cached

    def _get_parse_delay(self) -> float:
        cached = getattr(self, "_parse_delay", None)
        if cached is None:
            self._parse_delay = self._get_scaled_delay(
                getattr(self._Settings.common, "parse_delay", 0.15),
                minimum=0.04
            )
            return self._parse_delay
        return cached

    def _get_image_delay(self) -> float:
        cached = getattr(self, "_image_delay", None)
        if cached is None:
            self._image_delay = self._get_scaled_delay(
                getattr(self._Settings.common, "image_delay", 0.1),
                minimum=0.03
            )
            return self._image_delay
        return cached

    def _PostInitMethod(self):
        """–ú–µ—Ç–æ–¥, –≤—ã–ø–æ–ª–Ω—è—é—â–∏–π—Å—è –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–∞."""
        
        print(f"[CRITICAL_DEBUG] _PostInitMethod() CALLED!", flush=True)

        self.__TitleSlug = None
        self.__API = "api.cdnlibs.org"
        self.__Sites = {
            "mangalib.me": 1,
            "slashlib.me": 2,
            "hentailib.me": 4
        }
        
        print(f"[CRITICAL_DEBUG] About to initialize AdaptiveParallelDownloader...", flush=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ProxyRotator –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        self._ProxyRotator = None
        try:
            import sys
            from pathlib import Path
            
            melon_service_path = Path(__file__).parent.parent.parent
            if str(melon_service_path) not in sys.path:
                sys.path.insert(0, str(melon_service_path))
            
            from proxy_rotator import ProxyRotator
            rotator = ProxyRotator(parser="mangalib")
            
            if rotator.enabled and rotator.get_proxy_count() > 0:
                self._ProxyRotator = rotator
                print(f"[INFO] ‚úÖ ProxyRotator initialized: {rotator.get_proxy_count()} proxies", flush=True)
        except Exception as e:
            print(f"[WARNING] ProxyRotator not available: {e}", flush=True)
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –≤ __init__, –∞ –Ω–µ –≤ parse()
        # –ü–æ—Ç–æ–º—É —á—Ç–æ build –º–æ–∂–µ—Ç –≤—ã–∑—ã–≤–∞—Ç—å—Å—è –±–µ–∑ parse (–∫–æ–≥–¥–∞ JSON —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
        proxy_count = self._get_proxy_count()
        self._common_delay = self._get_common_delay()
        self._parse_delay = self._get_parse_delay()
        self._image_delay = self._get_image_delay()
        image_delay = self._image_delay

        max_workers_override = getattr(self._Settings.common, 'image_max_workers', None)
        if max_workers_override is None:
            try:
                import os
                env_override = os.getenv("MELON_IMAGE_MAX_WORKERS")
                if env_override is not None:
                    max_workers_override = int(env_override)
            except ValueError:
                print(f"[WARNING] Invalid MELON_IMAGE_MAX_WORKERS value, skipping override", flush=True)

        print(
            f"[CRITICAL_DEBUG] proxy_count={proxy_count}, image_delay={image_delay}, override={max_workers_override}",
            flush=True
        )
        
        # –ù–ï –ù–£–ñ–ï–ù Lock ‚Äî –∫–∞–∂–¥—ã–π –ø–æ—Ç–æ–∫ —Å–æ–∑–¥–∞–µ—Ç —Å–≤–æ—é —Å–µ—Å—Å–∏—é requests!
        self._parallel_downloader = AdaptiveParallelDownloader(
            proxy_count=proxy_count,
            download_func=self._download_image_wrapper,
            max_workers_per_proxy=2,
            max_retries=3,
            base_delay=image_delay,
            max_total_workers=max_workers_override
        )
        
        print(f"[CRITICAL_DEBUG] AdaptiveParallelDownloader CREATED successfully!", flush=True)
        
        # –ö–µ—à–∏—Ä—É–µ–º —Å–µ—Ä–≤–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        self._cached_image_server = None

    def __IsSlideLink(self, link: str, servers: list[str]) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –≤–µ–¥—ë—Ç –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å–ª–∞–π–¥.
            link ‚Äì —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ;
            servers ‚Äì —Å–ø–∏—Å–æ–∫ —Å–µ—Ä–≤–µ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
        """

        for Server in servers:
            if Server in link: return True

        return False
    
    def __ParseSlideLink(self, link: str, servers: list[str]) -> tuple[str]:
        """
        –ü–∞—Ä—Å–∏—Ç —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–∞–π–¥.
            link ‚Äì —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ;
            servers ‚Äì —Å–ø–∏—Å–æ–∫ —Å–µ—Ä–≤–µ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
        """

        OriginalServer = None
        URI = None

        for Server in servers:

            if Server in link:
                OriginalServer = Server
                URI = link.replace(OriginalServer, "")

        return (OriginalServer, URI)

    #==========================================================================================#
    # >>>>> –ü–†–ò–í–ê–¢–ù–´–ï –ú–ï–¢–û–î–´ –ü–ê–†–°–ò–ù–ì–ê <<<<< #
    #==========================================================================================#

    def __CheckCorrectDomain(self, data: dict) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –≤–æ–∑—Ä–∞—Å—Ç–Ω–æ–π —Ä–µ–π—Ç–∏–Ω–≥.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Domain = self._Manifest.site

        if self._Title.site:

            if data["site"] != self.__GetSiteID(self._Title.site):
                Domain = self.__GetSiteDomain(data["site"])
                self._Portals.warning(f"Title site changed to \"{Domain}\".")

        return Domain 

    def __GetAgeLimit(self, data: dict) -> int:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –≤–æ–∑—Ä–∞—Å—Ç–Ω–æ–π —Ä–µ–π—Ç–∏–Ω–≥.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Rating = None
        RatingString = data["ageRestriction"]["label"].split(" ")[0].replace("+", "").replace("–ù–µ—Ç", "")
        if RatingString.isdigit(): Rating = int(RatingString)

        return Rating 

    def __GetAuthors(self, data: dict) -> list[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∞–≤—Ç–æ—Ä–æ–≤."""

        Authors = list()
        for Author in data["authors"]: Authors.append(Author["name"])

        return Authors

    def __GetBranches(self) -> list[Branch]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–∞–π—Ç–ª–∞."""

        Branches: dict[int, Branch] = dict()
        Response = self._Requestor.get(f"https://{self.__API}/api/manga/{self.__TitleSlug}/chapters")
        
        if Response.status_code == 200:
            Data = Response.json["data"]
            sleep(self._get_common_delay())

            for CurrentChapterData in Data:

                for BranchData in CurrentChapterData["branches"]:
                    BranchID = BranchData["branch_id"]
                    if BranchID == None: BranchID = int(str(self._Title.id) + "0")
                    if BranchID not in Branches.keys(): Branches[BranchID] = Branch(BranchID)

                    ChapterObject = Chapter(self._SystemObjects)
                    ChapterObject.set_id(BranchData["id"])
                    ChapterObject.set_volume(CurrentChapterData["volume"])
                    ChapterObject.set_number(CurrentChapterData["number"])
                    ChapterObject.set_name(CurrentChapterData["name"])
                    ChapterObject.set_is_paid("restricted_view" in BranchData and not BranchData["restricted_view"]["is_open"])
                    ChapterObject.set_workers([sub["name"] for sub in BranchData["teams"]])
                    ChapterObject.add_extra_data("moderated", False if "moderation" in BranchData.keys() else True)

                    if self._Settings.custom["add_free_publication_date"] and ChapterObject.is_paid:
                        ChapterObject.add_extra_data("free-publication-date", BranchData["restricted_view"]["expired_at"])

                    Branches[BranchID].add_chapter(ChapterObject)

        else: self._Portals.request_error(Response, "Unable to request chapter.", exception = False)

        for CurrentBranch in Branches.values(): self._Title.add_branch(CurrentBranch)

    def __GetCovers(self, data: dict) -> list[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±–ª–æ–∂–µ–∫."""

        Covers = list()

        if data["cover"]:
            Covers.append({
                "link": data["cover"]["default"],
                "filename": data["cover"]["default"].split("/")[-1]
            })

        if self._Settings.common.sizing_images:
            Covers[0]["width"] = None
            Covers[0]["height"] = None

        return Covers

    def __GetDescription(self, data: dict) -> str | None:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Description = None
        if "summary" in data.keys(): Description = RemoveRecurringSubstrings(data["summary"], "\n").strip().replace(" \n", "\n")
        Description = Zerotify(Description)

        return Description

    def __GetFranchises(self, data: dict) -> list[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–µ—Ä–∏–π.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Franchises = list()
        for Franchise in data["franchise"]: Franchises.append(Franchise["name"])
        if "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–∞–±–æ—Ç—ã" in Franchises: Franchises.remove("–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–∞–±–æ—Ç—ã")

        return Franchises

    def __GetGenres(self, data: dict) -> list[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∂–∞–Ω—Ä–æ–≤.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Genres = list()
        for Genre in data["genres"]: Genres.append(Genre["name"])

        return Genres

    def __GetImagesServers(self, server_id: str | None = None, all_sites: bool = False) -> list[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–º–µ–Ω–æ–≤ —Å–µ—Ä–≤–µ—Ä–æ–≤ —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
            server_id ‚Äì –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–µ—Ä–≤–µ—Ä–∞;\n
            all_sites ‚Äì —É–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤–µ—Ä–Ω—É—Ç—å –Ω—É–∂–Ω–æ –¥–æ–º–µ–Ω—ã —Ö—Ä–∞–Ω–∏–ª–∏—â –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤.
        """

        Servers = list()
        CurrentSiteID = self.__GetSiteID()
        URL = f"https://{self.__API}/api/constants?fields[]=imageServers"
        Headers = {
            "Authorization": self._Settings.custom["token"],
            "Referer": f"https://{self._Manifest.site}/"
        }
        Response = self._Requestor.get(URL, headers = Headers)

        if Response.status_code == 200:
            Data = Response.json["data"]["imageServers"]
            sleep(self._get_common_delay())

            for ServerData in Data:

                if server_id:
                    if ServerData["id"] == server_id and CurrentSiteID in ServerData["site_ids"]: Servers.append(ServerData["url"])
                    elif ServerData["id"] == server_id and all_sites: Servers.append(ServerData["url"])

                else:
                    if CurrentSiteID in ServerData["site_ids"] or all_sites: Servers.append(ServerData["url"])

        else:
            self._Portals.request_error(Response, "Unable to request site constants.")

        return Servers

    def __GetSiteDomain(self, id: str) -> int | None:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ–º–µ–Ω —Å–∞–π—Ç–∞.
            id ‚Äì —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–∞–π—Ç–∞.
        """

        SiteDomain = None

        for Domain, ID in self.__Sites.items():
            if ID == id: SiteDomain = Domain
        
        return SiteDomain

    def __GetSiteID(self, site: str = None) -> int | None:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–∞–π—Ç–∞.
            site ‚Äì –¥–æ–º–µ–Ω —Å–∞–π—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –ø–∞—Ä—Å–µ—Ä–æ–º).
        """

        if not site: site = self._Manifest.site
        SiteID = None

        for Domain in self.__Sites.keys():
            if Domain in site: SiteID = self.__Sites[Domain]
        
        return SiteID

    def __GetSlides(self, branch_id: int, chapter: Chapter) -> list[dict]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Å–ª–∞–π–¥–∞—Ö –≥–ª–∞–≤—ã.
            branch_id ‚Äì –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≤–µ—Ç–≤–∏;\n
            chapter ‚Äì –¥–∞–Ω–Ω—ã–µ –≥–ª–∞–≤—ã.
        """

        Slides = list()

        if "moderated" in chapter.to_dict().keys() and not chapter["moderated"]:
            self._Portals.chapter_skipped(self._Title, chapter, comment = "Not moderated.")
            return Slides
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –≤–º–µ—Å—Ç–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        if self._cached_image_server is None:
            self._cached_image_server = self.__GetImagesServers(self._Settings.custom["server"])[0]
        Server = self._cached_image_server

        parse_delay = self._get_parse_delay()

        token = None
        custom_settings = getattr(self._Settings, "custom", None)
        if custom_settings is not None:
            try:
                token = custom_settings["token"]
            except KeyError:
                token = None
        headers: dict[str, str] = {
            "Referer": f"https://{self._Manifest.site}/"
        }

        site_id = self.__GetSiteID()
        if site_id is not None:
            headers["Site-Id"] = str(site_id)

        if token:
            headers["Authorization"] = token

        default_branch_id = int(str(self._Title.id) + "0")
        branch_query_value = branch_id if branch_id and branch_id != default_branch_id else None

        base_endpoint = f"https://{self.__API}/api/manga/{self.__TitleSlug}/chapter"
        url_variants: list[str] = []

        query_params: list[str] = []
        if chapter.number:
            query_params.append(f"number={chapter.number}")
        if chapter.volume:
            query_params.append(f"volume={chapter.volume}")
        if branch_query_value:
            query_params.append(f"branch_id={branch_query_value}")

        if query_params:
            url_variants.append(f"{base_endpoint}?{'&'.join(query_params)}")

        if chapter.id:
            branch_suffix = f"?branch_id={branch_query_value}" if branch_query_value else ""
            url_variants.append(f"{base_endpoint}/{chapter.id}{branch_suffix}")

            id_query_params = [f"chapter_id={chapter.id}"]
            if branch_query_value:
                id_query_params.append(f"branch_id={branch_query_value}")
            url_variants.append(f"{base_endpoint}?{'&'.join(id_query_params)}")

            generic_id_query = [f"id={chapter.id}"]
            if branch_query_value:
                generic_id_query.append(f"branch_id={branch_query_value}")
            url_variants.append(f"{base_endpoint}?{'&'.join(generic_id_query)}")

        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –Ω–æ–º–µ—Ä/—Ç–æ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç
        if not url_variants:
            fallback_params = []
            if branch_query_value:
                fallback_params.append(f"branch_id={branch_query_value}")
            if chapter.id:
                fallback_params.append(f"id={chapter.id}")
            if fallback_params:
                url_variants.append(f"{base_endpoint}?{'&'.join(fallback_params)}")

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        url_variants = list(dict.fromkeys(url_variants))

        last_error: str | None = None
        last_status: int | None = None
        last_response = None

        retryable_statuses = {408, 409, 423, 425, 429, 500, 502, 503, 504}
        max_retry_attempts = getattr(self._Settings.common, "chapter_retry_attempts", 3)
        initial_retry_delay = getattr(self._Settings.common, "chapter_retry_delay", 2.0)
        retry_backoff_factor = getattr(self._Settings.common, "chapter_retry_backoff", 2.0)

        def extract_error_message(response) -> str | None:
            if response is None:
                return None
            try:
                payload = response.json
            except Exception:
                payload = None

            if isinstance(payload, dict):
                for key in ("message", "error", "detail", "reason"):
                    value = payload.get(key)
                    if isinstance(value, str) and value.strip():
                        return value.strip()

                errors_obj = payload.get("errors")
                if isinstance(errors_obj, dict):
                    first_error = next((str(v) for v in errors_obj.values() if v), None)
                    if first_error:
                        return first_error

            if hasattr(response, "text"):
                text_value = response.text
                if isinstance(text_value, str):
                    snippet = text_value.strip()
                    if snippet:
                        return snippet[:200]
            return None

        for url in url_variants:
            attempt = 0
            while True:
                Response = self._Requestor.get(url, headers=headers if headers else None)
                last_response = Response
                last_status = Response.status_code

                if Response.status_code == 200:
                    break

                error_message = extract_error_message(Response)
                last_error = f"HTTP {Response.status_code}"
                if error_message:
                    last_error = f"HTTP {Response.status_code}: {error_message}"

                if Response.status_code in retryable_statuses and attempt < max_retry_attempts:
                    wait_seconds = initial_retry_delay * (retry_backoff_factor ** attempt)
                    wait_seconds = max(wait_seconds, 1.5)
                    wait_seconds = min(wait_seconds, 30.0)
                    self._SystemObjects.logger.warning(
                        f"Chapter {chapter.id} request to {url} returned {Response.status_code}. "
                        f"Retrying in {wait_seconds:.1f}s (attempt {attempt + 1}/{max_retry_attempts}).")
                    sleep(wait_seconds)
                    attempt += 1
                    continue

                break

            if Response.status_code != 200:
                continue

            Payload = Response.json
            Data = Payload.get("data") if isinstance(Payload, dict) else None

            if not isinstance(Data, dict):
                last_error = "No data in response"
                continue

            Pages = Data.get("pages")
            if not Pages:
                last_error = "Empty pages list"
                continue

            sleep(parse_delay)

            for SlideIndex, Page in enumerate(Pages, start=1):
                RelativeURL = Page.get("url")
                if not RelativeURL:
                    continue

                Buffer = {
                    "index": SlideIndex,
                    "link": Server + RelativeURL.replace(" ", "%20"),
                    "width": Page.get("width"),
                    "height": Page.get("height")
                }
                Slides.append(Buffer)

            if Slides:
                self._Portals.chapter_download_start(self._Title, chapter, len(Pages), len(Pages))
                break

        if Slides:
            return Slides

        reason_parts: list[str] = []
        if chapter.is_paid:
            reason_parts.append("–ø–ª–∞—Ç–Ω–∞—è –≥–ª–∞–≤–∞ ‚Äî –¥–æ—Å—Ç—É–ø –æ–≥—Ä–∞–Ω–∏—á–µ–Ω")
        if last_error:
            reason_parts.append(last_error)
        elif last_status:
            reason_parts.append(f"HTTP {last_status}")
        else:
            reason_parts.append("–∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–µ –≤–µ—Ä–Ω—É–ª —Å—Ç—Ä–∞–Ω–∏—Ü—ã")

        reason_comment = "; ".join(reason_parts)
        chapter.add_extra_data("empty_reason", reason_comment)
        self._SystemObjects.logger.warning(f"Chapter {chapter.id} returned no slides ({reason_comment}).")
        if last_response and last_response.status_code != 200:
            self._Portals.request_error(last_response, "Unable to request chapter content.", exception=False)
        self._Portals.chapter_skipped(self._Title, chapter, comment=reason_comment)

        return Slides

    def __GetStatus(self, data: dict) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç—É—Å.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Status = None
        StatusesDetermination = {
            1: Statuses.ongoing,
            2: Statuses.completed,
            3: Statuses.announced,
            4: Statuses.dropped,
            5: Statuses.dropped
        }
        SiteStatusIndex = data["status"]["id"]
        if SiteStatusIndex in StatusesDetermination.keys(): Status = StatusesDetermination[SiteStatusIndex]

        return Status

    def __GetTitleData(self) -> dict | None:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ç–∞–π—Ç–ª–∞.
            slug ‚Äì –∞–ª–∏–∞—Å.
        """
        
        URL = f"https://{self.__API}/api/manga/{self.__TitleSlug}?fields[]=eng_name&fields[]=otherNames&fields[]=summary&fields[]=releaseDate&fields[]=type_id&fields[]=caution&fields[]=genres&fields[]=tags&fields[]=franchise&fields[]=authors&fields[]=manga_status_id&fields[]=status_id"
        
        # DEBUG: –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        print(f"[DEBUG] üîç Requesting title data for: {self.__TitleSlug}")
        print(f"[DEBUG] üåê URL: {URL}")
        
        Response = self._Requestor.get(URL)
        
        # DEBUG: –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        print(f"[DEBUG] üì° Response status: {Response.status_code}")
        if hasattr(Response, 'text'):
            response_preview = Response.text[:500] if hasattr(Response.text, '__getitem__') else str(Response.text)[:500]
            print(f"[DEBUG] üìÑ Response preview: {response_preview}")

        if Response.status_code == 200:
            Response = Response.json["data"]
            sleep(self._get_common_delay())

        elif Response.status_code == 451: 
            self._Portals.request_error(Response, "Account banned.")
            return None
        elif Response.status_code == 404: 
            self._Portals.title_not_found(self._Title)
            return None
        else: 
            self._Portals.request_error(Response, "Unable to request title data.")
            return None

        return Response

    def __GetTags(self, data: dict) -> list[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–≥–æ–≤.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Tags = list()
        for Tag in data["tags"]: Tags.append(Tag["name"])

        return Tags

    def __GetType(self, data: dict) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ç–∏–ø —Ç–∞–π—Ç–ª–∞.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Type = None

        TypeData = data.get("type") or {}
        SiteTypeLabel = TypeData.get("label") or ""
        SiteTypeId = TypeData.get("id")

        TypeById = {
            9: Types.western_comic,   # –ö–æ–º–∏–∫—Å
            4: Types.oel,             # OEL-–º–∞–Ω–≥–∞
            8: Types.russian_comic,   # –†—É–º–∞–Ω–≥–∞
        }

        TypeByLabel = {
            "–ú–∞–Ω–≥–∞": Types.manga,
            "–ú–∞–Ω—Ö–≤–∞": Types.manhwa,
            "–ú–∞–Ω—å—Ö—É–∞": Types.manhua,
            "–†—É–º–∞–Ω–≥–∞": Types.russian_comic,
            "–ö–æ–º–∏–∫—Å": Types.western_comic,
            "–ö–æ–º–∏–∫—Å –∑–∞–ø–∞–¥–Ω—ã–π": Types.western_comic,
            "OEL-–º–∞–Ω–≥–∞": Types.oel,
        }

        if SiteTypeId in TypeById:
            Type = TypeById[SiteTypeId]
        else:
            NormalizedLabel = SiteTypeLabel.strip()
            if NormalizedLabel in TypeByLabel:
                Type = TypeByLabel[NormalizedLabel]

        return Type

    def __StringToDate(self, date_str: str) -> datetime:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –≤—Ä–µ–º—è –≤ –æ–±—ä–µ–∫—Ç–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é.
            date_str ‚Äì —Å—Ç—Ä–æ–∫–æ–≤–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è.
        """

        DatePattern = "%Y-%m-%dT%H:%M:%S.%fZ"

        return datetime.strptime(date_str, DatePattern)

    #==========================================================================================#
    # >>>>> –ü–£–ë–õ–ò–ß–ù–´–ï –ú–ï–¢–û–î–´ <<<<< #
    #==========================================================================================#

    def amend(self, branch: Branch, chapter: Chapter):
        """
        –î–æ–ø–æ–ª–Ω—è–µ—Ç –≥–ª–∞–≤—É –¥–∞–π–Ω—ã–º–∏ –æ —Å–ª–∞–π–¥–∞—Ö.
            branch ‚Äì –¥–∞–Ω–Ω—ã–µ –≤–µ—Ç–≤–∏;\n
            chapter ‚Äì –¥–∞–Ω–Ω—ã–µ –≥–ª–∞–≤—ã.
        """

        Slides = self.__GetSlides(branch.id, chapter)
        for Slide in Slides: chapter.add_slide(Slide["link"], Slide["width"], Slide["height"])

    def amend_postprocessor(self, chapter: Chapter):
        """
        –í–Ω–æ—Å–∏—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –≥–ª–∞–≤—É –ø–æ—Å–ª–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –µ—ë –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º. –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è.
        
        –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –¥–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.

        :param chapter: –î–∞–Ω–Ω—ã–µ –≥–ª–∞–≤—ã.
        :type chapter: Chapter
        """

        if not self._Settings.custom["add_moderation_status"]: chapter.remove_extra_data("moderated")

    def collect(self, period: int | None = None, filters: str | None = None, pages: int | None = None) -> list[str]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–∞–π—Ç–ª–æ–≤ –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º.
            period ‚Äì –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤ –¥–æ —Ç–µ–∫—É—â–µ–≥–æ –º–æ–º–µ–Ω—Ç–∞, —Å–æ—Å—Ç–∞–≤–ª—è—é—â–µ–µ –ø–µ—Ä–∏–æ–¥ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö;\n
            filters ‚Äì —Å—Ç—Ä–æ–∫–∞, –æ–ø–∏—Å—ã–≤–∞—é—â–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (–ø–æ–¥—Ä–æ–±–Ω–µ–µ –≤ README.md);\n
            pages ‚Äì –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞.
        """

        Updates = list()
        IsUpdatePeriodOut = False
        Page = 1
        UpdatesCount = 0
        Headers = {
            "Site-Id": str(self.__GetSiteID())
        }
        CurrentDate = datetime.utcnow()

        while not IsUpdatePeriodOut:
            Response = self._Requestor.get(f"https://{self.__API}/api/latest-updates?page={Page}", headers = Headers)

            if Response.status_code == 200:
                UpdatesPage = Response.json["data"]

                for UpdateNote in UpdatesPage:
                    Delta = CurrentDate - self.__StringToDate(UpdateNote["last_item_at"])
                    
                    if Delta.total_seconds() / 3600 <= period:
                        Updates.append(UpdateNote["slug_url"])
                        UpdatesCount += 1

                    else:
                        IsUpdatePeriodOut = True

            else:
                IsUpdatePeriodOut = True
                self._Portals.request_error(Response, f"Unable to request updates page {Page}.")


            if not IsUpdatePeriodOut:
                self._Portals.collect_progress_by_page(Page)
                Page += 1
                sleep(self._get_common_delay())

        return Updates

    def image(self, url: str) -> str | None:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Å–∞–π—Ç–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥ –ø–∞—Ä—Å–µ—Ä–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞.
            url ‚Äì —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.
        """

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π delay –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–º–µ–Ω—å—à–µ —á–µ–º –¥–ª—è API)
        image_delay = self._get_image_delay()

        Result = self._ImagesDownloader.temp_image(url)
        
        # –ï—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞ - –¥–æ–±–∞–≤–ª—è–µ–º delay –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—Ä–æ—Å–æ–º
        if Result:
            sleep(image_delay)
            return Result
        
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å - –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ —Å–µ—Ä–≤–µ—Ä—ã
        Servers = self.__GetImagesServers(all_sites = True)

        if self.__IsSlideLink(url, Servers):
            OriginalServer, ImageURI = self.__ParseSlideLink(url, Servers)
            Servers.remove(OriginalServer)

            for Server in Servers:
                Link = Server + ImageURI
                Result = self._ImagesDownloader.temp_image(Link)
                
                if Result: 
                    sleep(image_delay)
                    break
                elif Server != Servers[-1]: 
                    sleep(image_delay)

        return Result

    def _get_proxy_count(self) -> int:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏."""
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º settings.json (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1)
        if hasattr(self._Settings, 'proxy') and hasattr(self._Settings.proxy, 'enable') and self._Settings.proxy.enable:
            if hasattr(self._Settings.proxy, 'proxies') and self._Settings.proxy.proxies:
                proxy_count = len(self._Settings.proxy.proxies)
                print(f"[INFO] üåê Detected {proxy_count} proxies from settings.json")
                return proxy_count
        
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ ProxyRotator (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2) - –¢–û–¢ –ñ–ï –ö–û–î –ß–¢–û –í _InitializeRequestor
        try:
            import sys
            import os
            from pathlib import Path
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ MelonService –≤ sys.path –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ proxy_rotator
            melon_service_path = Path(__file__).parent.parent.parent
            if str(melon_service_path) not in sys.path:
                sys.path.insert(0, str(melon_service_path))
            
            from proxy_rotator import ProxyRotator
            
            # –°–æ–∑–¥–∞—ë–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Ä–æ—Ç–∞—Ç–æ—Ä–∞ –¥–ª—è –ø–∞—Ä—Å–µ—Ä–∞
            rotator = ProxyRotator(parser="mangalib")
            
            if rotator.enabled and rotator.get_proxy_count() > 0:
                proxy_count = rotator.get_proxy_count()
                print(f"[INFO] üåê Detected {proxy_count} proxies from ProxyRotator")
                return proxy_count
                
        except ImportError:
            pass
        except Exception as e:
            print(f"[WARNING] ‚ö†Ô∏è  Error detecting ProxyRotator: {e}")
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º —á—Ç–æ 1 –ø—Ä–æ–∫—Å–∏ (–∏–ª–∏ –ø—Ä—è–º–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ)
        print(f"[INFO] üåê No proxies detected, using 1 worker")
        return 1
        cached = getattr(self, "_proxy_count_cache", None)
        if isinstance(cached, int) and cached > 0:
            return cached

        from pathlib import Path
        import sys
        import json

        melon_service_path = Path(__file__).parent.parent.parent
        if str(melon_service_path) not in sys.path:
            sys.path.insert(0, str(melon_service_path))

        proxy_count = 0

        try:
            from proxy_rotator import ProxyRotator
            rotator = ProxyRotator(parser="mangalib")
            if rotator.enabled:
                proxy_count = rotator.get_proxy_count()
        except Exception as e:
            print(f"[WARNING] ProxyRotator not available: {e}")

        if proxy_count <= 0:
            try:
                settings_path = Path(__file__).parent / "settings.json"
                if settings_path.exists():
                    with open(settings_path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        prox_list = data.get("Main", {}).get("proxy", [])
                        if isinstance(prox_list, list):
                            proxy_count = len(prox_list)
            except Exception as e:
                print(f"[WARNING] Unable to read proxy count from settings: {e}")

        proxy_count = max(1, proxy_count)
        self._proxy_count_cache = proxy_count
        return proxy_count

    def batch_download_images(self, urls: list[str]) -> list[str | None]:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –±–∞—Ç—á–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
        
        :param urls: –°–ø–∏—Å–æ–∫ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        :return: –°–ø–∏—Å–æ–∫ –∏–º—ë–Ω —Ñ–∞–π–ª–æ–≤ (–∏–ª–∏ None –¥–ª—è –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫) –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ
        """
        
        if not urls:
            return []

        import time

        batch_started_at = time.perf_counter()
        progress_state = {
            "last_log_at": batch_started_at,
            "last_downloaded": 0
        }

        report_every = max(10, len(urls) // 12) if len(urls) > 0 else 1
        min_seconds_between_logs = 15.0

        def progress_callback(downloaded: int, total: int):
            if total <= 0:
                return

            now = time.perf_counter()
            should_log = (
                downloaded == total
                or downloaded - progress_state["last_downloaded"] >= report_every
                or (now - progress_state["last_log_at"]) >= min_seconds_between_logs
            )

            if not should_log:
                return

            percent = (downloaded / total) * 100
            # –£–±–∏—Ä–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω—ã–π debug –ª–æ–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ - –≤–∞–∂–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ MangaBuilder
            progress_state["last_log_at"] = now
            progress_state["last_downloaded"] = downloaded

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        results = self._parallel_downloader.download_batch(urls, progress_callback=progress_callback)

        elapsed = max(time.perf_counter() - batch_started_at, 0.0001)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å–ø–∏—Å–æ–∫ –∏–º—ë–Ω —Ñ–∞–π–ª–æ–≤ (—Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫)
        from urllib.parse import urlparse, unquote

        total_images = len(urls)
        filenames: list[str | None] = [None] * total_images
        failed_indices: list[int] = []
        fallback_attempts = 0
        successful_downloads = 0

        def _expected_filename(target_url: str) -> str:
            parsed = urlparse(target_url)
            raw_name = parsed.path.split('/')[-1]
            return unquote(raw_name)

        for result in results:
            index = result.get('index', 0) - 1
            url = result.get('url')

            if index < 0 or index >= total_images:
                if url:
                    self._SystemObjects.logger.warning(
                        f"Received download result with invalid index: idx={index}, url={url}"
                    )
                continue

            if result.get('success'):
                filenames[index] = result.get('filename')
                successful_downloads += 1
                continue

            # –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å ‚Äî –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–µ—Ä–≤–µ—Ä–∞
            fallback_attempts += 1
            fallback_filename: str | None = None
            fallback_status = self._try_alternative_servers(url)

            if fallback_status:
                if hasattr(fallback_status, "value") and fallback_status.value:
                    fallback_filename = fallback_status.value
                elif isinstance(fallback_status, str):
                    fallback_filename = fallback_status
                elif hasattr(fallback_status, "filename") and fallback_status.filename:
                    fallback_filename = fallback_status.filename

            if not fallback_filename and url:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—è–≤–∏–ª—Å—è –ª–∏ —Ñ–∞–π–ª –≤ temp –ø–æ—Å–ª–µ —Ñ–æ–ª–±—ç–∫–∞
                if self._ImagesDownloader.is_exists(url):
                    fallback_filename = _expected_filename(url)

            if fallback_filename:
                filenames[index] = fallback_filename
                successful_downloads += 1
            else:
                failed_indices.append(index)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –º–µ–¥–ª–µ–Ω–Ω—ã–π —Ñ–æ–ª–±—ç–∫: –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –¥–ª—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è
        if failed_indices:
            slow_delay = max(self._get_image_delay() * 3, 0.6)
            max_additional_attempts = 3

            self._SystemObjects.logger.warning(
                f"Sequential fallback engaged for {len(failed_indices)} images (delay {slow_delay:.2f}s)"
            )

            for idx in failed_indices:
                url = urls[idx]
                recovered = False

                for attempt in range(1, max_additional_attempts + 1):
                    sleep(slow_delay * attempt)
                    sequential_filename = self._download_image_wrapper(url)

                    if sequential_filename:
                        filenames[idx] = sequential_filename
                        successful_downloads += 1
                        recovered = True
                        self._SystemObjects.logger.info(
                            f"‚úÖ Sequential fallback recovered image {idx + 1}/{total_images} (attempt {attempt})"
                        )
                        break

                if not recovered:
                    filenames[idx] = None
                    self._SystemObjects.logger.error(
                        f"‚ùå Unable to recover image {idx + 1}/{total_images} after sequential fallback: {url}"
                    )

        failed_after_fallback = total_images - successful_downloads
        avg_speed = successful_downloads / elapsed if elapsed > 0 else 0

        # –£–±–∏—Ä–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω—ã–π debug –ª–æ–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è - –≤–∞–∂–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ MangaBuilder

        return filenames

    def _try_alternative_servers(self, url: str) -> str | None:
        """–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤ (fallback)."""
        
        Servers = self.__GetImagesServers(all_sites=True)
        
        if self.__IsSlideLink(url, Servers):
            OriginalServer, ImageURI = self.__ParseSlideLink(url, Servers)
            
            try:
                Servers.remove(OriginalServer)
            except ValueError:
                pass
            
            for Server in Servers:
                Link = Server + ImageURI
                Result = self._ImagesDownloader.temp_image(Link)
                
                if Result:
                    return Result
        
        return None

    def parse(self):
        """–ü–æ–ª—É—á–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–∞–π—Ç–ª–∞."""

        print(f"[DEBUG] üöÄ Starting parse() for title: {self._Title.slug}")
        
        self._Requestor.config.add_header("Site-Id", str(self.__Sites[self._Manifest.site]))

        # MangaLib –∏–∑–º–µ–Ω–∏–ª —Å—Ç—Ä—É–∫—Ç—É—Ä—É: —Ç–µ–ø–µ—Ä—å slug'–∏ –∏–∑ API –∏–º–µ—é—Ç —Ñ–æ—Ä–º–∞—Ç "ID--slug"
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ slug ID (—Ñ–æ—Ä–º–∞—Ç: "7580--i-alone-level-up")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º slug –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        slug_with_id = self._Title.slug
        clean_slug = self._Title.slug
        extracted_id = None
        
        if "--" in self._Title.slug:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏ slug –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ "ID--slug"
            parts = self._Title.slug.split("--", 1)
            if len(parts) == 2 and parts[0].isdigit():
                extracted_id = int(parts[0])
                clean_slug = parts[1]
                print(f"[DEBUG] Extracted: ID={extracted_id}, slug={clean_slug}")
        
        # API —Ç—Ä–µ–±—É–µ—Ç –ø–æ–ª–Ω—ã–π slug (ID--slug)
        self.__TitleSlug = slug_with_id
        # –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –±–µ–∑ ID
        self._Title.set_slug(clean_slug)

        print(f"[DEBUG] TitleSlug (API): {self.__TitleSlug}")
        print(f"[DEBUG] Title.slug (file): {self._Title.slug}")

        Data = self.__GetTitleData()
        
        print(f"[DEBUG] üì¶ GetTitleData returned: {type(Data)}, is None: {Data is None}")
        if Data:
            print(f"[DEBUG] ‚úÖ Data keys: {list(Data.keys())[:10] if isinstance(Data, dict) else 'NOT A DICT'}")
        
        self._SystemObjects.manager.get_parser_settings()

        if Data:
            self._Title.set_site(self.__CheckCorrectDomain(Data))
            
            # ID —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–∑ –∏–∑–≤–ª–µ—á—ë–Ω–Ω–æ–≥–æ –∏–ª–∏ –∏–∑ API
            if extracted_id is not None:
                self._Title.set_id(extracted_id)
                if extracted_id != Data["id"]:
                    print(f"[WARNING] ID mismatch: extracted={extracted_id}, API={Data['id']} (using extracted)")
            else:
                self._Title.set_id(Data["id"])
            
            # Slug –£–ñ–ï —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤—ã—à–µ (—á–∏—Å—Ç—ã–π, –±–µ–∑ ID)
            print(f"[DEBUG] Final slug (file): {self._Title.slug}")
            print(f"[DEBUG] Final ID: {self._Title.id}")
            
            self._Title.set_content_language("rus")
            self._Title.set_localized_name(Data["rus_name"])
            self._Title.set_eng_name(Data["eng_name"])
            self._Title.set_another_names(Data["otherNames"])
            if Data["name"] not in Data["otherNames"] and Data["name"] != Data["rus_name"] and Data["name"] != Data["eng_name"]: self._Title.add_another_name(Data["name"])
            self._Title.set_covers(self.__GetCovers(Data))
            self._Title.set_authors(self.__GetAuthors(Data))
            self._Title.set_publication_year(int(Data["releaseDate"]) if Data["releaseDate"] else None)
            self._Title.set_description(self.__GetDescription(Data))
            self._Title.set_age_limit(self.__GetAgeLimit(Data))
            self._Title.set_type(self.__GetType(Data))
            self._Title.set_status(self.__GetStatus(Data))
            self._Title.set_is_licensed(Data["is_licensed"])
            self._Title.set_genres(self.__GetGenres(Data))
            self._Title.set_tags(self.__GetTags(Data))
            self._Title.set_franchises(self.__GetFranchises(Data))

            self.__GetBranches()