from Source.Core.Base.Formats.Manga import Branch, Chapter, Types
from Source.Core.Base.Parsers.MangaParser import MangaParser
from Source.Core.Base.Formats.BaseFormat import Statuses

from dublib.Methods.Data import RemoveRecurringSubstrings, Zerotify
from dublib.WebRequestor import WebRequestor

import random
from datetime import datetime
from threading import Lock
from time import sleep
from typing import Optional
from email.utils import parsedate_to_datetime

# –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
from .parallel_downloader import AdaptiveParallelDownloader

class Parser(MangaParser):
    def amend(self, branch: Branch, chapter: Chapter):
        """
        –î–æ–ø–æ–ª–Ω—è–µ—Ç –≥–ª–∞–≤—É –¥–∞–π–Ω—ã–º–∏ –æ —Å–ª–∞–π–¥–∞—Ö.
            branch ‚Äì –¥–∞–Ω–Ω—ã–µ –≤–µ—Ç–≤–∏;
            chapter ‚Äì –¥–∞–Ω–Ω—ã–µ –≥–ª–∞–≤—ã.
        """

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–ª–∞–≤ –∏ —Ç–µ–∫—É—â–∏–π –∏–Ω–¥–µ–∫—Å
        total_chapters = sum(len(b.chapters) for b in self._Title.branches)
        current_chapter_index = 0
        
        # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å —Ç–µ–∫—É—â–µ–π –≥–ª–∞–≤—ã
        for b in self._Title.branches:
            for ch in b.chapters:
                current_chapter_index += 1
                if ch.id == chapter.id:
                    break
            else:
                continue
            break
        
        # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≥–ª–∞–≤—ã —á–µ—Ä–µ–∑ Portals
        self._Portals.chapter_parsing_start(self._Title, chapter, current_chapter_index, total_chapters)

        import time
        start_time = time.time()
        Slides = self.__GetSlides(branch.id, chapter)
        parse_time = time.time() - start_time
        
        # –°–ò–ù–ò–ô –õ–û–ì: –ü—Ä–æ–≥—Ä–µ—Å—Å –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        chapter_name = f"Vol.{chapter.volume} " if chapter.volume else ""
        chapter_name += f"Ch.{chapter.number}"
        if chapter.name:
            chapter_name += f": {chapter.name}"
        
        slides_count = len(Slides)
        self._SystemObjects.logger.info(f"\033[94müîç [{current_chapter_index}/{total_chapters}] {chapter_name} - {slides_count} pages ({parse_time:.2f}s)\033[0m")

        for Slide in Slides:
            chapter.add_slide(Slide["link"], Slide["width"], Slide["height"])

        # –ù–û–í–û–ï: –í—ã–≤–æ–¥–∏–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≥–ª–∞–≤—ã
        slides_count = len(Slides)
        print(f"[{current_chapter_index}/{total_chapters}] Chapter {chapter_name} completed ({slides_count} slides)")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≥–ª–∞–≤—ã —á–µ—Ä–µ–∑ Portals
        self._Portals.chapter_parsing_complete(self._Title, chapter, current_chapter_index, total_chapters, slides_count)

    def _InitializeRequestor(self) -> WebRequestor:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥—É–ª—å WEB-–∑–∞–ø—Ä–æ—Å–æ–≤."""

        WebRequestorObject = super()._InitializeRequestor()

        # –î–æ–±–∞–≤–ª—è–µ–º –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω –µ—Å–ª–∏ –µ—Å—Ç—å
        if self._Settings.custom["token"]:
            WebRequestorObject.config.add_header("Authorization", self._Settings.custom["token"])

        # PROXY ROTATION SUPPORT:
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏:
        # 1. ProxyRotator –∏–∑ settings.json (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω –∏ –µ—Å—Ç—å –ø—Ä–æ–∫—Å–∏)
        # 2. –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è HTTP_PROXY/HTTPS_PROXY
        # 3. –ë–µ–∑ –ø—Ä–æ–∫—Å–∏

        import sys
        import os
        from pathlib import Path

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ MelonService –≤ sys.path –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ proxy_rotator
        melon_service_path = Path(__file__).parent.parent.parent
        if str(melon_service_path) not in sys.path:
            sys.path.insert(0, str(melon_service_path))

        try:
            from proxy_rotator import ProxyRotator

            # –°–æ–∑–¥–∞—ë–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Ä–æ—Ç–∞—Ç–æ—Ä–∞ –¥–ª—è –ø–∞—Ä—Å–µ—Ä–∞
            rotator = ProxyRotator(parser="mangalib")

            if rotator.enabled and rotator.get_proxy_count() > 0:
                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–∫—Å–∏ (—Å —Ä–æ—Ç–∞—Ü–∏–µ–π –µ—Å–ª–∏ –∏—Ö –Ω–µ—Å–∫–æ–ª—å–∫–æ)
                if rotator.get_proxy_count() == 1:
                    proxy_dict = rotator.get_current_proxy()
                    print(f"[INFO] üîí Single proxy mode (no rotation): {rotator.get_proxy_count()} proxy")
                else:
                    proxy_dict = rotator.get_next_proxy()
                    print(f"[INFO] üîÑ Proxy rotation enabled: {rotator.get_proxy_count()} proxies, strategy={rotator.rotation_strategy}")

                if proxy_dict:
                    try:
                        if hasattr(WebRequestorObject, '_WebRequestor__Session'):
                            WebRequestorObject._WebRequestor__Session.proxies.update(proxy_dict)
                            print(f"[INFO] ‚úÖ Proxy configured via ProxyRotator")
                        elif hasattr(WebRequestorObject, 'session'):
                            WebRequestorObject.session.proxies.update(proxy_dict)
                            print(f"[INFO] ‚úÖ Proxy configured via ProxyRotator (public session)")
                    except Exception as e:
                        print(f"[WARNING] ‚ö†Ô∏è  Failed to set proxy from ProxyRotator: {e}")
            else:
                print(f"[INFO] ‚ÑπÔ∏è  ProxyRotator disabled, checking environment variables...")

                # Fallback: –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
                http_proxy = os.getenv("HTTP_PROXY") or os.getenv("http_proxy")
                https_proxy = os.getenv("HTTPS_PROXY") or os.getenv("https_proxy")

                if http_proxy or https_proxy:
                    proxies = {}
                    if http_proxy:
                        proxies['http'] = http_proxy
                    if https_proxy:
                        proxies['https'] = https_proxy

                    try:
                        if hasattr(WebRequestorObject, '_WebRequestor__Session'):
                            WebRequestorObject._WebRequestor__Session.proxies.update(proxies)
                            print(f"[INFO] ‚úÖ Proxy configured from env vars: {http_proxy or https_proxy}")
                        elif hasattr(WebRequestorObject, 'session'):
                            WebRequestorObject.session.proxies.update(proxies)
                            print(f"[INFO] ‚úÖ Proxy configured from env vars (public session)")
                    except Exception as e:
                        print(f"[WARNING] ‚ö†Ô∏è  Failed to configure proxy from env: {e}")
                else:
                    print(f"[INFO] ‚ÑπÔ∏è  No proxy configured (direct connection)")

        except ImportError as e:
            print(f"[WARNING] ‚ö†Ô∏è  ProxyRotator not available: {e}")
            print(f"[INFO] ‚ÑπÔ∏è  Falling back to environment variables...")

            # Fallback –Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
            http_proxy = os.getenv("HTTP_PROXY") or os.getenv("http_proxy")
            https_proxy = os.getenv("HTTPS_PROXY") or os.getenv("https_proxy")

            if http_proxy or https_proxy:
                proxies = {}
                if http_proxy:
                    proxies['http'] = http_proxy
                if https_proxy:
                    proxies['https'] = https_proxy

                try:
                    if hasattr(WebRequestorObject, '_WebRequestor__Session'):
                        WebRequestorObject._WebRequestor__Session.proxies.update(proxies)
                        print(f"[INFO] ‚úÖ Proxy configured from env vars: {http_proxy or https_proxy}")
                    elif hasattr(WebRequestorObject, 'session'):
                        WebRequestorObject.session.proxies.update(proxies)
                        print(f"[INFO] ‚úÖ Proxy configured from env vars (public)")
                except Exception as e:
                    print(f"[WARNING] ‚ö†Ô∏è  Failed to configure proxy: {e}")

        return WebRequestorObject

    def _download_image_wrapper(self, url: str, proxies: dict | None = None) -> str | None:
        """Thread-safe –æ–±–µ—Ä—Ç–∫–∞ —Å –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–π —Å–µ—Å—Å–∏–µ–π requests –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ç–æ–∫–∞.

        :param url: URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        :return: –ò–º—è —Ñ–∞–π–ª–∞ –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, None –µ—Å–ª–∏ –æ—à–∏–±–∫–∞
        """
        import os
        from pathlib import Path
        import hashlib
        import requests
        from urllib.parse import urlparse, unquote

        directory = self._SystemObjects.temper.parser_temp
        os.makedirs(directory, exist_ok=True)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL —Å —É—á—ë—Ç–æ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
        parsed_url = urlparse(url)
        decoded_path = unquote(parsed_url.path or "")
        trimmed_path = decoded_path.rstrip("/")
        path_obj = Path(trimmed_path) if trimmed_path else Path("")

        resolved_suffix = path_obj.suffix if trimmed_path else ""
        resolved_name = path_obj.stem if trimmed_path else ""

        if not resolved_name or resolved_name in {".", ".."}:
            candidate_name = path_obj.name if trimmed_path else ""
            if candidate_name not in {"", ".", ".."}:
                resolved_name = Path(candidate_name).stem
            else:
                resolved_name = ""

        if not resolved_suffix and trimmed_path:
            resolved_suffix = Path(path_obj.name).suffix

        if not resolved_name:
            resolved_name = hashlib.sha1(url.encode("utf-8")).hexdigest()

        image_filename = f"{resolved_name}{resolved_suffix}"
        image_path = os.path.join(directory, image_filename)

        # –ï—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –Ω–µ FORCE_MODE, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–º—è
        if os.path.exists(image_path) and not self._SystemObjects.FORCE_MODE:
            return image_filename

        proxy_key = None
        # If upstream passed explicit proxies (deterministic assignment from downloader), use them
        if proxies:
            try:
                proxy_key = self._normalize_proxy_key(proxies)
            except Exception:
                proxy_key = None
        else:
            # otherwise try to acquire via rotator (legacy behavior)
            if hasattr(self, '_ProxyRotator') and self._ProxyRotator:
                proxies, proxy_key = self._acquire_proxy()

        try:
            # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π WebRequestor
            requestor = self._ImagesDownloader._ImagesDownloader__Requestor

            # –°–æ–∑–¥–∞–µ–º –ù–ï–ó–ê–í–ò–°–ò–ú–£–Æ —Å–µ—Å—Å–∏—é requests –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ—Ç–æ–∫–∞
            session = requests.Session()
            import time

            # –ö–æ–ø–∏—Ä—É–µ–º cookies –∏–∑ WebRequestor Session (thread-safe read)
            source_session = None
            if hasattr(requestor, '_WebRequestor__Session'):
                source_session = requestor._WebRequestor__Session
            elif hasattr(requestor, 'session'):
                source_session = requestor.session
            elif hasattr(requestor, '_session'):
                source_session = requestor._session

            if source_session and hasattr(source_session, 'cookies'):
                try:
                    # –ö–†–ò–¢–ò–ß–ù–û: –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º update() –Ω–∞–ø—Ä—è–º—É—é - —ç—Ç–æ –≤—ã–∑—ã–≤–∞–µ—Ç deadlock!
                    cookies_dict = dict(source_session.cookies)
                    for name, value in cookies_dict.items():
                        session.cookies.set(name, value)
                except Exception:
                    pass

            # –ö–æ–ø–∏—Ä—É–µ–º headers –∏–∑ source session (–ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫)
            if source_session and hasattr(source_session, 'headers'):
                try:
                    headers_dict = dict(source_session.headers)
                    for hn, hv in headers_dict.items():
                        try:
                            session.headers[hn] = hv
                        except Exception:
                            # –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
                            pass
                except Exception:
                    pass

            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ headers –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9,ru;q=0.8',
                'Referer': 'https://mangalib.me/',
            })

            # –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô HTTP –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –Ω–µ–∑–∞–≤–∏—Å–∏–º—É—é —Å–µ—Å—Å–∏—é!
            # stream=True –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç IncompleteRead –Ω–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–∞—Ö
            request_started_at = time.perf_counter()
            # –£–º–µ–Ω—å—à–∞–µ–º —Ç–∞–π–º–∞—É—Ç—ã: —Å–Ω–∞—á–∞–ª–∞ connect timeout (10s), –∑–∞—Ç–µ–º read timeout (20s)
            response = session.get(url, timeout=(10, 20), proxies=proxies, stream=True)

            if response.status_code == 200:
                # –ü–æ—Ç–æ–∫–æ–≤–æ –ø–∏—à–µ–º —Ñ–∞–π–ª, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å O(n^2) –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏–∏ –±–∞–π—Ç–æ–≤
                total_bytes = 0
                with open(image_path, "wb") as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if not chunk:
                            continue
                        total_bytes += len(chunk)
                        f.write(chunk)

                elapsed = time.perf_counter() - request_started_at
                speed = total_bytes / elapsed if elapsed > 0 else 0

                min_size = getattr(self, "_slow_check_min_size_bytes", 80 * 1024)
                min_speed = getattr(self, "_proxy_min_speed_bytes", 120 * 1024)
                min_duration = getattr(self, "_proxy_min_duration_for_speed", 3.0)

                if (
                    total_bytes >= min_size
                    and elapsed >= min_duration
                    and speed < min_speed
                ):
                    self._record_proxy_failure(
                        proxy_key,
                        reason="slow-throughput",
                        details={"speed": speed, "elapsed": elapsed, "bytes": total_bytes}
                    )
                    self._maybe_log_slow_proxy(proxy_key, speed, elapsed, total_bytes, url)
                    try:
                        os.remove(image_path)
                    except OSError:
                        pass
                    return None

                if total_bytes > 1000:
                    self._record_proxy_success(proxy_key, elapsed, total_bytes)
                    return image_filename

                # –ï—Å–ª–∏ —Ñ–∞–π–ª –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –º–∞–ª–µ–Ω—å–∫–∏–π, —É–¥–∞–ª—è–µ–º –µ–≥–æ –∏ —Å—á–∏—Ç–∞–µ–º –ø–æ–ø—ã—Ç–∫—É –Ω–µ—É—Å–ø–µ—à–Ω–æ–π
                if total_bytes > 0:
                    try:
                        os.remove(image_path)
                    except OSError:
                        pass
                self._record_proxy_failure(proxy_key, reason="tiny-response", details={"bytes": total_bytes, "elapsed": elapsed})
                return None

            self._record_proxy_failure(proxy_key, reason=f"HTTP {response.status_code}")

        except Exception as e:
            # –¢–∏—Ö–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—à–∏–±–∫–∏, retry –º–µ—Ö–∞–Ω–∏–∑–º –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç
            self._record_proxy_failure(proxy_key, reason=str(e))
        finally:
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–µ—Å—Å–∏—é
            if 'session' in locals():
                session.close()

        return None

    def _acquire_proxy(self) -> tuple[dict[str, str] | None, str | None]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä—É (proxy_dict, proxy_key), —É—á–∏—Ç—ã–≤–∞—è –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –±–∞–Ω—ã."""

        rotator = getattr(self, "_ProxyRotator", None)
        if not rotator:
            return None, None

        try:
            total = rotator.get_proxy_count()
        except Exception:
            total = 0

        total = max(1, total)
        fallback_proxy = None
        fallback_key = None

        # Try to fetch full proxy list if available to avoid advancing internal rotator iterator too much
        proxies_list = None
        try:
            proxies_list = rotator.get_all_proxies() or None
        except Exception:
            proxies_list = None

        attempts = min(total, len(proxies_list) if proxies_list else total)
        if attempts <= 0:
            attempts = total

        tried = 0
        while tried < attempts:
            proxy_candidate = None
            try:
                proxy_candidate = rotator.get_next_proxy()
            except Exception:
                proxy_candidate = None

            if not proxy_candidate:
                break

            proxy_key = self._normalize_proxy_key(proxy_candidate)

            if fallback_proxy is None:
                fallback_proxy = proxy_candidate
                fallback_key = proxy_key

            if proxy_key and self._is_proxy_blacklisted(proxy_key):
                tried += 1
                continue

            return proxy_candidate, proxy_key

        return fallback_proxy, fallback_key

    def _normalize_proxy_key(self, proxy: dict[str, str] | None) -> str | None:
        if not proxy:
            return None

        raw_url = proxy.get('http') or proxy.get('https')
        if not raw_url:
            return None

        try:
            from urllib.parse import urlparse
            parsed = urlparse(raw_url)
            host = parsed.hostname or ""
            port = parsed.port
            if not host:
                return raw_url
            return f"{host}:{port}" if port else host
        except Exception:
            return raw_url

    def _is_proxy_blacklisted(self, proxy_key: str | None) -> bool:
        if not proxy_key or not hasattr(self, "_proxy_health_lock"):
            return False

        import time

        with self._proxy_health_lock:
            info = self._proxy_health.get(proxy_key)
            if not info:
                return False

            until = info.get("blacklisted_until")
            if until and until > time.time():
                return True

            if until and until <= time.time():
                info["blacklisted_until"] = None

        return False

    def _record_proxy_success(self, proxy_key: str | None, elapsed: float, total_bytes: int) -> None:
        if not proxy_key or not hasattr(self, "_proxy_health_lock"):
            return

        import time

        min_speed = getattr(self, "_proxy_min_speed_bytes", 120 * 1024)
        min_duration = getattr(self, "_proxy_min_duration_for_speed", 3.0)
        slow_hits_threshold = getattr(self, "_proxy_slow_hits_threshold", 3)
        slow_penalty_seconds = getattr(self, "_proxy_slow_penalty_seconds", 150.0)

        with self._proxy_health_lock:
            info = self._proxy_health.setdefault(proxy_key, {
                "successes": 0,
                "failures": 0,
                "slow_hits": 0,
                "avg_speed": 0.0,
                "blacklisted_until": None
            })

            info["successes"] = info.get("successes", 0) + 1
            info["failures"] = max(info.get("failures", 0) - 1, 0)

            speed = 0.0
            if elapsed > 0 and total_bytes > 0:
                speed = total_bytes / elapsed

            prev_speed = info.get("avg_speed", 0.0)
            window = min(info["successes"], 20)
            info["avg_speed"] = prev_speed + (speed - prev_speed) / max(window, 1)

            info["last_speed"] = speed
            info["last_duration"] = elapsed
            info["last_bytes"] = total_bytes
            info["last_success_at"] = time.time()

            slow_hits = info.get("slow_hits", 0)
            if speed > 0 and speed < min_speed and elapsed >= min_duration:
                slow_hits += 1
            else:
                slow_hits = max(slow_hits - 1, 0)

            info["slow_hits"] = slow_hits

            if (
                slow_hits >= slow_hits_threshold
                and slow_penalty_seconds > 0
            ):
                info["blacklisted_until"] = time.time() + slow_penalty_seconds
                info["slow_hits"] = 0
                label = self._mask_proxy_for_log(proxy_key)
                self._log_proxy_warning(
                    f"üí§ Proxy {label} throttled at {speed/1024:.0f} KB/s ‚Äî pausing for {slow_penalty_seconds:.0f}s"
                )

    def _record_proxy_failure(
        self,
        proxy_key: str | None,
        *,
        reason: str = "",
        details: dict[str, object] | None = None
    ) -> None:
        if not proxy_key or not hasattr(self, "_proxy_health_lock"):
            return

        import time

        failure_threshold = getattr(self, "_proxy_failure_threshold", 2)
        penalty_seconds = getattr(self, "_proxy_failure_penalty_seconds", 300.0)

        with self._proxy_health_lock:
            info = self._proxy_health.setdefault(proxy_key, {
                "successes": 0,
                "failures": 0,
                "slow_hits": 0,
                "avg_speed": 0.0,
                "blacklisted_until": None
            })

            info["failures"] = info.get("failures", 0) + 1
            info["slow_hits"] = 0

            if info["failures"] < failure_threshold:
                return

            info["failures"] = 0
            if penalty_seconds <= 0:
                return

            info["blacklisted_until"] = time.time() + penalty_seconds
            label = self._mask_proxy_for_log(proxy_key)

            extra = ""
            if reason:
                extra = f" ({reason})"
            elif details:
                extra = f" ({details})"

            self._log_proxy_warning(
                f"üö´ Proxy {label} disabled for {penalty_seconds:.0f}s after repeated errors{extra}"
            )

    def _mask_proxy_for_log(self, proxy_key: str | None) -> str:
        if not proxy_key:
            return "<direct>"

        host, sep, port = proxy_key.partition(":")
        parts = host.split(".")
        if len(parts) == 4 and all(segment.isdigit() for segment in parts):
            parts[-1] = "x"
            host = ".".join(parts)
        elif len(host) > 6:
            host = f"{host[:2]}‚Ä¶{host[-2:]}"

        return f"{host}{sep}{port}" if sep else host

    def _log_proxy_warning(self, message: str) -> None:
        logger = getattr(getattr(self, "_SystemObjects", None), "logger", None)
        if logger and hasattr(logger, "warning"):
            logger.warning(message)
        else:
            print(message)

    def _maybe_log_slow_proxy(
        self,
        proxy_key: str | None,
        speed_bytes: float,
        elapsed: float,
        total_bytes: int,
        url: str
    ) -> None:
        if not proxy_key:
            return

        import time

        cooldown = getattr(self, "_slow_retry_logging_cooldown", 45.0)
        now = time.time()
        last_logged = self._proxy_last_warn.get(proxy_key, 0.0)
        if cooldown > 0 and (now - last_logged) < cooldown:
            return

        self._proxy_last_warn[proxy_key] = now

        masked_proxy = self._mask_proxy_for_log(proxy_key)
        speed_kb = speed_bytes / 1024 if speed_bytes else 0.0

        try:
            from urllib.parse import urlparse
            host = urlparse(url).hostname or "?"
        except Exception:
            host = "?"

        message = (
            f"üê¢ Proxy {masked_proxy} delivered {total_bytes/1024:.0f} KB from {host} in {elapsed:.1f}s "
            f"({speed_kb:.0f} KB/s) ‚Äî retrying with a different proxy"
        )
        self._log_proxy_warning(message)

    def _get_scaled_delay(
        self,
        base_value: float,
        *,
        baseline_proxies: int = 5,
        minimum: float = 0.02,
        maximum: Optional[float] = None
    ) -> float:
        """–ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç –∑–∞–¥–µ—Ä–∂–∫—É –ø–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏."""

        proxies = getattr(self, "_proxy_count_cache", None)
        if not isinstance(proxies, int) or proxies <= 0:
            proxies = self._get_proxy_count()

        proxies = max(1, proxies)
        baseline = max(1, baseline_proxies)
        scale = baseline / proxies
        scaled = base_value * scale

        if maximum is not None:
            scaled = min(scaled, maximum)

        if scaled < minimum:
            return minimum

        return scaled

    def _get_common_delay(self) -> float:
        cached = getattr(self, "_common_delay", None)
        if cached is None:
            self._common_delay = self._get_scaled_delay(
                getattr(self._Settings.common, "delay", 0.25),
                minimum=0.05
            )
            return self._common_delay
        return cached

    def _get_parse_delay(self) -> float:
        cached = getattr(self, "_parse_delay", None)
        if cached is None:
            self._parse_delay = self._get_scaled_delay(
                getattr(self._Settings.common, "parse_delay", 0.15),
                minimum=0.04
            )
            return self._parse_delay
        return cached

    def _get_image_delay(self) -> float:
        cached = getattr(self, "_image_delay", None)
        if cached is None:
            self._image_delay = self._get_scaled_delay(
                getattr(self._Settings.common, "image_delay", 0.1),
                minimum=0.03
            )
            return self._image_delay
        return cached

    def _PostInitMethod(self):
        """–ú–µ—Ç–æ–¥, –≤—ã–ø–æ–ª–Ω—è—é—â–∏–π—Å—è –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–∞."""
        
        print(f"[CRITICAL_DEBUG] _PostInitMethod() CALLED!", flush=True)

        self.__TitleSlug = None
        self.__API = "api.cdnlibs.org"
        self.__Sites = {
            "mangalib.me": 1,
            "slashlib.me": 2,
            "hentailib.me": 4
        }
        
        print(f"[CRITICAL_DEBUG] About to initialize AdaptiveParallelDownloader...", flush=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ProxyRotator –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        self._ProxyRotator = None
        try:
            import sys
            from pathlib import Path
            
            melon_service_path = Path(__file__).parent.parent.parent
            if str(melon_service_path) not in sys.path:
                sys.path.insert(0, str(melon_service_path))
            
            from proxy_rotator import ProxyRotator
            rotator = ProxyRotator(parser="mangalib")
            
            if rotator.enabled and rotator.get_proxy_count() > 0:
                self._ProxyRotator = rotator
                print(f"[INFO] ‚úÖ ProxyRotator initialized: {rotator.get_proxy_count()} proxies", flush=True)
        except Exception as e:
            print(f"[WARNING] ProxyRotator not available: {e}", flush=True)
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –≤ __init__, –∞ –Ω–µ –≤ parse()
        # –ü–æ—Ç–æ–º—É —á—Ç–æ build –º–æ–∂–µ—Ç –≤—ã–∑—ã–≤–∞—Ç—å—Å—è –±–µ–∑ parse (–∫–æ–≥–¥–∞ JSON —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
        proxy_count = self._get_proxy_count()
        self._common_delay = self._get_common_delay()
        self._parse_delay = self._get_parse_delay()
        self._image_delay = self._get_image_delay()
        image_delay = self._image_delay

        max_workers_override = getattr(self._Settings.common, 'image_max_workers', None)
        if max_workers_override is None:
            try:
                import os
                env_override = os.getenv("MELON_IMAGE_MAX_WORKERS")
                if env_override is not None:
                    max_workers_override = int(env_override)
            except ValueError:
                print(f"[WARNING] Invalid MELON_IMAGE_MAX_WORKERS value, skipping override", flush=True)

        print(
            f"[CRITICAL_DEBUG] proxy_count={proxy_count}, image_delay={image_delay}, override={max_workers_override}",
            flush=True
        )
        
        # –ù–ï –ù–£–ñ–ï–ù Lock ‚Äî –∫–∞–∂–¥—ã–π –ø–æ—Ç–æ–∫ —Å–æ–∑–¥–∞–µ—Ç —Å–≤–æ—é —Å–µ—Å—Å–∏—é requests!
        proxy_pool = None
        if self._ProxyRotator:
            try:
                proxy_pool = self._ProxyRotator.get_all_proxies()
            except Exception:
                proxy_pool = None

        self._parallel_downloader = AdaptiveParallelDownloader(
            proxy_count=proxy_count,
            download_func=self._download_image_wrapper,
            max_workers_per_proxy=1,
            max_retries=3,
            base_delay=image_delay,
            max_total_workers=max_workers_override,
            proxy_pool=proxy_pool
        )
        
        print(f"[CRITICAL_DEBUG] AdaptiveParallelDownloader CREATED successfully!", flush=True)
        self._proxy_health_lock = Lock()
        self._proxy_health = {}
        # –ë–æ–ª–µ–µ —Ç–µ—Ä–ø–∏–º—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —á—Ç–æ–±—ã –Ω–µ –±–∞–Ω–∏—Ç—å –º–Ω–æ–≥–æ –ø—Ä–æ–∫—Å–∏ —Å—Ä–∞–∑—É
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫ –¥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –±–∞–Ω–∞ (—É–≤–µ–ª–∏—á–µ–Ω–æ —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –±–∞–Ω–∞)
        self._proxy_failure_threshold = 4
        # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –±–∞–Ω–∞ (—É–º–µ–Ω—å—à–µ–Ω–∞ —á—Ç–æ–±—ã –±—ã—Å—Ç—Ä–æ –≤–µ—Ä–Ω—É—Ç—å –ø—Ä–æ–∫—Å–∏ –≤ –ø—É–ª)
        self._proxy_failure_penalty_seconds = 120.0
        # –°–∫–æ–ª—å–∫–æ "–º–µ–¥–ª–µ–Ω–Ω—ã—Ö" –ø–æ–ø–∞–¥–∞–Ω–∏–π —É—á–∏—Ç—ã–≤–∞–µ–º –ø–µ—Ä–µ–¥ –Ω–∞–∫–∞–∑–∞–Ω–∏–µ–º
        self._proxy_slow_hits_threshold = 5
        # –ú—è–≥–∫–∏–π penalty –¥–ª—è –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –ø–æ–ø–∞–¥–∞–Ω–∏–π
        self._proxy_slow_penalty_seconds = 60.0
        # –ü–æ—Ä–æ–≥ —Å–∫–æ—Ä–æ—Å—Ç–∏ (–±–∞–π—Ç/—Å–µ–∫). –°–Ω–∏–∑–∏–ª –¥–æ ~40 KB/s —á—Ç–æ–±—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–º –ø—Ä–æ–∫—Å—è–º
        self._proxy_min_speed_bytes = 40 * 1024  # 40 KB/s
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ (—Å–µ–∫)
        self._proxy_min_duration_for_speed = 3.0
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ ‚Äî —É–≤–µ–ª–∏—á–∏–º –Ω–µ–º–Ω–æ–≥–æ —á—Ç–æ–±—ã –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å –º–µ–ª–∫–∏–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏
        self._slow_check_min_size_bytes = 120 * 1024  # 120 KB
        # cooldown –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏
        self._slow_retry_logging_cooldown = 45.0
        self._proxy_last_warn: dict[str, float] = {}

        # –ö–µ—à–∏—Ä—É–µ–º —Å–µ—Ä–≤–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        self._cached_image_server = None

    def __IsSlideLink(self, link: str, servers: list[str]) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –≤–µ–¥—ë—Ç –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å–ª–∞–π–¥.
            link ‚Äì —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ;
            servers ‚Äì —Å–ø–∏—Å–æ–∫ —Å–µ—Ä–≤–µ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
        """

        for Server in servers:
            if Server in link: return True

        return False
    
    def __ParseSlideLink(self, link: str, servers: list[str]) -> tuple[str]:
        """
        –ü–∞—Ä—Å–∏—Ç —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–∞–π–¥.
            link ‚Äì —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ;
            servers ‚Äì —Å–ø–∏—Å–æ–∫ —Å–µ—Ä–≤–µ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
        """

        OriginalServer = None
        URI = None

        for Server in servers:

            if Server in link:
                OriginalServer = Server
                URI = link.replace(OriginalServer, "")

        return (OriginalServer, URI)

    #==========================================================================================#
    # >>>>> –ü–†–ò–í–ê–¢–ù–´–ï –ú–ï–¢–û–î–´ –ü–ê–†–°–ò–ù–ì–ê <<<<< #
    #==========================================================================================#

    def __CheckCorrectDomain(self, data: dict) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –≤–æ–∑—Ä–∞—Å—Ç–Ω–æ–π —Ä–µ–π—Ç–∏–Ω–≥.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Domain = self._Manifest.site

        if self._Title.site:

            if data["site"] != self.__GetSiteID(self._Title.site):
                Domain = self.__GetSiteDomain(data["site"])
                self._Portals.warning(f"Title site changed to \"{Domain}\".")

        return Domain 

    def __GetAgeLimit(self, data: dict) -> int:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –≤–æ–∑—Ä–∞—Å—Ç–Ω–æ–π —Ä–µ–π—Ç–∏–Ω–≥.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Rating = None
        RatingString = data["ageRestriction"]["label"].split(" ")[0].replace("+", "").replace("–ù–µ—Ç", "")
        if RatingString.isdigit(): Rating = int(RatingString)

        return Rating 

    def __GetAuthors(self, data: dict) -> list[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∞–≤—Ç–æ—Ä–æ–≤."""

        Authors = list()
        for Author in data["authors"]: Authors.append(Author["name"])

        return Authors

    def __GetBranches(self) -> list[Branch]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–∞–π—Ç–ª–∞."""

        Branches: dict[int, Branch] = dict()
        Response = self._Requestor.get(f"https://{self.__API}/api/manga/{self.__TitleSlug}/chapters")
        
        if Response.status_code == 200:
            Data = Response.json["data"]
            sleep(self._get_common_delay())

            for CurrentChapterData in Data:

                for BranchData in CurrentChapterData["branches"]:
                    BranchID = BranchData["branch_id"]
                    if BranchID == None: BranchID = int(str(self._Title.id) + "0")
                    if BranchID not in Branches.keys(): Branches[BranchID] = Branch(BranchID)

                    ChapterObject = Chapter(self._SystemObjects)
                    ChapterObject.set_id(BranchData["id"])
                    ChapterObject.set_volume(CurrentChapterData["volume"])
                    ChapterObject.set_number(CurrentChapterData["number"])
                    ChapterObject.set_name(CurrentChapterData["name"])
                    ChapterObject.set_is_paid("restricted_view" in BranchData and not BranchData["restricted_view"]["is_open"])
                    ChapterObject.set_workers([sub["name"] for sub in BranchData["teams"]])
                    ChapterObject.add_extra_data("moderated", False if "moderation" in BranchData.keys() else True)

                    if self._Settings.custom["add_free_publication_date"] and ChapterObject.is_paid:
                        ChapterObject.add_extra_data("free-publication-date", BranchData["restricted_view"]["expired_at"])

                    Branches[BranchID].add_chapter(ChapterObject)

        else: self._Portals.request_error(Response, "Unable to request chapter.", exception = False)

        for CurrentBranch in Branches.values(): self._Title.add_branch(CurrentBranch)

    def __GetCovers(self, data: dict) -> list[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±–ª–æ–∂–µ–∫."""

        Covers = list()

        if data["cover"]:
            Covers.append({
                "link": data["cover"]["default"],
                "filename": data["cover"]["default"].split("/")[-1]
            })

        if self._Settings.common.sizing_images:
            Covers[0]["width"] = None
            Covers[0]["height"] = None

        return Covers

    def __GetDescription(self, data: dict) -> str | None:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Description = None
        if "summary" in data.keys(): Description = RemoveRecurringSubstrings(data["summary"], "\n").strip().replace(" \n", "\n")
        Description = Zerotify(Description)

        return Description

    def __GetFranchises(self, data: dict) -> list[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–µ—Ä–∏–π.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Franchises = list()
        for Franchise in data["franchise"]: Franchises.append(Franchise["name"])
        if "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–∞–±–æ—Ç—ã" in Franchises: Franchises.remove("–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–∞–±–æ—Ç—ã")

        return Franchises

    def __GetGenres(self, data: dict) -> list[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∂–∞–Ω—Ä–æ–≤.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Genres = list()
        for Genre in data["genres"]: Genres.append(Genre["name"])

        return Genres

    def __GetImagesServers(self, server_id: str | None = None, all_sites: bool = False) -> list[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–º–µ–Ω–æ–≤ —Å–µ—Ä–≤–µ—Ä–æ–≤ —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
            server_id ‚Äì –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–µ—Ä–≤–µ—Ä–∞;\n
            all_sites ‚Äì —É–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤–µ—Ä–Ω—É—Ç—å –Ω—É–∂–Ω–æ –¥–æ–º–µ–Ω—ã —Ö—Ä–∞–Ω–∏–ª–∏—â –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤.
        """

        Servers = list()
        CurrentSiteID = self.__GetSiteID()
        URL = f"https://{self.__API}/api/constants?fields[]=imageServers"
        Headers = {
            "Authorization": self._Settings.custom["token"],
            "Referer": f"https://{self._Manifest.site}/"
        }
        Response = self._Requestor.get(URL, headers = Headers)

        if Response.status_code == 200:
            Data = Response.json["data"]["imageServers"]
            sleep(self._get_common_delay())

            for ServerData in Data:

                if server_id:
                    if ServerData["id"] == server_id and CurrentSiteID in ServerData["site_ids"]: Servers.append(ServerData["url"])
                    elif ServerData["id"] == server_id and all_sites: Servers.append(ServerData["url"])

                else:
                    if CurrentSiteID in ServerData["site_ids"] or all_sites: Servers.append(ServerData["url"])

        else:
            self._Portals.request_error(Response, "Unable to request site constants.")

        return Servers

    def __GetSiteDomain(self, id: str) -> int | None:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ–º–µ–Ω —Å–∞–π—Ç–∞.
            id ‚Äì —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–∞–π—Ç–∞.
        """

        SiteDomain = None

        for Domain, ID in self.__Sites.items():
            if ID == id: SiteDomain = Domain
        
        return SiteDomain

    def __GetSiteID(self, site: str = None) -> int | None:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–∞–π—Ç–∞.
            site ‚Äì –¥–æ–º–µ–Ω —Å–∞–π—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –ø–∞—Ä—Å–µ—Ä–æ–º).
        """

        if not site: site = self._Manifest.site
        SiteID = None

        for Domain in self.__Sites.keys():
            if Domain in site: SiteID = self.__Sites[Domain]
        
        return SiteID

    def __GetSlides(self, branch_id: int, chapter: Chapter) -> list[dict]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Å–ª–∞–π–¥–∞—Ö –≥–ª–∞–≤—ã.
            branch_id ‚Äì –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≤–µ—Ç–≤–∏;\n
            chapter ‚Äì –¥–∞–Ω–Ω—ã–µ –≥–ª–∞–≤—ã.
        """

        Slides = list()

        if "moderated" in chapter.to_dict().keys() and not chapter["moderated"]:
            self._Portals.chapter_skipped(self._Title, chapter, comment = "Not moderated.")
            return Slides
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –≤–º–µ—Å—Ç–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        if self._cached_image_server is None:
            self._cached_image_server = self.__GetImagesServers(self._Settings.custom["server"])[0]
        Server = self._cached_image_server

        parse_delay = self._get_parse_delay()

        token = None
        custom_settings = getattr(self._Settings, "custom", None)
        if custom_settings is not None:
            try:
                token = custom_settings["token"]
            except KeyError:
                token = None
        headers: dict[str, str] = {
            "Referer": f"https://{self._Manifest.site}/"
        }

        site_id = self.__GetSiteID()
        if site_id is not None:
            headers["Site-Id"] = str(site_id)

        if token:
            headers["Authorization"] = token

        default_branch_id = int(str(self._Title.id) + "0")
        branch_query_value = branch_id if branch_id and branch_id != default_branch_id else None

        base_endpoint = f"https://{self.__API}/api/manga/{self.__TitleSlug}/chapter"
        url_variants: list[str] = []

        def _build_query(params: dict[str, str | None]) -> str | None:
            components = [f"{key}={value}" for key, value in params.items() if value is not None and value != ""]
            if not components:
                return None
            return f"{base_endpoint}?{'&'.join(components)}"

        def _ensure_str(value) -> str | None:
            if value is None:
                return None
            try:
                return str(value)
            except Exception:
                return None

        def _append_query(path: str, params: dict[str, str | None]) -> str:
            filtered = [f"{key}={value}" for key, value in params.items() if value is not None and value != ""]
            if not filtered:
                return path
            separator = "&" if "?" in path else "?"
            return f"{path}{separator}{'&'.join(filtered)}"

        number_value = _ensure_str(chapter.number)
        volume_value = _ensure_str(chapter.volume)
        if volume_value is None or volume_value.strip().lower() in {"", "none", "null"}:
            volume_value = "1"
        chapter_id_value = _ensure_str(chapter.id)

        def _extend_variants(include_branch: bool) -> None:
            branch_value = _ensure_str(branch_query_value) if include_branch else None

            query = _build_query({
                "number": number_value,
                "volume": volume_value,
                "branch_id": branch_value,
            })
            if query:
                url_variants.append(query)

            if chapter_id_value:
                path_variant = f"{base_endpoint}/{chapter_id_value}"
                path_variant = _append_query(path_variant, {
                    "branch_id": branch_value,
                    "volume": volume_value,
                    "number": number_value,
                })
                url_variants.append(path_variant)

                chapter_id_query = _build_query({
                    "chapter_id": chapter_id_value,
                    "branch_id": branch_value,
                    "volume": volume_value,
                    "number": number_value,
                })
                if chapter_id_query:
                    url_variants.append(chapter_id_query)

                generic_id_query = _build_query({
                    "id": chapter_id_value,
                    "branch_id": branch_value,
                    "volume": volume_value,
                    "number": number_value,
                })
                if generic_id_query:
                    url_variants.append(generic_id_query)

            fallback_query = _build_query({
                "branch_id": branch_value,
                "id": chapter_id_value,
                "volume": volume_value,
                "number": number_value,
            })
            if fallback_query:
                url_variants.append(fallback_query)

        # –ü—ã—Ç–∞–µ–º—Å—è —Å–Ω–∞—á–∞–ª–∞ —Å branch_id, –∑–∞—Ç–µ–º –±–µ–∑ –Ω–µ–≥–æ, —á—Ç–æ–±—ã –æ–±–æ–π—Ç–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è API
        _extend_variants(include_branch=True)
        _extend_variants(include_branch=False)

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
        url_variants = list(dict.fromkeys(url_variants))

        last_error: str | None = None
        last_status: int | None = None
        last_response = None

        retryable_statuses = {408, 409, 423, 425, 429, 500, 502, 503, 504}
        max_retry_attempts = getattr(self._Settings.common, "chapter_retry_attempts", 3)
        initial_retry_delay = getattr(self._Settings.common, "chapter_retry_delay", 2.0)
        retry_backoff_factor = getattr(self._Settings.common, "chapter_retry_backoff", 2.0)
        min_retry_delay = getattr(self._Settings.common, "chapter_retry_min_delay", 1.5)
        max_retry_delay = getattr(self._Settings.common, "chapter_retry_max_delay", 45.0)
        extra_rate_limit_attempts = getattr(self._Settings.common, "chapter_retry_attempts_429_extra", 2)
        retry_jitter_min = getattr(self._Settings.common, "chapter_retry_jitter_min", 0.85)
        retry_jitter_max = getattr(self._Settings.common, "chapter_retry_jitter_max", 1.25)
        if retry_jitter_min <= 0 or retry_jitter_max < retry_jitter_min:
            retry_jitter_min, retry_jitter_max = 0.9, 1.1

        def extract_error_message(response) -> str | None:
            if response is None:
                return None
            try:
                payload = response.json
            except Exception:
                payload = None

            if isinstance(payload, dict):
                for key in ("message", "error", "detail", "reason"):
                    value = payload.get(key)
                    if isinstance(value, str) and value.strip():
                        return value.strip()

                errors_obj = payload.get("errors")
                if isinstance(errors_obj, dict):
                    first_error = next((str(v) for v in errors_obj.values() if v), None)
                    if first_error:
                        return first_error

            if hasattr(response, "text"):
                text_value = response.text
                if isinstance(text_value, str):
                    snippet = text_value.strip()
                    if snippet:
                        return snippet[:200]
            return None

        for url in url_variants:
            attempt = 0
            while True:
                Response = self._Requestor.get(url, headers=headers if headers else None)
                last_response = Response
                last_status = Response.status_code

                if Response.status_code == 200:
                    break

                error_message = extract_error_message(Response)
                last_error = f"HTTP {Response.status_code}"
                if error_message:
                    last_error = f"HTTP {Response.status_code}: {error_message}"

                if Response.status_code in retryable_statuses:
                    allowed_attempts = max_retry_attempts
                    if Response.status_code == 429:
                        allowed_attempts += extra_rate_limit_attempts

                    if allowed_attempts > 1 and attempt < allowed_attempts - 1:
                        wait_seconds = initial_retry_delay * (retry_backoff_factor ** attempt)
                        retry_after_header = Response.headers.get("Retry-After") if hasattr(Response, "headers") else None
                        retry_after_value = None
                        if retry_after_header:
                            header_value = retry_after_header.strip()
                            if header_value.isdigit():
                                retry_after_value = float(header_value)
                            else:
                                try:
                                    retry_dt = parsedate_to_datetime(header_value)
                                    if retry_dt is not None:
                                        retry_after_value = max(0.0, (retry_dt - datetime.utcnow()).total_seconds())
                                except Exception:
                                    retry_after_value = None

                        if retry_after_value is not None:
                            wait_seconds = max(wait_seconds, retry_after_value)

                        jitter = random.uniform(retry_jitter_min, retry_jitter_max)
                        wait_seconds *= jitter
                        wait_seconds = max(wait_seconds, min_retry_delay)
                        wait_seconds = min(wait_seconds, max_retry_delay)

                        next_attempt_number = attempt + 2
                        self._SystemObjects.logger.warning(
                            f"Chapter {chapter.id} request to {url} returned {Response.status_code}. "
                            f"Retrying in {wait_seconds:.1f}s (attempt {next_attempt_number}/{allowed_attempts}).")
                        sleep(wait_seconds)
                        attempt += 1
                        continue

                break

            if Response.status_code != 200:
                continue

            Payload = Response.json
            Data = Payload.get("data") if isinstance(Payload, dict) else None

            if not isinstance(Data, dict):
                last_error = "No data in response"
                continue

            Pages = Data.get("pages")
            if not Pages:
                last_error = "Empty pages list"
                continue

            sleep(parse_delay)

            for SlideIndex, Page in enumerate(Pages, start=1):
                RelativeURL = Page.get("url")
                if not RelativeURL:
                    continue

                Buffer = {
                    "index": SlideIndex,
                    "link": Server + RelativeURL.replace(" ", "%20"),
                    "width": Page.get("width"),
                    "height": Page.get("height")
                }
                Slides.append(Buffer)

            if Slides:
                self._Portals.chapter_download_start(self._Title, chapter, len(Pages), len(Pages))
                break

        if Slides:
            return Slides

        reason_parts: list[str] = []
        if chapter.is_paid:
            reason_parts.append("–ø–ª–∞—Ç–Ω–∞—è –≥–ª–∞–≤–∞ ‚Äî –¥–æ—Å—Ç—É–ø –æ–≥—Ä–∞–Ω–∏—á–µ–Ω")
        if last_error:
            reason_parts.append(last_error)
        elif last_status:
            reason_parts.append(f"HTTP {last_status}")
        else:
            reason_parts.append("–∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–µ –≤–µ—Ä–Ω—É–ª —Å—Ç—Ä–∞–Ω–∏—Ü—ã")

        reason_comment = "; ".join(reason_parts)
        chapter.add_extra_data("empty_reason", reason_comment)
        self._SystemObjects.logger.warning(f"Chapter {chapter.id} returned no slides ({reason_comment}).")
        if last_response and last_response.status_code != 200:
            self._Portals.request_error(last_response, "Unable to request chapter content.", exception=False)
        self._Portals.chapter_skipped(self._Title, chapter, comment=reason_comment)

        return Slides

    def __GetStatus(self, data: dict) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç—É—Å.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Status = None
        StatusesDetermination = {
            1: Statuses.ongoing,
            2: Statuses.completed,
            3: Statuses.announced,
            4: Statuses.dropped,
            5: Statuses.dropped
        }
        SiteStatusIndex = data["status"]["id"]
        if SiteStatusIndex in StatusesDetermination.keys(): Status = StatusesDetermination[SiteStatusIndex]

        return Status

    def __GetTitleData(self) -> dict | None:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ç–∞–π—Ç–ª–∞.
            slug ‚Äì –∞–ª–∏–∞—Å.
        """
        
        URL = f"https://{self.__API}/api/manga/{self.__TitleSlug}?fields[]=eng_name&fields[]=otherNames&fields[]=summary&fields[]=releaseDate&fields[]=type_id&fields[]=caution&fields[]=genres&fields[]=tags&fields[]=franchise&fields[]=authors&fields[]=manga_status_id&fields[]=status_id"
        
        # DEBUG: –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        print(f"[DEBUG] üîç Requesting title data for: {self.__TitleSlug}")
        print(f"[DEBUG] üåê URL: {URL}")
        
        Response = self._Requestor.get(URL)
        
        # DEBUG: –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        print(f"[DEBUG] üì° Response status: {Response.status_code}")
        if hasattr(Response, 'text'):
            response_preview = Response.text[:500] if hasattr(Response.text, '__getitem__') else str(Response.text)[:500]
            print(f"[DEBUG] üìÑ Response preview: {response_preview}")

        if Response.status_code == 200:
            Response = Response.json["data"]
            sleep(self._get_common_delay())

        elif Response.status_code == 451: 
            self._Portals.request_error(Response, "Account banned.")
            return None
        elif Response.status_code == 404: 
            self._Portals.title_not_found(self._Title)
            return None
        else: 
            self._Portals.request_error(Response, "Unable to request title data.")
            return None

        return Response

    def __GetTags(self, data: dict) -> list[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–≥–æ–≤.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Tags = list()
        for Tag in data["tags"]: Tags.append(Tag["name"])

        return Tags

    def __GetType(self, data: dict) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ç–∏–ø —Ç–∞–π—Ç–ª–∞.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Type = None

        TypeData = data.get("type") or {}
        SiteTypeLabel = TypeData.get("label") or ""
        SiteTypeId = TypeData.get("id")

        TypeById = {
            9: Types.western_comic,   # –ö–æ–º–∏–∫—Å
            4: Types.oel,             # OEL-–º–∞–Ω–≥–∞
            8: Types.russian_comic,   # –†—É–º–∞–Ω–≥–∞
        }

        TypeByLabel = {
            "–ú–∞–Ω–≥–∞": Types.manga,
            "–ú–∞–Ω—Ö–≤–∞": Types.manhwa,
            "–ú–∞–Ω—å—Ö—É–∞": Types.manhua,
            "–†—É–º–∞–Ω–≥–∞": Types.russian_comic,
            "–ö–æ–º–∏–∫—Å": Types.western_comic,
            "–ö–æ–º–∏–∫—Å –∑–∞–ø–∞–¥–Ω—ã–π": Types.western_comic,
            "OEL-–º–∞–Ω–≥–∞": Types.oel,
        }

        if SiteTypeId in TypeById:
            Type = TypeById[SiteTypeId]
        else:
            NormalizedLabel = SiteTypeLabel.strip()
            if NormalizedLabel in TypeByLabel:
                Type = TypeByLabel[NormalizedLabel]

        return Type

    def __StringToDate(self, date_str: str) -> datetime:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –≤—Ä–µ–º—è –≤ –æ–±—ä–µ–∫—Ç–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é.
            date_str ‚Äì —Å—Ç—Ä–æ–∫–æ–≤–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è.
        """

        DatePattern = "%Y-%m-%dT%H:%M:%S.%fZ"

        return datetime.strptime(date_str, DatePattern)

    #==========================================================================================#
    # >>>>> –ü–£–ë–õ–ò–ß–ù–´–ï –ú–ï–¢–û–î–´ <<<<< #
    #==========================================================================================#

    def amend(self, branch: Branch, chapter: Chapter):
        """
        –î–æ–ø–æ–ª–Ω—è–µ—Ç –≥–ª–∞–≤—É –¥–∞–π–Ω—ã–º–∏ –æ —Å–ª–∞–π–¥–∞—Ö.
            branch ‚Äì –¥–∞–Ω–Ω—ã–µ –≤–µ—Ç–≤–∏;\n
            chapter ‚Äì –¥–∞–Ω–Ω—ã–µ –≥–ª–∞–≤—ã.
        """

        Slides = self.__GetSlides(branch.id, chapter)
        for Slide in Slides: chapter.add_slide(Slide["link"], Slide["width"], Slide["height"])

    def amend_postprocessor(self, chapter: Chapter):
        """
        –í–Ω–æ—Å–∏—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –≥–ª–∞–≤—É –ø–æ—Å–ª–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –µ—ë –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º. –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è.
        
        –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –¥–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.

        :param chapter: –î–∞–Ω–Ω—ã–µ –≥–ª–∞–≤—ã.
        :type chapter: Chapter
        """

        if not self._Settings.custom["add_moderation_status"]: chapter.remove_extra_data("moderated")

    def collect(self, period: int | None = None, filters: str | None = None, pages: int | None = None) -> list[str]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–∞–π—Ç–ª–æ–≤ –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º.
            period ‚Äì –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤ –¥–æ —Ç–µ–∫—É—â–µ–≥–æ –º–æ–º–µ–Ω—Ç–∞, —Å–æ—Å—Ç–∞–≤–ª—è—é—â–µ–µ –ø–µ—Ä–∏–æ–¥ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö;\n
            filters ‚Äì —Å—Ç—Ä–æ–∫–∞, –æ–ø–∏—Å—ã–≤–∞—é—â–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (–ø–æ–¥—Ä–æ–±–Ω–µ–µ –≤ README.md);\n
            pages ‚Äì –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞.
        """

        Updates = list()
        IsUpdatePeriodOut = False
        Page = 1
        UpdatesCount = 0
        Headers = {
            "Site-Id": str(self.__GetSiteID())
        }
        CurrentDate = datetime.utcnow()

        while not IsUpdatePeriodOut:
            Response = self._Requestor.get(f"https://{self.__API}/api/latest-updates?page={Page}", headers = Headers)

            if Response.status_code == 200:
                UpdatesPage = Response.json["data"]

                for UpdateNote in UpdatesPage:
                    Delta = CurrentDate - self.__StringToDate(UpdateNote["last_item_at"])
                    
                    if Delta.total_seconds() / 3600 <= period:
                        Updates.append(UpdateNote["slug_url"])
                        UpdatesCount += 1

                    else:
                        IsUpdatePeriodOut = True

            else:
                IsUpdatePeriodOut = True
                self._Portals.request_error(Response, f"Unable to request updates page {Page}.")


            if not IsUpdatePeriodOut:
                self._Portals.collect_progress_by_page(Page)
                Page += 1
                sleep(self._get_common_delay())

        return Updates

    def image(self, url: str) -> str | None:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Å–∞–π—Ç–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥ –ø–∞—Ä—Å–µ—Ä–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞.
            url ‚Äì —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.
        """

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π delay –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–º–µ–Ω—å—à–µ —á–µ–º –¥–ª—è API)
        image_delay = self._get_image_delay()

        Result = self._ImagesDownloader.temp_image(url)
        
        # –ï—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞ - –¥–æ–±–∞–≤–ª—è–µ–º delay –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—Ä–æ—Å–æ–º
        if Result:
            sleep(image_delay)
            return Result
        
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å - –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ —Å–µ—Ä–≤–µ—Ä—ã
        Servers = self.__GetImagesServers(all_sites = True)

        if self.__IsSlideLink(url, Servers):
            OriginalServer, ImageURI = self.__ParseSlideLink(url, Servers)
            Servers.remove(OriginalServer)

            for Server in Servers:
                Link = Server + ImageURI
                Result = self._ImagesDownloader.temp_image(Link)
                
                if Result: 
                    sleep(image_delay)
                    break
                elif Server != Servers[-1]: 
                    sleep(image_delay)

        return Result

    def _get_proxy_count(self) -> int:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏."""
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º settings.json (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1)
        if hasattr(self._Settings, 'proxy') and hasattr(self._Settings.proxy, 'enable') and self._Settings.proxy.enable:
            if hasattr(self._Settings.proxy, 'proxies') and self._Settings.proxy.proxies:
                proxy_count = len(self._Settings.proxy.proxies)
                print(f"[INFO] üåê Detected {proxy_count} proxies from settings.json")
                return proxy_count
        
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ ProxyRotator (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2) - –¢–û–¢ –ñ–ï –ö–û–î –ß–¢–û –í _InitializeRequestor
        try:
            import sys
            import os
            from pathlib import Path
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ MelonService –≤ sys.path –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ proxy_rotator
            melon_service_path = Path(__file__).parent.parent.parent
            if str(melon_service_path) not in sys.path:
                sys.path.insert(0, str(melon_service_path))
            
            from proxy_rotator import ProxyRotator
            
            # –°–æ–∑–¥–∞—ë–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Ä–æ—Ç–∞—Ç–æ—Ä–∞ –¥–ª—è –ø–∞—Ä—Å–µ—Ä–∞
            rotator = ProxyRotator(parser="mangalib")
            
            if rotator.enabled and rotator.get_proxy_count() > 0:
                proxy_count = rotator.get_proxy_count()
                print(f"[INFO] üåê Detected {proxy_count} proxies from ProxyRotator")
                return proxy_count
                
        except ImportError:
            pass
        except Exception as e:
            print(f"[WARNING] ‚ö†Ô∏è  Error detecting ProxyRotator: {e}")
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º —á—Ç–æ 1 –ø—Ä–æ–∫—Å–∏ (–∏–ª–∏ –ø—Ä—è–º–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ)
        print(f"[INFO] üåê No proxies detected, using 1 worker")
        return 1
        cached = getattr(self, "_proxy_count_cache", None)
        if isinstance(cached, int) and cached > 0:
            return cached

        from pathlib import Path
        import sys
        import json

        melon_service_path = Path(__file__).parent.parent.parent
        if str(melon_service_path) not in sys.path:
            sys.path.insert(0, str(melon_service_path))

        proxy_count = 0

        try:
            from proxy_rotator import ProxyRotator
            rotator = ProxyRotator(parser="mangalib")
            if rotator.enabled:
                proxy_count = rotator.get_proxy_count()
        except Exception as e:
            print(f"[WARNING] ProxyRotator not available: {e}")

        if proxy_count <= 0:
            try:
                settings_path = Path(__file__).parent / "settings.json"
                if settings_path.exists():
                    with open(settings_path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        prox_list = data.get("Main", {}).get("proxy", [])
                        if isinstance(prox_list, list):
                            proxy_count = len(prox_list)
            except Exception as e:
                print(f"[WARNING] Unable to read proxy count from settings: {e}")

        proxy_count = max(1, proxy_count)
        self._proxy_count_cache = proxy_count
        return proxy_count

    def batch_download_images(self, urls: list[str]) -> list[str | None]:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –±–∞—Ç—á–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
        
        :param urls: –°–ø–∏—Å–æ–∫ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        :return: –°–ø–∏—Å–æ–∫ –∏–º—ë–Ω —Ñ–∞–π–ª–æ–≤ (–∏–ª–∏ None –¥–ª—è –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫) –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ
        """
        
        if not urls:
            return []

        import time

        batch_started_at = time.perf_counter()
        progress_state = {
            "last_log_at": batch_started_at,
            "last_downloaded": 0
        }

        report_every = max(10, len(urls) // 12) if len(urls) > 0 else 1
        min_seconds_between_logs = 15.0

        def progress_callback(downloaded: int, total: int):
            if total <= 0:
                return

            now = time.perf_counter()
            should_log = (
                downloaded == total
                or downloaded - progress_state["last_downloaded"] >= report_every
                or (now - progress_state["last_log_at"]) >= min_seconds_between_logs
            )

            if not should_log:
                return

            percent = (downloaded / total) * 100
            # –£–±–∏—Ä–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω—ã–π debug –ª–æ–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ - –≤–∞–∂–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ MangaBuilder
            progress_state["last_log_at"] = now
            progress_state["last_downloaded"] = downloaded

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        results = self._parallel_downloader.download_batch(urls, progress_callback=progress_callback)

        elapsed = max(time.perf_counter() - batch_started_at, 0.0001)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å–ø–∏—Å–æ–∫ –∏–º—ë–Ω —Ñ–∞–π–ª–æ–≤ (—Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫)
        from urllib.parse import urlparse, unquote

        total_images = len(urls)
        filenames: list[str | None] = [None] * total_images
        failed_indices: list[int] = []
        fallback_attempts = 0
        successful_downloads = 0

        def _expected_filename(target_url: str) -> str:
            parsed = urlparse(target_url)
            raw_name = parsed.path.split('/')[-1]
            return unquote(raw_name)

        for result in results:
            index = result.get('index', 0) - 1
            url = result.get('url')

            if index < 0 or index >= total_images:
                if url:
                    self._SystemObjects.logger.warning(
                        f"Received download result with invalid index: idx={index}, url={url}"
                    )
                continue

            if result.get('success'):
                filenames[index] = result.get('filename')
                successful_downloads += 1
                continue

            # –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å ‚Äî –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–µ—Ä–≤–µ—Ä–∞
            fallback_attempts += 1
            fallback_filename: str | None = None
            fallback_status = self._try_alternative_servers(url)

            if fallback_status:
                if hasattr(fallback_status, "value") and fallback_status.value:
                    fallback_filename = fallback_status.value
                elif isinstance(fallback_status, str):
                    fallback_filename = fallback_status
                elif hasattr(fallback_status, "filename") and fallback_status.filename:
                    fallback_filename = fallback_status.filename

            if not fallback_filename and url:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—è–≤–∏–ª—Å—è –ª–∏ —Ñ–∞–π–ª –≤ temp –ø–æ—Å–ª–µ —Ñ–æ–ª–±—ç–∫–∞
                if self._ImagesDownloader.is_exists(url):
                    fallback_filename = _expected_filename(url)

            if fallback_filename:
                filenames[index] = fallback_filename
                successful_downloads += 1
            else:
                failed_indices.append(index)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –º–µ–¥–ª–µ–Ω–Ω—ã–π —Ñ–æ–ª–±—ç–∫: –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –¥–ª—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è
        if failed_indices:
            slow_delay = max(self._get_image_delay() * 3, 0.6)
            max_additional_attempts = 3

            self._SystemObjects.logger.warning(
                f"Sequential fallback engaged for {len(failed_indices)} images (delay {slow_delay:.2f}s)"
            )

            for idx in failed_indices:
                url = urls[idx]
                recovered = False

                for attempt in range(1, max_additional_attempts + 1):
                    sleep(slow_delay * attempt)
                    sequential_filename = self._download_image_wrapper(url)

                    if sequential_filename:
                        filenames[idx] = sequential_filename
                        successful_downloads += 1
                        recovered = True
                        self._SystemObjects.logger.info(
                            f"‚úÖ Sequential fallback recovered image {idx + 1}/{total_images} (attempt {attempt})"
                        )
                        break

                if not recovered:
                    filenames[idx] = None
                    self._SystemObjects.logger.error(
                        f"‚ùå Unable to recover image {idx + 1}/{total_images} after sequential fallback: {url}"
                    )

        failed_after_fallback = total_images - successful_downloads
        avg_speed = successful_downloads / elapsed if elapsed > 0 else 0

        # –£–±–∏—Ä–∞–µ–º –ø–æ–¥—Ä–æ–±–Ω—ã–π debug –ª–æ–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è - –≤–∞–∂–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ MangaBuilder

        return filenames

    def _try_alternative_servers(self, url: str) -> str | None:
        """–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤ (fallback)."""
        
        Servers = self.__GetImagesServers(all_sites=True)
        
        if self.__IsSlideLink(url, Servers):
            OriginalServer, ImageURI = self.__ParseSlideLink(url, Servers)
            
            try:
                Servers.remove(OriginalServer)
            except ValueError:
                pass
            
            for Server in Servers:
                Link = Server + ImageURI
                Result = self._ImagesDownloader.temp_image(Link)
                
                if Result:
                    return Result
        
        return None

    def parse(self):
        """–ü–æ–ª—É—á–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–∞–π—Ç–ª–∞."""

        print(f"[DEBUG] üöÄ Starting parse() for title: {self._Title.slug}")
        
        self._Requestor.config.add_header("Site-Id", str(self.__Sites[self._Manifest.site]))

        # MangaLib –∏–∑–º–µ–Ω–∏–ª —Å—Ç—Ä—É–∫—Ç—É—Ä—É: —Ç–µ–ø–µ—Ä—å slug'–∏ –∏–∑ API –∏–º–µ—é—Ç —Ñ–æ—Ä–º–∞—Ç "ID--slug"
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ slug ID (—Ñ–æ—Ä–º–∞—Ç: "7580--i-alone-level-up")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º slug –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        slug_with_id = self._Title.slug
        clean_slug = self._Title.slug
        extracted_id = None
        
        if "--" in self._Title.slug:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏ slug –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ "ID--slug"
            parts = self._Title.slug.split("--", 1)
            if len(parts) == 2 and parts[0].isdigit():
                extracted_id = int(parts[0])
                clean_slug = parts[1]
                print(f"[DEBUG] Extracted: ID={extracted_id}, slug={clean_slug}")
        
        # API —Ç—Ä–µ–±—É–µ—Ç –ø–æ–ª–Ω—ã–π slug (ID--slug)
        self.__TitleSlug = slug_with_id
        # –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –±–µ–∑ ID
        self._Title.set_slug(clean_slug)

        print(f"[DEBUG] TitleSlug (API): {self.__TitleSlug}")
        print(f"[DEBUG] Title.slug (file): {self._Title.slug}")

        Data = self.__GetTitleData()
        
        print(f"[DEBUG] üì¶ GetTitleData returned: {type(Data)}, is None: {Data is None}")
        if Data:
            print(f"[DEBUG] ‚úÖ Data keys: {list(Data.keys())[:10] if isinstance(Data, dict) else 'NOT A DICT'}")
        
        self._SystemObjects.manager.get_parser_settings()

        if Data:
            self._Title.set_site(self.__CheckCorrectDomain(Data))
            
            # ID —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–∑ –∏–∑–≤–ª–µ—á—ë–Ω–Ω–æ–≥–æ –∏–ª–∏ –∏–∑ API
            if extracted_id is not None:
                self._Title.set_id(extracted_id)
                if extracted_id != Data["id"]:
                    print(f"[WARNING] ID mismatch: extracted={extracted_id}, API={Data['id']} (using extracted)")
            else:
                self._Title.set_id(Data["id"])
            
            # Slug –£–ñ–ï —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤—ã—à–µ (—á–∏—Å—Ç—ã–π, –±–µ–∑ ID)
            print(f"[DEBUG] Final slug (file): {self._Title.slug}")
            print(f"[DEBUG] Final ID: {self._Title.id}")
            
            self._Title.set_content_language("rus")
            self._Title.set_localized_name(Data["rus_name"])
            self._Title.set_eng_name(Data["eng_name"])
            self._Title.set_another_names(Data["otherNames"])
            if Data["name"] not in Data["otherNames"] and Data["name"] != Data["rus_name"] and Data["name"] != Data["eng_name"]: self._Title.add_another_name(Data["name"])
            self._Title.set_covers(self.__GetCovers(Data))
            self._Title.set_authors(self.__GetAuthors(Data))
            self._Title.set_publication_year(int(Data["releaseDate"]) if Data["releaseDate"] else None)
            self._Title.set_description(self.__GetDescription(Data))
            self._Title.set_age_limit(self.__GetAgeLimit(Data))
            self._Title.set_type(self.__GetType(Data))
            self._Title.set_status(self.__GetStatus(Data))
            self._Title.set_is_licensed(Data["is_licensed"])
            self._Title.set_genres(self.__GetGenres(Data))
            self._Title.set_tags(self.__GetTags(Data))
            self._Title.set_franchises(self.__GetFranchises(Data))

            self.__GetBranches()