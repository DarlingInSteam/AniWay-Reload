from Source.Core.Base.Formats.Manga import Branch, Chapter, Types
from Source.Core.Base.Parsers.MangaParser import MangaParser
from Source.Core.Base.Formats.BaseFormat import Statuses

from dublib.Methods.Data import RemoveRecurringSubstrings, Zerotify
from dublib.WebRequestor import WebRequestor

from datetime import datetime
from time import sleep

# –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
from .parallel_downloader import AdaptiveParallelDownloader

class Parser(MangaParser):
    def amend(self, branch: Branch, chapter: Chapter):
        """
        –î–æ–ø–æ–ª–Ω—è–µ—Ç –≥–ª–∞–≤—É –¥–∞–π–Ω—ã–º–∏ –æ —Å–ª–∞–π–¥–∞—Ö.
            branch ‚Äì –¥–∞–Ω–Ω—ã–µ –≤–µ—Ç–≤–∏;
            chapter ‚Äì –¥–∞–Ω–Ω—ã–µ –≥–ª–∞–≤—ã.
        """

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–ª–∞–≤ –∏ —Ç–µ–∫—É—â–∏–π –∏–Ω–¥–µ–∫—Å
        total_chapters = sum(len(b.chapters) for b in self._Title.branches)
        current_chapter_index = 0
        
        # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å —Ç–µ–∫—É—â–µ–π –≥–ª–∞–≤—ã
        for b in self._Title.branches:
            for ch in b.chapters:
                current_chapter_index += 1
                if ch.id == chapter.id:
                    break
            else:
                continue
            break
        
        # –ù–û–í–û–ï: –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–≥—Ä–µ—Å—Å –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ stdout –¥–ª—è –∑–∞—Ö–≤–∞—Ç–∞ –ª–æ–≥–æ–≤
        chapter_name = f"{chapter.volume}.{chapter.number}" if chapter.volume else str(chapter.number)
        if chapter.name:
            chapter_name += f" - {chapter.name}"
        print(f"[{current_chapter_index}/{total_chapters}] Chapter {chapter_name} parsing...")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≥–ª–∞–≤—ã —á–µ—Ä–µ–∑ Portals
        self._Portals.chapter_parsing_start(self._Title, chapter, current_chapter_index, total_chapters)

        Slides = self.__GetSlides(branch.id, chapter)

        for Slide in Slides:
            chapter.add_slide(Slide["link"], Slide["width"], Slide["height"])

        # –ù–û–í–û–ï: –í—ã–≤–æ–¥–∏–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≥–ª–∞–≤—ã
        slides_count = len(Slides)
        print(f"[{current_chapter_index}/{total_chapters}] Chapter {chapter_name} completed ({slides_count} slides)")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≥–ª–∞–≤—ã —á–µ—Ä–µ–∑ Portals
        self._Portals.chapter_parsing_complete(self._Title, chapter, current_chapter_index, total_chapters, slides_count)

    def _InitializeRequestor(self) -> WebRequestor:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥—É–ª—å WEB-–∑–∞–ø—Ä–æ—Å–æ–≤."""

        WebRequestorObject = super()._InitializeRequestor()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω –µ—Å–ª–∏ –µ—Å—Ç—å
        if self._Settings.custom["token"]: 
            WebRequestorObject.config.add_header("Authorization", self._Settings.custom["token"])
        
        # PROXY ROTATION SUPPORT:
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏:
        # 1. ProxyRotator –∏–∑ settings.json (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω –∏ –µ—Å—Ç—å –ø—Ä–æ–∫—Å–∏)
        # 2. –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è HTTP_PROXY/HTTPS_PROXY
        # 3. –ë–µ–∑ –ø—Ä–æ–∫—Å–∏
        
        import sys
        import os
        from pathlib import Path
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ MelonService –≤ sys.path –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ proxy_rotator
        melon_service_path = Path(__file__).parent.parent.parent
        if str(melon_service_path) not in sys.path:
            sys.path.insert(0, str(melon_service_path))
        
        try:
            from proxy_rotator import ProxyRotator
            
            # –°–æ–∑–¥–∞—ë–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Ä–æ—Ç–∞—Ç–æ—Ä–∞ –¥–ª—è –ø–∞—Ä—Å–µ—Ä–∞
            rotator = ProxyRotator(parser="mangalib")
            
            if rotator.enabled and rotator.get_proxy_count() > 0:
                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–∫—Å–∏ (—Å —Ä–æ—Ç–∞—Ü–∏–µ–π –µ—Å–ª–∏ –∏—Ö –Ω–µ—Å–∫–æ–ª—å–∫–æ)
                if rotator.get_proxy_count() == 1:
                    proxy_dict = rotator.get_current_proxy()
                    print(f"[INFO] üîí Single proxy mode (no rotation): {rotator.get_proxy_count()} proxy")
                else:
                    proxy_dict = rotator.get_next_proxy()
                    print(f"[INFO] üîÑ Proxy rotation enabled: {rotator.get_proxy_count()} proxies, strategy={rotator.rotation_strategy}")
                
                if proxy_dict:
                    try:
                        if hasattr(WebRequestorObject, '_WebRequestor__Session'):
                            WebRequestorObject._WebRequestor__Session.proxies.update(proxy_dict)
                            print(f"[INFO] ‚úÖ Proxy configured via ProxyRotator")
                        elif hasattr(WebRequestorObject, 'session'):
                            WebRequestorObject.session.proxies.update(proxy_dict)
                            print(f"[INFO] ‚úÖ Proxy configured via ProxyRotator (public session)")
                    except Exception as e:
                        print(f"[WARNING] ‚ö†Ô∏è  Failed to set proxy from ProxyRotator: {e}")
            else:
                print(f"[INFO] ‚ÑπÔ∏è  ProxyRotator disabled, checking environment variables...")
                
                # Fallback: –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
                http_proxy = os.getenv("HTTP_PROXY") or os.getenv("http_proxy")
                https_proxy = os.getenv("HTTPS_PROXY") or os.getenv("https_proxy")
                
                if http_proxy or https_proxy:
                    proxies = {}
                    if http_proxy:
                        proxies['http'] = http_proxy
                    if https_proxy:
                        proxies['https'] = https_proxy
                    
                    try:
                        if hasattr(WebRequestorObject, '_WebRequestor__Session'):
                            WebRequestorObject._WebRequestor__Session.proxies.update(proxies)
                            print(f"[INFO] ‚úÖ Proxy configured from env vars: {http_proxy or https_proxy}")
                        elif hasattr(WebRequestorObject, 'session'):
                            WebRequestorObject.session.proxies.update(proxies)
                            print(f"[INFO] ‚úÖ Proxy configured from env vars (public session)")
                    except Exception as e:
                        print(f"[WARNING] ‚ö†Ô∏è  Failed to configure proxy from env: {e}")
                else:
                    print(f"[INFO] ‚ÑπÔ∏è  No proxy configured (direct connection)")
        
        except ImportError as e:
            print(f"[WARNING] ‚ö†Ô∏è  ProxyRotator not available: {e}")
            print(f"[INFO] ‚ÑπÔ∏è  Falling back to environment variables...")
            
            # Fallback –Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
            http_proxy = os.getenv("HTTP_PROXY") or os.getenv("http_proxy")
            https_proxy = os.getenv("HTTPS_PROXY") or os.getenv("https_proxy")
            
            if http_proxy or https_proxy:
                proxies = {}
                if http_proxy:
                    proxies['http'] = http_proxy
                if https_proxy:
                    proxies['https'] = https_proxy
                
                try:
                    if hasattr(WebRequestorObject, '_WebRequestor__Session'):
                        WebRequestorObject._WebRequestor__Session.proxies.update(proxies)
                        print(f"[INFO] ‚úÖ Proxy configured from env vars: {http_proxy or https_proxy}")
                    elif hasattr(WebRequestorObject, 'session'):
                        WebRequestorObject.session.proxies.update(proxies)
                        print(f"[INFO] ‚úÖ Proxy configured from env vars (public)")
                except Exception as e:
                    print(f"[WARNING] ‚ö†Ô∏è  Failed to configure proxy: {e}")

        return WebRequestorObject
    
    def _download_image_wrapper(self, url: str) -> str | None:
        """Thread-safe –æ–±–µ—Ä—Ç–∫–∞ —Å –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ–π —Å–µ—Å—Å–∏–µ–π requests –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ç–æ–∫–∞.
        
        :param url: URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        :return: –ò–º—è —Ñ–∞–π–ª–∞ –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, None –µ—Å–ª–∏ –æ—à–∏–±–∫–∞
        """
        import os
        from pathlib import Path
        import threading
        import requests
        
        thread_id = threading.current_thread().name
        print(f"[CRITICAL_DEBUG] [{thread_id}] ‚≠ê WRAPPER CALLED for {url[:80]}...", flush=True)
        
        directory = self._SystemObjects.temper.parser_temp
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL
        parsed_url = Path(url)
        filetype = parsed_url.suffix
        filename = parsed_url.stem
        image_path = f"{directory}/{filename}{filetype}"
        
        print(f"[CRITICAL_DEBUG] [{thread_id}] Checking cache: {image_path}", flush=True)
        
        # –ï—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –Ω–µ FORCE_MODE, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–º—è
        if os.path.exists(image_path) and not self._SystemObjects.FORCE_MODE:
            print(f"[CRITICAL_DEBUG] [{thread_id}] ‚úÖ Cache HIT", flush=True)
            return filename + filetype
        
        print(f"[CRITICAL_DEBUG] [{thread_id}] üì• Cache MISS, starting download...", flush=True)
        
        try:
            print(f"[CRITICAL_DEBUG] [{thread_id}] Getting requestor...", flush=True)
            # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π WebRequestor
            requestor = self._ImagesDownloader._ImagesDownloader__Requestor
            print(f"[CRITICAL_DEBUG] [{thread_id}] Got requestor: {type(requestor)}", flush=True)
            
            # –°–æ–∑–¥–∞–µ–º –ù–ï–ó–ê–í–ò–°–ò–ú–£–Æ —Å–µ—Å—Å–∏—é requests –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ—Ç–æ–∫–∞
            print(f"[CRITICAL_DEBUG] [{thread_id}] Creating requests.Session()...", flush=True)
            session = requests.Session()
            print(f"[CRITICAL_DEBUG] [{thread_id}] Session created!", flush=True)
            
            print(f"[CRITICAL_DEBUG] [{thread_id}] Looking for source session...", flush=True)
            # –ö–æ–ø–∏—Ä—É–µ–º cookies –∏–∑ WebRequestor Session (thread-safe read)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã WebRequestor
            source_session = None
            if hasattr(requestor, '_WebRequestor__Session'):
                source_session = requestor._WebRequestor__Session
                print(f"[CRITICAL_DEBUG] [{thread_id}] Found _WebRequestor__Session", flush=True)
            elif hasattr(requestor, 'session'):
                source_session = requestor.session
                print(f"[CRITICAL_DEBUG] [{thread_id}] Found session", flush=True)
            elif hasattr(requestor, '_session'):
                source_session = requestor._session
                print(f"[CRITICAL_DEBUG] [{thread_id}] Found _session", flush=True)
            
            if source_session and hasattr(source_session, 'cookies'):
                print(f"[CRITICAL_DEBUG] [{thread_id}] Copying cookies... (getting dict first)", flush=True)
                # –ö–†–ò–¢–ò–ß–ù–û: –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º update() –Ω–∞–ø—Ä—è–º—É—é - —ç—Ç–æ –≤—ã–∑—ã–≤–∞–µ—Ç deadlock!
                # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º dict, –ø–æ—Ç–æ–º –æ–±–Ω–æ–≤–ª—è–µ–º
                try:
                    cookies_dict = dict(source_session.cookies)
                    print(f"[CRITICAL_DEBUG] [{thread_id}] Got {len(cookies_dict)} cookies as dict", flush=True)
                    for name, value in cookies_dict.items():
                        session.cookies.set(name, value)
                    print(f"[CRITICAL_DEBUG] [{thread_id}] ‚úÖ Cookies copied!", flush=True)
                except Exception as e:
                    print(f"[WARNING] [{thread_id}] Failed to copy cookies: {e}", flush=True)
            
            # –ö–æ–ø–∏—Ä—É–µ–º headers –∏–∑ source session
            if source_session and hasattr(source_session, 'headers'):
                print(f"[CRITICAL_DEBUG] [{thread_id}] Copying headers...", flush=True)
                try:
                    headers_dict = dict(source_session.headers)
                    session.headers.update(headers_dict)
                    print(f"[CRITICAL_DEBUG] [{thread_id}] ‚úÖ Headers copied!", flush=True)
                except Exception as e:
                    print(f"[WARNING] [{thread_id}] Failed to copy headers: {e}", flush=True)
            
            print(f"[CRITICAL_DEBUG] [{thread_id}] Adding standard image headers...", flush=True)
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ headers –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9,ru;q=0.8',
                'Referer': 'https://mangalib.me/',
            })
            print(f"[CRITICAL_DEBUG] [{thread_id}] ‚úÖ Headers updated!", flush=True)
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–∫—Å–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            print(f"[CRITICAL_DEBUG] [{thread_id}] Getting proxy from ProxyRotator...", flush=True)
            proxies = None
            if hasattr(self, '_ProxyRotator') and self._ProxyRotator:
                proxy = self._ProxyRotator.get_next_proxy()
                print(f"[CRITICAL_DEBUG] [{thread_id}] Got proxy: {proxy}", flush=True)
                if proxy and isinstance(proxy, dict):
                    proxies = proxy
            else:
                print(f"[CRITICAL_DEBUG] [{thread_id}] No ProxyRotator, using direct connection", flush=True)
            
            # –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô HTTP –∑–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ –Ω–µ–∑–∞–≤–∏—Å–∏–º—É—é —Å–µ—Å—Å–∏—é!
            print(f"[CRITICAL_DEBUG] [{thread_id}] üåê Starting HTTP GET request...", flush=True)
            response = session.get(url, timeout=30, proxies=proxies)
            print(f"[CRITICAL_DEBUG] [{thread_id}] ‚úÖ Got response: {response.status_code}, size: {len(response.content)}", flush=True)
            
            if response.status_code == 200 and len(response.content) > 1000:
                with open(image_path, "wb") as f:
                    f.write(response.content)
                return filename + filetype
            
        except Exception as e:
            print(f"[WARNING] [{thread_id}] Failed to download {url}: {e}", flush=True)
            import traceback
            traceback.print_exc()
        finally:
            # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–µ—Å—Å–∏—é
            if 'session' in locals():
                session.close()
        
        return None
    
    def _PostInitMethod(self):
        """–ú–µ—Ç–æ–¥, –≤—ã–ø–æ–ª–Ω—è—é—â–∏–π—Å—è –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–∞."""
        
        print(f"[CRITICAL_DEBUG] _PostInitMethod() CALLED!", flush=True)

        self.__TitleSlug = None
        self.__API = "api.cdnlibs.org"
        self.__Sites = {
            "mangalib.me": 1,
            "slashlib.me": 2,
            "hentailib.me": 4
        }
        
        print(f"[CRITICAL_DEBUG] About to initialize AdaptiveParallelDownloader...", flush=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ProxyRotator –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        self._ProxyRotator = None
        try:
            import sys
            from pathlib import Path
            
            melon_service_path = Path(__file__).parent.parent.parent
            if str(melon_service_path) not in sys.path:
                sys.path.insert(0, str(melon_service_path))
            
            from proxy_rotator import ProxyRotator
            rotator = ProxyRotator(parser="mangalib")
            
            if rotator.enabled and rotator.get_proxy_count() > 0:
                self._ProxyRotator = rotator
                print(f"[INFO] ‚úÖ ProxyRotator initialized: {rotator.get_proxy_count()} proxies", flush=True)
        except Exception as e:
            print(f"[WARNING] ProxyRotator not available: {e}", flush=True)
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –≤ __init__, –∞ –Ω–µ –≤ parse()
        # –ü–æ—Ç–æ–º—É —á—Ç–æ build –º–æ–∂–µ—Ç –≤—ã–∑—ã–≤–∞—Ç—å—Å—è –±–µ–∑ parse (–∫–æ–≥–¥–∞ JSON —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç)
        proxy_count = self._get_proxy_count()
        image_delay = getattr(self._Settings.common, 'image_delay', 0.2)
        
        print(f"[CRITICAL_DEBUG] proxy_count={proxy_count}, image_delay={image_delay}", flush=True)
        
        # –ù–ï –ù–£–ñ–ï–ù Lock ‚Äî –∫–∞–∂–¥—ã–π –ø–æ—Ç–æ–∫ —Å–æ–∑–¥–∞–µ—Ç —Å–≤–æ—é —Å–µ—Å—Å–∏—é requests!
        self._parallel_downloader = AdaptiveParallelDownloader(
            proxy_count=proxy_count,
            download_func=self._download_image_wrapper,
            max_workers_per_proxy=2,
            max_retries=3,
            base_delay=image_delay
        )
        
        print(f"[CRITICAL_DEBUG] AdaptiveParallelDownloader CREATED successfully!", flush=True)

    def __IsSlideLink(self, link: str, servers: list[str]) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –≤–µ–¥—ë—Ç –ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å–ª–∞–π–¥.
            link ‚Äì —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ;
            servers ‚Äì —Å–ø–∏—Å–æ–∫ —Å–µ—Ä–≤–µ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
        """

        for Server in servers:
            if Server in link: return True

        return False
    
    def __ParseSlideLink(self, link: str, servers: list[str]) -> tuple[str]:
        """
        –ü–∞—Ä—Å–∏—Ç —Å—Å—ã–ª–∫—É –Ω–∞ —Å–ª–∞–π–¥.
            link ‚Äì —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ;
            servers ‚Äì —Å–ø–∏—Å–æ–∫ —Å–µ—Ä–≤–µ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
        """

        OriginalServer = None
        URI = None

        for Server in servers:

            if Server in link:
                OriginalServer = Server
                URI = link.replace(OriginalServer, "")

        return (OriginalServer, URI)

    #==========================================================================================#
    # >>>>> –ü–†–ò–í–ê–¢–ù–´–ï –ú–ï–¢–û–î–´ –ü–ê–†–°–ò–ù–ì–ê <<<<< #
    #==========================================================================================#

    def __CheckCorrectDomain(self, data: dict) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –≤–æ–∑—Ä–∞—Å—Ç–Ω–æ–π —Ä–µ–π—Ç–∏–Ω–≥.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Domain = self._Manifest.site

        if self._Title.site:

            if data["site"] != self.__GetSiteID(self._Title.site):
                Domain = self.__GetSiteDomain(data["site"])
                self._Portals.warning(f"Title site changed to \"{Domain}\".")

        return Domain 

    def __GetAgeLimit(self, data: dict) -> int:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –≤–æ–∑—Ä–∞—Å—Ç–Ω–æ–π —Ä–µ–π—Ç–∏–Ω–≥.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Rating = None
        RatingString = data["ageRestriction"]["label"].split(" ")[0].replace("+", "").replace("–ù–µ—Ç", "")
        if RatingString.isdigit(): Rating = int(RatingString)

        return Rating 

    def __GetAuthors(self, data: dict) -> list[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∞–≤—Ç–æ—Ä–æ–≤."""

        Authors = list()
        for Author in data["authors"]: Authors.append(Author["name"])

        return Authors

    def __GetBranches(self) -> list[Branch]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–∞–π—Ç–ª–∞."""

        Branches: dict[int, Branch] = dict()
        Response = self._Requestor.get(f"https://{self.__API}/api/manga/{self.__TitleSlug}/chapters")
        
        if Response.status_code == 200:
            Data = Response.json["data"]
            sleep(self._Settings.common.delay)

            for CurrentChapterData in Data:

                for BranchData in CurrentChapterData["branches"]:
                    BranchID = BranchData["branch_id"]
                    if BranchID == None: BranchID = int(str(self._Title.id) + "0")
                    if BranchID not in Branches.keys(): Branches[BranchID] = Branch(BranchID)

                    ChapterObject = Chapter(self._SystemObjects)
                    ChapterObject.set_id(BranchData["id"])
                    ChapterObject.set_volume(CurrentChapterData["volume"])
                    ChapterObject.set_number(CurrentChapterData["number"])
                    ChapterObject.set_name(CurrentChapterData["name"])
                    ChapterObject.set_is_paid("restricted_view" in BranchData and not BranchData["restricted_view"]["is_open"])
                    ChapterObject.set_workers([sub["name"] for sub in BranchData["teams"]])
                    ChapterObject.add_extra_data("moderated", False if "moderation" in BranchData.keys() else True)

                    if self._Settings.custom["add_free_publication_date"] and ChapterObject.is_paid:
                        ChapterObject.add_extra_data("free-publication-date", BranchData["restricted_view"]["expired_at"])

                    Branches[BranchID].add_chapter(ChapterObject)

        else: self._Portals.request_error(Response, "Unable to request chapter.", exception = False)

        for CurrentBranch in Branches.values(): self._Title.add_branch(CurrentBranch)

    def __GetCovers(self, data: dict) -> list[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±–ª–æ–∂–µ–∫."""

        Covers = list()

        if data["cover"]:
            Covers.append({
                "link": data["cover"]["default"],
                "filename": data["cover"]["default"].split("/")[-1]
            })

        if self._Settings.common.sizing_images:
            Covers[0]["width"] = None
            Covers[0]["height"] = None

        return Covers

    def __GetDescription(self, data: dict) -> str | None:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Description = None
        if "summary" in data.keys(): Description = RemoveRecurringSubstrings(data["summary"], "\n").strip().replace(" \n", "\n")
        Description = Zerotify(Description)

        return Description

    def __GetFranchises(self, data: dict) -> list[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–µ—Ä–∏–π.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Franchises = list()
        for Franchise in data["franchise"]: Franchises.append(Franchise["name"])
        if "–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–∞–±–æ—Ç—ã" in Franchises: Franchises.remove("–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–∞–±–æ—Ç—ã")

        return Franchises

    def __GetGenres(self, data: dict) -> list[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∂–∞–Ω—Ä–æ–≤.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Genres = list()
        for Genre in data["genres"]: Genres.append(Genre["name"])

        return Genres

    def __GetImagesServers(self, server_id: str | None = None, all_sites: bool = False) -> list[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–æ–º–µ–Ω–æ–≤ —Å–µ—Ä–≤–µ—Ä–æ–≤ —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
            server_id ‚Äì –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–µ—Ä–≤–µ—Ä–∞;\n
            all_sites ‚Äì —É–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ –≤–µ—Ä–Ω—É—Ç—å –Ω—É–∂–Ω–æ –¥–æ–º–µ–Ω—ã —Ö—Ä–∞–Ω–∏–ª–∏—â –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤.
        """

        Servers = list()
        CurrentSiteID = self.__GetSiteID()
        URL = f"https://{self.__API}/api/constants?fields[]=imageServers"
        Headers = {
            "Authorization": self._Settings.custom["token"],
            "Referer": f"https://{self._Manifest.site}/"
        }
        Response = self._Requestor.get(URL, headers = Headers)

        if Response.status_code == 200:
            Data = Response.json["data"]["imageServers"]
            sleep(self._Settings.common.delay)

            for ServerData in Data:

                if server_id:
                    if ServerData["id"] == server_id and CurrentSiteID in ServerData["site_ids"]: Servers.append(ServerData["url"])
                    elif ServerData["id"] == server_id and all_sites: Servers.append(ServerData["url"])

                else:
                    if CurrentSiteID in ServerData["site_ids"] or all_sites: Servers.append(ServerData["url"])

        else:
            self._Portals.request_error(Response, "Unable to request site constants.")

        return Servers

    def __GetSiteDomain(self, id: str) -> int | None:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ–º–µ–Ω —Å–∞–π—Ç–∞.
            id ‚Äì —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–∞–π—Ç–∞.
        """

        SiteDomain = None

        for Domain, ID in self.__Sites.items():
            if ID == id: SiteDomain = Domain
        
        return SiteDomain

    def __GetSiteID(self, site: str = None) -> int | None:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ü–µ–ª–æ—á–∏—Å–ª–µ–Ω–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–∞–π—Ç–∞.
            site ‚Äì –¥–æ–º–µ–Ω —Å–∞–π—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –ø–∞—Ä—Å–µ—Ä–æ–º).
        """

        if not site: site = self._Manifest.site
        SiteID = None

        for Domain in self.__Sites.keys():
            if Domain in site: SiteID = self.__Sites[Domain]
        
        return SiteID

    def __GetSlides(self, branch_id: int, chapter: Chapter) -> list[dict]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ —Å–ª–∞–π–¥–∞—Ö –≥–ª–∞–≤—ã.
            branch_id ‚Äì –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≤–µ—Ç–≤–∏;\n
            chapter ‚Äì –¥–∞–Ω–Ω—ã–µ –≥–ª–∞–≤—ã.
        """

        Slides = list()

        if "moderated" in chapter.to_dict().keys() and not chapter["moderated"]:
            self._Portals.chapter_skipped(self._Title, chapter, comment = "Not moderated.")
            return Slides
        
        Server = self.__GetImagesServers(self._Settings.custom["server"])[0]
        Branch = "" if branch_id == str(self._Title.id) + "0" else f"&branch_id={branch_id}"
        URL = f"https://{self.__API}/api/manga/{self.__TitleSlug}/chapter?number={chapter.number}&volume={chapter.volume}{Branch}"
        Response = self._Requestor.get(URL)
        
        if Response.status_code == 200:
            Data = Response.json["data"].setdefault("pages", tuple())
            sleep(self._Settings.common.delay)

            for SlideIndex in range(len(Data)):
                Buffer = {
                    "index": SlideIndex + 1,
                    "link": Server + Data[SlideIndex]["url"].replace(" ", "%20"),
                    "width": Data[SlideIndex]["width"],
                    "height": Data[SlideIndex]["height"]
                }
                Slides.append(Buffer)

                # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                self._Portals.chapter_download_start(self._Title, chapter, SlideIndex + 1, len(Data))

        else: self._Portals.request_error(Response, "Unable to request chapter content.", exception = False)

        return Slides

    def __GetStatus(self, data: dict) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç—É—Å.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Status = None
        StatusesDetermination = {
            1: Statuses.ongoing,
            2: Statuses.completed,
            3: Statuses.announced,
            4: Statuses.dropped,
            5: Statuses.dropped
        }
        SiteStatusIndex = data["status"]["id"]
        if SiteStatusIndex in StatusesDetermination.keys(): Status = StatusesDetermination[SiteStatusIndex]

        return Status

    def __GetTitleData(self) -> dict | None:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Ç–∞–π—Ç–ª–∞.
            slug ‚Äì –∞–ª–∏–∞—Å.
        """
        
        URL = f"https://{self.__API}/api/manga/{self.__TitleSlug}?fields[]=eng_name&fields[]=otherNames&fields[]=summary&fields[]=releaseDate&fields[]=type_id&fields[]=caution&fields[]=genres&fields[]=tags&fields[]=franchise&fields[]=authors&fields[]=manga_status_id&fields[]=status_id"
        
        # DEBUG: –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        print(f"[DEBUG] üîç Requesting title data for: {self.__TitleSlug}")
        print(f"[DEBUG] üåê URL: {URL}")
        
        Response = self._Requestor.get(URL)
        
        # DEBUG: –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        print(f"[DEBUG] üì° Response status: {Response.status_code}")
        if hasattr(Response, 'text'):
            response_preview = Response.text[:500] if hasattr(Response.text, '__getitem__') else str(Response.text)[:500]
            print(f"[DEBUG] üìÑ Response preview: {response_preview}")

        if Response.status_code == 200:
            Response = Response.json["data"]
            sleep(self._Settings.common.delay)

        elif Response.status_code == 451: 
            self._Portals.request_error(Response, "Account banned.")
            return None
        elif Response.status_code == 404: 
            self._Portals.title_not_found(self._Title)
            return None
        else: 
            self._Portals.request_error(Response, "Unable to request title data.")
            return None

        return Response

    def __GetTags(self, data: dict) -> list[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–≥–æ–≤.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Tags = list()
        for Tag in data["tags"]: Tags.append(Tag["name"])

        return Tags

    def __GetType(self, data: dict) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ç–∏–ø —Ç–∞–π—Ç–ª–∞.
            data ‚Äì —Å–ª–æ–≤–∞—Ä—å –¥–∞–Ω–Ω—ã—Ö —Ç–∞–π—Ç–ª–∞.
        """

        Type = None
        TypesDeterminations = {
            "–ú–∞–Ω–≥–∞": Types.manga,
            "–ú–∞–Ω—Ö–≤–∞": Types.manhwa,
            "–ú–∞–Ω—å—Ö—É–∞": Types.manhua,
            "–†—É–º–∞–Ω–≥–∞": Types.russian_comic,
            "–ö–æ–º–∏–∫—Å –∑–∞–ø–∞–¥–Ω—ã–π": Types.western_comic,
            "OEL-–º–∞–Ω–≥–∞": Types.oel
        }
        SiteType = data["type"]["label"]
        if SiteType in TypesDeterminations.keys(): Type = TypesDeterminations[SiteType]

        return Type

    def __StringToDate(self, date_str: str) -> datetime:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –≤—Ä–µ–º—è –≤ –æ–±—ä–µ–∫—Ç–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é.
            date_str ‚Äì —Å—Ç—Ä–æ–∫–æ–≤–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è.
        """

        DatePattern = "%Y-%m-%dT%H:%M:%S.%fZ"

        return datetime.strptime(date_str, DatePattern)

    #==========================================================================================#
    # >>>>> –ü–£–ë–õ–ò–ß–ù–´–ï –ú–ï–¢–û–î–´ <<<<< #
    #==========================================================================================#

    def amend(self, branch: Branch, chapter: Chapter):
        """
        –î–æ–ø–æ–ª–Ω—è–µ—Ç –≥–ª–∞–≤—É –¥–∞–π–Ω—ã–º–∏ –æ —Å–ª–∞–π–¥–∞—Ö.
            branch ‚Äì –¥–∞–Ω–Ω—ã–µ –≤–µ—Ç–≤–∏;\n
            chapter ‚Äì –¥–∞–Ω–Ω—ã–µ –≥–ª–∞–≤—ã.
        """

        Slides = self.__GetSlides(branch.id, chapter)
        for Slide in Slides: chapter.add_slide(Slide["link"], Slide["width"], Slide["height"])

    def amend_postprocessor(self, chapter: Chapter):
        """
        –í–Ω–æ—Å–∏—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –≥–ª–∞–≤—É –ø–æ—Å–ª–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –µ—ë –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º. –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è.
        
        –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –¥–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.

        :param chapter: –î–∞–Ω–Ω—ã–µ –≥–ª–∞–≤—ã.
        :type chapter: Chapter
        """

        if not self._Settings.custom["add_moderation_status"]: chapter.remove_extra_data("moderated")

    def collect(self, period: int | None = None, filters: str | None = None, pages: int | None = None) -> list[str]:
        """
        –°–æ–±–∏—Ä–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–∞–π—Ç–ª–æ–≤ –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º.
            period ‚Äì –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤ –¥–æ —Ç–µ–∫—É—â–µ–≥–æ –º–æ–º–µ–Ω—Ç–∞, —Å–æ—Å—Ç–∞–≤–ª—è—é—â–µ–µ –ø–µ—Ä–∏–æ–¥ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö;\n
            filters ‚Äì —Å—Ç—Ä–æ–∫–∞, –æ–ø–∏—Å—ã–≤–∞—é—â–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é (–ø–æ–¥—Ä–æ–±–Ω–µ–µ –≤ README.md);\n
            pages ‚Äì –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –∫–∞—Ç–∞–ª–æ–≥–∞.
        """

        Updates = list()
        IsUpdatePeriodOut = False
        Page = 1
        UpdatesCount = 0
        Headers = {
            "Site-Id": str(self.__GetSiteID())
        }
        CurrentDate = datetime.utcnow()

        while not IsUpdatePeriodOut:
            Response = self._Requestor.get(f"https://{self.__API}/api/latest-updates?page={Page}", headers = Headers)

            if Response.status_code == 200:
                UpdatesPage = Response.json["data"]

                for UpdateNote in UpdatesPage:
                    Delta = CurrentDate - self.__StringToDate(UpdateNote["last_item_at"])
                    
                    if Delta.total_seconds() / 3600 <= period:
                        Updates.append(UpdateNote["slug_url"])
                        UpdatesCount += 1

                    else:
                        IsUpdatePeriodOut = True

            else:
                IsUpdatePeriodOut = True
                self._Portals.request_error(Response, f"Unable to request updates page {Page}.")


            if not IsUpdatePeriodOut:
                self._Portals.collect_progress_by_page(Page)
                Page += 1
                sleep(self._Settings.common.delay)

        return Updates

    def image(self, url: str) -> str | None:
        """
        –°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Å–∞–π—Ç–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫–∞—Ç–∞–ª–æ–≥ –ø–∞—Ä—Å–µ—Ä–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞.
            url ‚Äì —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.
        """

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π delay –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–º–µ–Ω—å—à–µ —á–µ–º –¥–ª—è API)
        image_delay = getattr(self._Settings.common, 'image_delay', 0.2)

        Result = self._ImagesDownloader.temp_image(url)
        
        # –ï—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞ - –¥–æ–±–∞–≤–ª—è–µ–º delay –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—Ä–æ—Å–æ–º
        if Result:
            sleep(image_delay)
            return Result
        
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å - –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ —Å–µ—Ä–≤–µ—Ä—ã
        Servers = self.__GetImagesServers(all_sites = True)

        if self.__IsSlideLink(url, Servers):
            OriginalServer, ImageURI = self.__ParseSlideLink(url, Servers)
            Servers.remove(OriginalServer)

            for Server in Servers:
                Link = Server + ImageURI
                Result = self._ImagesDownloader.temp_image(Link)
                
                if Result: 
                    sleep(image_delay)
                    break
                elif Server != Servers[-1]: 
                    sleep(image_delay)

        return Result

    def _get_proxy_count(self) -> int:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏."""
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º settings.json (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1)
        if hasattr(self._Settings, 'proxy') and hasattr(self._Settings.proxy, 'enable') and self._Settings.proxy.enable:
            if hasattr(self._Settings.proxy, 'proxies') and self._Settings.proxy.proxies:
                proxy_count = len(self._Settings.proxy.proxies)
                print(f"[INFO] üåê Detected {proxy_count} proxies from settings.json")
                return proxy_count
        
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ ProxyRotator (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2) - –¢–û–¢ –ñ–ï –ö–û–î –ß–¢–û –í _InitializeRequestor
        try:
            import sys
            import os
            from pathlib import Path
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ MelonService –≤ sys.path –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ proxy_rotator
            melon_service_path = Path(__file__).parent.parent.parent
            if str(melon_service_path) not in sys.path:
                sys.path.insert(0, str(melon_service_path))
            
            from proxy_rotator import ProxyRotator
            
            # –°–æ–∑–¥–∞—ë–º —ç–∫–∑–µ–º–ø–ª—è—Ä —Ä–æ—Ç–∞—Ç–æ—Ä–∞ –¥–ª—è –ø–∞—Ä—Å–µ—Ä–∞
            rotator = ProxyRotator(parser="mangalib")
            
            if rotator.enabled and rotator.get_proxy_count() > 0:
                proxy_count = rotator.get_proxy_count()
                print(f"[INFO] üåê Detected {proxy_count} proxies from ProxyRotator")
                return proxy_count
                
        except ImportError:
            pass
        except Exception as e:
            print(f"[WARNING] ‚ö†Ô∏è  Error detecting ProxyRotator: {e}")
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º —á—Ç–æ 1 –ø—Ä–æ–∫—Å–∏ (–∏–ª–∏ –ø—Ä—è–º–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ)
        print(f"[INFO] üåê No proxies detected, using 1 worker")
        return 1

    def batch_download_images(self, urls: list[str]) -> list[str | None]:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –±–∞—Ç—á–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
        
        :param urls: –°–ø–∏—Å–æ–∫ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        :return: –°–ø–∏—Å–æ–∫ –∏–º—ë–Ω —Ñ–∞–π–ª–æ–≤ (–∏–ª–∏ None –¥–ª—è –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫) –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ
        """
        
        print(f"[INFO] [DEBUG] ‚úÖ batch_download_images() CALLED with {len(urls)} URLs")
        print(f"[INFO] [DEBUG] _parallel_downloader initialized: {hasattr(self, '_parallel_downloader')}")
        
        if not urls:
            print(f"[INFO] [DEBUG] ‚ö†Ô∏è No URLs provided, returning empty list")
            return []
        
        print(f"[INFO] [DEBUG] üöÄ Starting parallel download with {len(urls)} images...")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫
        results = self._parallel_downloader.download_batch(urls)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å–ø–∏—Å–æ–∫ –∏–º—ë–Ω —Ñ–∞–π–ª–æ–≤ (—Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫)
        filenames = []
        for result in results:
            if result['success']:
                filenames.append(result['filename'])
            else:
                # –î–ª—è –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–≥—Ä—É–∑–æ–∫ –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–µ—Ä–≤–µ—Ä—ã
                fallback_result = self._try_alternative_servers(result['url'])
                filenames.append(fallback_result)
        
        return filenames

    def _try_alternative_servers(self, url: str) -> str | None:
        """–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤ (fallback)."""
        
        Servers = self.__GetImagesServers(all_sites=True)
        
        if self.__IsSlideLink(url, Servers):
            OriginalServer, ImageURI = self.__ParseSlideLink(url, Servers)
            
            try:
                Servers.remove(OriginalServer)
            except ValueError:
                pass
            
            for Server in Servers:
                Link = Server + ImageURI
                Result = self._ImagesDownloader.temp_image(Link)
                
                if Result:
                    return Result
        
        return None

    def parse(self):
        """–ü–æ–ª—É—á–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–∞–π—Ç–ª–∞."""

        print(f"[DEBUG] üöÄ Starting parse() for title: {self._Title.slug}")
        
        self._Requestor.config.add_header("Site-Id", str(self.__Sites[self._Manifest.site]))

        # MangaLib –∏–∑–º–µ–Ω–∏–ª —Å—Ç—Ä—É–∫—Ç—É—Ä—É: —Ç–µ–ø–µ—Ä—å slug'–∏ –∏–∑ API –∏–º–µ—é—Ç —Ñ–æ—Ä–º–∞—Ç "ID--slug"
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ slug ID (—Ñ–æ—Ä–º–∞—Ç: "7580--i-alone-level-up")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º slug –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        slug_with_id = self._Title.slug
        clean_slug = self._Title.slug
        extracted_id = None
        
        if "--" in self._Title.slug:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏ slug –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ "ID--slug"
            parts = self._Title.slug.split("--", 1)
            if len(parts) == 2 and parts[0].isdigit():
                extracted_id = int(parts[0])
                clean_slug = parts[1]
                print(f"[DEBUG] Extracted: ID={extracted_id}, slug={clean_slug}")
        
        # API —Ç—Ä–µ–±—É–µ—Ç –ø–æ–ª–Ω—ã–π slug (ID--slug)
        self.__TitleSlug = slug_with_id
        # –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –±–µ–∑ ID
        self._Title.set_slug(clean_slug)

        print(f"[DEBUG] TitleSlug (API): {self.__TitleSlug}")
        print(f"[DEBUG] Title.slug (file): {self._Title.slug}")

        Data = self.__GetTitleData()
        
        print(f"[DEBUG] üì¶ GetTitleData returned: {type(Data)}, is None: {Data is None}")
        if Data:
            print(f"[DEBUG] ‚úÖ Data keys: {list(Data.keys())[:10] if isinstance(Data, dict) else 'NOT A DICT'}")
        
        self._SystemObjects.manager.get_parser_settings()

        if Data:
            self._Title.set_site(self.__CheckCorrectDomain(Data))
            
            # ID —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–∑ –∏–∑–≤–ª–µ—á—ë–Ω–Ω–æ–≥–æ –∏–ª–∏ –∏–∑ API
            if extracted_id is not None:
                self._Title.set_id(extracted_id)
                if extracted_id != Data["id"]:
                    print(f"[WARNING] ID mismatch: extracted={extracted_id}, API={Data['id']} (using extracted)")
            else:
                self._Title.set_id(Data["id"])
            
            # Slug –£–ñ–ï —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤—ã—à–µ (—á–∏—Å—Ç—ã–π, –±–µ–∑ ID)
            print(f"[DEBUG] Final slug (file): {self._Title.slug}")
            print(f"[DEBUG] Final ID: {self._Title.id}")
            
            self._Title.set_content_language("rus")
            self._Title.set_localized_name(Data["rus_name"])
            self._Title.set_eng_name(Data["eng_name"])
            self._Title.set_another_names(Data["otherNames"])
            if Data["name"] not in Data["otherNames"] and Data["name"] != Data["rus_name"] and Data["name"] != Data["eng_name"]: self._Title.add_another_name(Data["name"])
            self._Title.set_covers(self.__GetCovers(Data))
            self._Title.set_authors(self.__GetAuthors(Data))
            self._Title.set_publication_year(int(Data["releaseDate"]) if Data["releaseDate"] else None)
            self._Title.set_description(self.__GetDescription(Data))
            self._Title.set_age_limit(self.__GetAgeLimit(Data))
            self._Title.set_type(self.__GetType(Data))
            self._Title.set_status(self.__GetStatus(Data))
            self._Title.set_is_licensed(Data["is_licensed"])
            self._Title.set_genres(self.__GetGenres(Data))
            self._Title.set_tags(self.__GetTags(Data))
            self._Title.set_franchises(self.__GetFranchises(Data))

            self.__GetBranches()